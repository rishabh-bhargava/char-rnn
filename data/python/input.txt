import unirest
import os
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats.mstats import mquantiles
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import preprocessing
from itertools import izip

def get_atts(path):
	my_file = open(path, 'r')
	x = []
	y = []
	my_file.readline()
	for line in my_file:
		temp = line.rstrip().split(',')
		x.append(float(temp[0]))
		y.append(float(temp[1]))

	x_prime = []
	y_prime = []
	speed = []
	for i in xrange(len(x) - 1):
		x_prime.append(x[i+1] - x[i])
		y_prime.append(y[i+1] - y[i])
		speed.append((x_prime[i]**2 + y_prime[i]**2)**0.5)

	x_prime2 = []
	y_prime2 = []
	acc = []
	for i in xrange(len(x_prime) - 1):
		x_prime2.append(x_prime[i+1] - x_prime[i])
		y_prime2.append(y_prime[i+1] - y_prime[i])
		acc.append((x_prime2[i]**2 + y_prime2[i]**2)**0.5)

	#print len(speed)
	#print path
	atts = [np.mean(speed), np.std(speed), max(speed), np.mean(acc), np.std(acc), sum(speed)]
	#print atts
	return atts

def get_preds(atts, org_ind=-1):
	#a = get_atts(path)[:5]
	print atts
	data_array = np.genfromtxt('driver_stats.csv', delimiter=',', dtype=float)
	max_sim = 0
	max_ind = -1
	for ind, row in enumerate(data_array):
		sim = cosine_similarity(atts, row)
		if sim > max_sim and ind != org_ind:
			max_sim = sim
			max_ind = ind
	#print data_array[max_ind,:]
	print max_sim
	print max_ind


def save_quantiles():
	data_array = np.genfromtxt('driver_stats.csv', delimiter=',', dtype=float)
	b = [mquantiles(data_array[:,i], [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]) for i in range(5)]
	b = np.transpose(np.array(b))
	file2 = open('deciles.csv', 'w')
	for line in b:
		file2.write(','.join([str(x) for x in line]) + '\n')

def save_score_quantiles():
	data_array = np.genfromtxt('scores.txt', dtype=float)
	b = mquantiles(data_array, [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])
	file2 = open('scoredeciles.txt', 'w')
	for el in b:
		file2.write(str(el) + '\n')

def get_quantile(atts):
	#atts = get_atts(path)[:5]
	print atts
	i = []
	data_array = np.genfromtxt('deciles.csv', delimiter=',', dtype=float)
	for k in range(5):
		count = 0
		arr = data_array[:,k]
		for ind, a in enumerate(arr):
			if a > atts[k]:
				i.append(ind)
				break
		if len(i) == i:
			i.append(9)

	return i

def get_risk_decile(score):
	print score
	data_array = np.genfromtxt('scoredeciles.txt', dtype=float)
	i = None
	for ind, el in enumerate(data_array):
		if el > score:
			i = ind
			break
	if i is None:
		i.append(9)

	return i


def get_scores():
	costs = np.array([1, 2, 3, 4, 2.5])
	data_array = np.genfromtxt('driver_stats.csv', delimiter=',', dtype=float)
	scaled = preprocessing.scale(data_array)
	b = []
	scores = open('scores.txt','w')
	for el in data_array:
		b.append(np.dot(costs, el))
		scores.write(str(np.dot(costs, el)) + '\n')

def plot_quantiles(atts):
	data_array = np.genfromtxt('deciles.csv', delimiter=',', dtype=float)
	titles = ["Deciles for mean value of speed", "Deciles for standard deviation of speed", "Deciles for max-speed", "Deciles for mean value of acceleration", "Deciles for standard deviation of acceleration"]
	for i in xrange(5):
		plt.figure()
		#print len([0, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])
		#print len([0] + data_array[:,i].tolist() + [1])
		plt.plot([0, 1,2,3,4,5,6,7,8,9], [0]+data_array[:,i].tolist())
		plt.scatter(get_quantile(atts)[i]+ 0.5, atts[i])
		plt.title(titles[i])

def plot_score_decile(score):
	score_array = np.genfromtxt('scoredeciles.txt', dtype=float)
	plt.figure()
	plt.plot([0, 1,2,3,4,5,6,7,8,9], [0]+score_array.tolist())
	plt.scatter(get_risk_decile(score)+0.5, score)
	plt.title("Decile for risk profile")


file1 = open('driver_names.txt', 'w')
#file2 = open('overall', 'r')

# These code snippets use an open-source library. http://unirest.io/python
response = unirest.get("https://yoda.p.mashape.com/yoda?sentence=You+will+learn+how+to+speak+like+me+someday.++Oh+wait.",
  headers={
    "X-Mashape-Key": "4FN38Wk6hJmshSTCdXqwygkrXibBp1ELAiwjsnuZ7kglgBVyva",
    "Accept": "text/plain"
  }
)

#print response.body

for dirs in os.listdir('drivers'):
	#my_file = open('drivers/' + dirs + '/' + 'atts.csv')
	#file1.write()
	file1.write(dirs + '\n')
	#data_array = np.genfromtxt('drivers/' + dirs + '/' + 'atts.csv', delimiter=',', dtype=str)
	#print ','.join([str(np.mean([float(x[1:-1]) for x in data_array[:,0]])), str(np.mean([float(x[1:-1]) for x in data_array[:,1]]))])
	#file1.write(','.join([str(np.mean([float(x[1:-1]) for x in data_array[:,0]])), str(np.mean([float(x[1:-1]) for x in data_array[:,1]])), str(np.mean([float(x[1:-1]) for x in data_array[:,2]])), str(np.mean([float(x[1:-1]) for x in data_array[:,3]])), str(np.mean([float(x[1:-1]) for x in data_array[:,4]]))])+ '\n')

#save_score_quantiles()
data_array = np.genfromtxt('driver_stats.csv', delimiter=',', dtype=float)
score_array = np.genfromtxt('scores.txt', dtype=float)
#print get_risk_decile(score_array[332])

print cosine_similarity(data_array[3,:], data_array[1004,:])

get_preds(data_array[3,:], 3)
#get_preds(get_atts('drivers/3/3.csv')[:5])
#print save_quantiles()
#get_quantile(data_array[332,:])

plot_quantiles(data_array[332,:])

plot_score_decile(score_array[332])

plt.figure()
plt.hist(data_array[:,0], 100)
plt.title('Mean speed')
plt.figure()
plt.hist(data_array[:,1], range(0,120,5))
plt.title('Std of speed')
plt.figure()
plt.hist(data_array[:,2], range(0,200,5))
plt.title('Max speed')
plt.figure()
plt.hist(data_array[:,3], range(0,8,1))
plt.title('Mean acceleration')
plt.figure()
plt.hist(data_array[:,4], range(0,40,1))
plt.title('Std acceleration')


plt.show()import csv

file1 = open('final2.csv', 'r')
file2 = open("final3.csv", 'w')
#wr = csv.writer(file2, quoting=csv.QUOTE_NONE)

file1.readline()
file2.write("driver_trip" +"," +"prob" + '\n')

for line in file1:
	temp = line.rstrip().split(',')
	#print str(temp[0][3:-3])
	#print float(temp[1][1:-1])
	file2.write(str(temp[0][3:-3])+ ','+(temp[1][1:-1]) + '\n')

file1.close()
file2.close()
import os

my_file = open('drivers/1/1.csv', 'r')
x = []
y = []
my_file.readline()
for line in my_file:
	temp = line.rstrip().split(',')
	x.append(float(temp[0]))
	y.append(float(temp[1]))

x_prime = []
y_prime = []
speed = []
for i in xrange(len(x) - 1):
	x_prime.append(x[i+1] - x[i])
	y_prime.append(y[i+1] - y[i])
	speed.append((x_prime[i]**2 + y_prime[i]**2)**0.5)

my_file.close()
my_file = open('speeds1_1.csv', 'w')
for i, s in enumerate(speed):
	my_file.write(str(i) + ',' + str(s) + '\n')import numpy as np
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import os
import csv
from scipy import stats
from sklearn.svm import OneClassSVM

data_array = np.genfromtxt("drivers/200/atts.csv")

def get_preds(path):
#print data_array
	my_file = open(path, "r")
	a = []
	for line in my_file:
		temp = (line.rstrip().split(','))
	#print float(temp[0][1:-1])
		temp1 = [float(x[1:-1]) for x in temp]
		a.append(temp1)
	pca = PCA(n_components = 2)
	x_new = pca.fit_transform(a)
#print (pca.explained_variance_ratio_)
#print x_new.shape

#print pca.inverse_transform([1,  0])
#print pca.inverse_transform([0,  1])

#print pca.score(a)



#plt.scatter(x_new[:,0], x_new[:,1])
#plt.show()

	km = KMeans(n_clusters = 2)
	kmeans = km.fit(x_new).predict(x_new)
	#print kmeans
	return kmeans, x_new


def get_preds_SVM(path):
	my_file = open(path, "r")
	a = []
	for line in my_file:
		temp = (line.rstrip().split(','))
		temp1 = [float(x[1:-1]) for x in temp]
		a.append(temp1)
		print a
	o_sv = OneClassSVM(nu=0.1)
	return o_sv.fit(a).predict(a)

def save_all_preds(path):
	my_file = open("final.csv", 'wb')
	wr = csv.writer(my_file, quoting=csv.QUOTE_ALL)
	wr.writerow(["driver_trip", "prob"])
	#count = 1
	for direc in os.listdir(path):
		print path + '/' + direc + '/' + 'atts.csv'
		kmeans, x_new = get_preds(path + '/' + direc + '/' + 'atts.csv')
		boolean = stats.mode(kmeans) == 1
		for i in xrange(len(kmeans)):
			if not boolean:
				wr.writerow([direc+"_"+str(i+1), 1 - kmeans[i]])
			else:
				wr.writerow([direc+"_"+str(i+1), kmeans[i]])

		#count+=1
		#if count == 13:
		#	break

#save_all_preds("drivers")

#plt.figure()
kmeans, x_new = get_preds("drivers/100/atts.csv")
print kmeans
svms = get_preds_SVM("drivers/1000/atts.csv")
for i in xrange(len(kmeans)):
	kmeans[i] = 1-kmeans[i];

for i in xrange(len(kmeans)):
	if kmeans[i] == 1:
		#print 'here'
		plt.scatter(x_new[i][0], x_new[i][1], c='r')
	else:
		print i
		plt.scatter(x_new[i][0], x_new[i][1], c='b')

plt.show()

import numpy as np
import scipy
import matplotlib.pyplot as plt
import csv
import os
import random
from scipy.stats.mstats import mquantiles
from sklearn.linear_model import LogisticRegression
import copy
from sklearn.metrics import roc_curve, auc
from sklearn import preprocessing


def get_atts(path):
	my_file = open(path, 'r')
	x = []
	y = []
	my_file.readline()
	for line in my_file:
		temp = line.rstrip().split(',')
		x.append(float(temp[0]))
		y.append(float(temp[1]))

	x_prime = []
	y_prime = []
	speed = []
	for i in xrange(len(x) - 1):
		x_prime.append(x[i+1] - x[i])
		y_prime.append(y[i+1] - y[i])
		speed.append((x_prime[i]**2 + y_prime[i]**2)**0.5)



	x_prime2 = []
	y_prime2 = []
	acc = []
	for i in xrange(len(x_prime) - 1):
		x_prime2.append(x_prime[i+1] - x_prime[i])
		y_prime2.append(y_prime[i+1] - y_prime[i])
		acc.append((x_prime2[i]**2 + y_prime2[i]**2)**0.5)

	#print len(speed)
	#print path
	atts = [np.mean(speed), np.std(speed), max(speed), np.mean(acc), np.std(acc), sum(speed)]
	#print atts
	atts = atts +mquantiles(speed, [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95]).tolist()
	return atts

def print_atts(path):
	my_file = open(path+ '/' + "atts.csv", 'wb')
	wr = csv.writer(my_file, quoting=csv.QUOTE_ALL)
	a =[get_atts(path + '/'+ x) for x in os.listdir(path) if not x.startswith("a")]
	#print a
	for el in a:
		wr.writerow(el)

	#return a

def print_all_atts(path):
	for p in os.listdir(path):
		print path+'/' + p
		print_atts(path + '/' + p)


l = random.sample(range(len(os.listdir('drivers'))), 5)
l = [1,2488,1913,1520,177]
train_x = []
print l
my_file = open("atts1.csv", 'wb')
path = 'drivers'
wr = csv.writer(my_file, quoting=csv.QUOTE_ALL)
for wrong in l:
	print path+ '/' + str(wrong)
	#print os.listdir(path+ '/' + str(wrong))
	for dirs in os.listdir(path+ '/' + str(wrong)):
		if not dirs.startswith("a"):
			
			a =get_atts(path+ '/' +str(wrong)+ '/'+dirs)
			train_x.append(a)
			#print a
		#print el
			wr.writerow(a)

count =1
my_file = open("final1.csv", 'wb')
wr = csv.writer(my_file, quoting=csv.QUOTE_ALL)
wr.writerow(["driver_trip", "prob"])
for dirs in os.listdir('drivers'):
	train_xx = copy.deepcopy(train_x)
	for csvs in os.listdir(path + '/' + dirs):
		if not csvs.startswith("a"):
			a = get_atts(path + '/' + dirs + '/' + csvs)
			train_xx.append(a)
	train_y = [0 for x in range(1000)] + [1 for x in range(200)]
	train_xx = preprocessing.scale(train_xx)
	#print train_xx
	#raw_input()
	#print train_y
	#print len(train_xx)
	lr = LogisticRegression()
	probs = lr.fit(train_xx, train_y).predict_proba(train_xx[1000:])
	#print probs
	#print len(train_y[1000:])
	#print len(probs)
	#fpr, tpr, thresholds = roc_curve(train_y, probs[:,1])
	#print fpr
	#print tpr
	print dirs
	
	for i in xrange(len(probs)):
		wr.writerow([dirs+"_"+str(i+1), probs[i,1]])
	#roc_auc = auc(fpr, tpr)
	#print roc_auc


#train_y = [0 for x in range(1000)] + [1 for x in range(200)]
#print len(train_x)
#print len(train_x[0])

#lr = LogisticRegression()
#probs = lr.fit(train_x, train_y).predict_proba(train_x[1000:])
#print probs
#print get_atts('drivers/1/2.csv')
#print_atts('drivers/1000')

#def get_preds_lr(path):


#print_all_atts('drivers')

#plt.plot(x,y)
#plt.show()
#my_file.close()
#!/usr/bin/env python
import os
import sys

if __name__ == "__main__":
    os.environ.setdefault("DJANGO_SETTINGS_MODULE", "projects.settings")

    from django.core.management import execute_from_command_line

    execute_from_command_line(sys.argv)
# -*- coding: utf-8 -*-
from south.utils import datetime_utils as datetime
from south.db import db
from south.v2 import SchemaMigration
from django.db import models


class Migration(SchemaMigration):

    def forwards(self, orm):
        # Adding model 'Category'
        db.create_table(u'projects_category', (
            (u'id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('total_pledged', self.gf('django.db.models.fields.IntegerField')(default=0)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=50)),
        ))
        db.send_create_signal(u'projects', ['Category'])

        # Adding model 'Project'
        db.create_table(u'projects_project', (
            (u'id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('category', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['projects.Category'])),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=50)),
            ('pledged', self.gf('django.db.models.fields.IntegerField')(default=0)),
        ))
        db.send_create_signal(u'projects', ['Project'])


    def backwards(self, orm):
        # Deleting model 'Category'
        db.delete_table(u'projects_category')

        # Deleting model 'Project'
        db.delete_table(u'projects_project')


    models = {
        u'projects.category': {
            'Meta': {'object_name': 'Category'},
            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '50'}),
            'total_pledged': ('django.db.models.fields.IntegerField', [], {'default': '0'})
        },
        u'projects.project': {
            'Meta': {'object_name': 'Project'},
            'category': ('django.db.models.fields.related.ForeignKey', [], {'to': u"orm['projects.Category']"}),
            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '50'}),
            'pledged': ('django.db.models.fields.IntegerField', [], {'default': '0'})
        }
    }

    complete_apps = ['projects']from django.db import models

# Create your models here.

class Category(models.Model):
	
	total_pledged = models.IntegerField(default = 0)
	name = models.CharField(max_length = 50)

	def __unicode__(self):
		return self.name


class Project(models.Model):
	
	category = models.ForeignKey(Category)
	name = models.CharField(max_length = 50)
	pledged = models.IntegerField(default = 0)

	def __unicode__(self):
		return self.name"""
Django settings for projects project.

For more information on this file, see
https://docs.djangoproject.com/en/1.6/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/1.6/ref/settings/
"""

# Build paths inside the project like this: os.path.join(BASE_DIR, ...)
import os
BASE_DIR = os.path.dirname(os.path.dirname(__file__))


# Quick-start development settings - unsuitable for production
# See https://docs.djangoproject.com/en/1.6/howto/deployment/checklist/

# SECURITY WARNING: keep the secret key used in production secret!
SECRET_KEY = 'go(bc=)0j8f1b_y73@)$lmc^@#*s_%qla_=m8wbyi4-#*b8(-f'

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = True

TEMPLATE_DEBUG = True

ALLOWED_HOSTS = []


# Application definition

INSTALLED_APPS = (
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'south',
    'projects',
)

MIDDLEWARE_CLASSES = (
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
)

ROOT_URLCONF = 'projects.urls'

WSGI_APPLICATION = 'projects.wsgi.application'


# Database
# https://docs.djangoproject.com/en/1.6/ref/settings/#databases

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),
    }
}

# Internationalization
# https://docs.djangoproject.com/en/1.6/topics/i18n/

LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'UTC'

USE_I18N = True

USE_L10N = True

USE_TZ = True


# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/1.6/howto/static-files/

STATIC_URL = '/static/'
from django.conf.urls import patterns, include, url

from django.contrib import admin
admin.autodiscover()

urlpatterns = patterns('',
    # Examples:
    # url(r'^$', 'projects.views.home', name='home'),
    # url(r'^blog/', include('blog.urls')),

    url(r'^admin/', include(admin.site.urls)),
)
"""
WSGI config for projects project.

It exposes the WSGI callable as a module-level variable named ``application``.

For more information on this file, see
https://docs.djangoproject.com/en/1.6/howto/deployment/wsgi/
"""

import os
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "projects.settings")

from django.core.wsgi import get_wsgi_application
application = get_wsgi_application()
import os

def populate():
	c = Category(name = "anothertest")
	c.save()


if __name__== '__main__':
	print "doing population stuff"
	os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'projects.settings')
	from projects.models import Category, Project
	populate()from bs4 import BeautifulSoup
from urllib2 import urlopen
import re

BASE_URL = "https://www.kickstarter.com/"

class Project(object):
	def __init__ (self, name, pledged=0):
		self.money_pledged = pledged
		self.name = name

	def __str__(self):
		return str(self.name) + ": $" + str(self.money_pledged)


# This gets the list of projects on the given category webpage
def get_projects(categ_url):
	html = urlopen(categ_url).read()
	soup = BeautifulSoup(html, "lxml")
	titles = [title.find("h2") for title in soup.findAll("div", "project-card")]
	links = [BASE_URL[0:len(BASE_URL)-1] + h2.a["href"] for h2 in titles]
	return links


# This gets the data (name and money_pledged) of a particular project given the link to the project page
def get_project_data(project_link):
	html = urlopen(project_link).read()
	soup = BeautifulSoup(html, "lxml")
	name_cont = soup.find("div", "NS-project_-running_board")
	name = name_cont.find("h2").text.encode('ascii', 'ignore').strip()

	money_cont = soup.find("div", "NS_projects__ecom")
	money_pledged = money_cont.find("h5").find_next_sibling("h5").div.data.text.encode('ascii', 'ignore').strip()
	money = ""
	for c in money_pledged:
		if c.isdigit():
			money += c
	return Project(name = name, pledged = money)


#	This returns all the categories of Kickstarter projects as a list from the website
def get_categories():
	html = urlopen(BASE_URL).read()
	soup = BeautifulSoup(html, "lxml")
	categ_list = soup.find("ul", "nav small_type")
	categories = [(li.text.encode('ascii', 'ignore').strip(), BASE_URL[0:len(BASE_URL)-1] + li.a["href"]) for li in categ_list.findAll("li")]
	#print categories
	return categories

def main():
	valid_categories = get_categories()
	not_found = True
	while not_found:
		category = raw_input("Write category" + '\n')
		for i in xrange(len(valid_categories)):
			if category == valid_categories[i][0]:
				link = valid_categories[i][1]
				not_found = False
		if not_found:
			print "Invalid category; try again."
	print link
	project_links = get_projects(link)
	total_data = []
	total_money = 0
	for p_link in project_links:
		p_data = get_project_data(p_link)
		print p_data
		total_data.append(p_data)
		total_money += float(p_data.money_pledged)
	print "Total money pledged to these 20 projects is: " + str(total_money)
	#print str(total_data)



if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print '\nGoodbye!'def strxor(a, b):     # xor two strings of different lengths
	if len(a) > len(b):
		return "".join([chr(ord(x) ^ ord(y)) for (x, y) in zip(a[:len(b)], b)])
	else:
		return "".join([chr(ord(x) ^ ord(y)) for (x, y) in zip(a, b[:len(a)])])

messages = ["315c4eeaa8b5f8aaf9174145bf43e1784b8fa00dc71d885a804e5ee9fa40b16349c146fb778cdf2d3aff021dfff5b403b510d0d0455468aeb98622b137dae857553ccd8883a7bc37520e06e515d22c954eba5025b8cc57ee59418ce7dc6bc41556bdb36bbca3e8774301fbcaa3b83b220809560987815f65286764703de0f3d524400a19b159610b11ef3e", "234c02ecbbfbafa3ed18510abd11fa724fcda2018a1a8342cf064bbde548b12b07df44ba7191d9606ef4081ffde5ad46a5069d9f7f543bedb9c861bf29c7e205132eda9382b0bc2c5c4b45f919cf3a9f1cb74151f6d551f4480c82b2cb24cc5b028aa76eb7b4ab24171ab3cdadb8356f", "32510ba9a7b2bba9b8005d43a304b5714cc0bb0c8a34884dd91304b8ad40b62b07df44ba6e9d8a2368e51d04e0e7b207b70b9b8261112bacb6c866a232dfe257527dc29398f5f3251a0d47e503c66e935de81230b59b7afb5f41afa8d661cb", "32510ba9aab2a8a4fd06414fb517b5605cc0aa0dc91a8908c2064ba8ad5ea06a029056f47a8ad3306ef5021eafe1ac01a81197847a5c68a1b78769a37bc8f4575432c198ccb4ef63590256e305cd3a9544ee4160ead45aef520489e7da7d835402bca670bda8eb775200b8dabbba246b130f040d8ec6447e2c767f3d30ed81ea2e4c1404e1315a1010e7229be6636aaa", "3f561ba9adb4b6ebec54424ba317b564418fac0dd35f8c08d31a1fe9e24fe56808c213f17c81d9607cee021dafe1e001b21ade877a5e68bea88d61b93ac5ee0d562e8e9582f5ef375f0a4ae20ed86e935de81230b59b73fb4302cd95d770c65b40aaa065f2a5e33a5a0bb5dcaba43722130f042f8ec85b7c2070", "32510bfbacfbb9befd54415da243e1695ecabd58c519cd4bd2061bbde24eb76a19d84aba34d8de287be84d07e7e9a30ee714979c7e1123a8bd9822a33ecaf512472e8e8f8db3f9635c1949e640c621854eba0d79eccf52ff111284b4cc61d11902aebc66f2b2e436434eacc0aba938220b084800c2ca4e693522643573b2c4ce35050b0cf774201f0fe52ac9f26d71b6cf61a711cc229f77ace7aa88a2f19983122b11be87a59c355d25f8e4", "32510bfbacfbb9befd54415da243e1695ecabd58c519cd4bd90f1fa6ea5ba47b01c909ba7696cf606ef40c04afe1ac0aa8148dd066592ded9f8774b529c7ea125d298e8883f5e9305f4b44f915cb2bd05af51373fd9b4af511039fa2d96f83414aaaf261bda2e97b170fb5cce2a53e675c154c0d9681596934777e2275b381ce2e40582afe67650b13e72287ff2270abcf73bb028932836fbdecfecee0a3b894473c1bbeb6b4913a536ce4f9b13f1efff71ea313c8661dd9a4ce", "315c4eeaa8b5f8bffd11155ea506b56041c6a00c8a08854dd21a4bbde54ce56801d943ba708b8a3574f40c00fff9e00fa1439fd0654327a3bfc860b92f89ee04132ecb9298f5fd2d5e4b45e40ecc3b9d59e9417df7c95bba410e9aa2ca24c5474da2f276baa3ac325918b2daada43d6712150441c2e04f6565517f317da9d3", "271946f9bbb2aeadec111841a81abc300ecaa01bd8069d5cc91005e9fe4aad6e04d513e96d99de2569bc5e50eeeca709b50a8a987f4264edb6896fb537d0a716132ddc938fb0f836480e06ed0fcd6e9759f40462f9cf57f4564186a2c1778f1543efa270bda5e933421cbe88a4a52222190f471e9bd15f652b653b7071aec59a2705081ffe72651d08f822c9ed6d76e48b63ab15d0208573a7eef027", "466d06ece998b7a2fb1d464fed2ced7641ddaa3cc31c9941cf110abbf409ed39598005b3399ccfafb61d0315fca0a314be138a9f32503bedac8067f03adbf3575c3b8edc9ba7f537530541ab0f9f3cd04ff50d66f1d559ba520e89a2cb2a83", "32510ba9babebbbefd001547a810e67149caee11d945cd7fc81a05e9f85aac650e9052ba6a8cd8257bf14d13e6f0a803b54fde9e77472dbff89d71b57bddef121336cb85ccb8f3315f4b52e301d16e9f52f904"]


print len(messages)
a = messages[0].decode('hex')


b1 = strxor(messages[0].decode('hex'), messages[10].decode('hex'))
b2 = strxor(messages[1].decode('hex'), messages[10].decode('hex'))
b3 = strxor(messages[2].decode('hex'), messages[10].decode('hex'))
b4 = strxor(messages[3].decode('hex'), messages[10].decode('hex'))

for c in xrange(len(b1)):
	print ord(b1[c]), ord(b2[c]), ord(b3[c]), ord(b4[c])

print "length is " +str(len(b3))from urllib2 import urlopen
import requests

width = raw_input("Enter width")
height = raw_input("Enter height")

url = "http://placekitten.com/" + width + "/" + height


kittens = urlopen(url)
response = kittens.read()
#print response
file_kitten = open("kitten.jpeg", "w")
file_kitten.write(response)
file_kitten.close()
#body = response[559:1000]

# Add your 'print' statement here!
#print body

#resp = requests.get("http://placekitten.com/")
#print resp.headers
#print resp.encoding
#file_kitten = open("ki.jpeg", "w")
#file_kitten.write(resp)
#file_kitten.close()#!/usr/bin/python
# -*- coding: utf-8 -*-

"""
The Kittens project

author: Rishabh Bhargava

"""

import sys
from PyQt4 import QtGui
from urllib2 import urlopen
import requests

class Kitten(QtGui.QWidget):

	def __init__(self):
		super(Kitten, self).__init__()

		self.initUI()

	def initUI(self):

		self.setGeometry(300, 300, 250, 150)
		self.setWindowTitle("KittenApp")
		self.setWindowIcon(QtGui.QIcon('kitten_icon.png'))

		self.height_label = QtGui.QLabel("Enter height: ", self)
		self.height_label.move(10,10)

		self.width_label = QtGui.QLabel("Enter width: ", self)
		self.width_label.move(10, 40)

		self.file_name_label = QtGui.QLabel("What name? ", self)
		self.file_name_label.move(10, 70)

		self.ok_button = QtGui.QPushButton('Click me', self)
		self.ok_button.resize(self.ok_button.sizeHint())
		self.ok_button.move(150,120)
		self.ok_button.setToolTip("Please <b>Click</b> me!")
		self.ok_button.clicked.connect(self.ok_button_clicked)

		self.height_line_edit = QtGui.QLineEdit(self)
		self.height_line_edit.move(100, 10)

		self.width_line_edit = QtGui.QLineEdit(self)
		self.width_line_edit.move(100, 40)

		self.file_name_line_edit = QtGui.QLineEdit(self)
		self.file_name_line_edit.move(100, 70)

		self.show()

	def ok_button_clicked(self):
		
		if is_number(str(self.height_line_edit.text())) and is_number(str(self.width_line_edit.text())) and str(self.file_name_line_edit.text()) != '':
			
			save_image(str(self.height_line_edit.text()), str(self.width_line_edit.text()), str(self.file_name_line_edit.text()))
			self.height_line_edit.clear()
			self.width_line_edit.clear()
			self.file_name_line_edit.clear()

		else:
			error = QtGui.QMessageBox()
			error.setText("Check the height, width and name fields again!")
			error.exec_()

def save_image(height, width, name):
	url = "http://placekitten.com/" + width + "/" + height


	kittens = urlopen(url)
	response = kittens.read()

	file_name = name + ".jpeg"

	file_kitten = open(file_name, "w")
	file_kitten.write(response)
	file_kitten.close()

def main():

	app = QtGui.QApplication(sys.argv)
	kit = Kitten()
	sys.exit(app.exec_())

def is_number(num):
	 try:
	 	float(num)
	 except:
	 	return False
	 return True

if __name__ == '__main__':
	main()

import sys
from PyQt4 import QtGui


class Example(QtGui.QWidget):
    
    def __init__(self):
        super(Example, self).__init__()
        
        self.initUI()
        
    def initUI(self):      

        self.btn = QtGui.QPushButton('Dialog', self)
        self.btn.move(20, 20)
        self.btn.clicked.connect(self.showDialog)
        
        self.le = QtGui.QLineEdit(self)
        self.le.move(130, 22)
        
        self.setGeometry(300, 300, 290, 150)
        self.setWindowTitle('Input dialog')
        self.show()
        
    def showDialog(self):
        
        text, ok = QtGui.QInputDialog.getText(self, 'Input Dialog', 
            'Enter your name:')
        
        if ok:
            self.le.setText(str(text))
        
def main():
    
    app = QtGui.QApplication(sys.argv)
    ex = Example()
    sys.exit(app.exec_())


if __name__ == '__main__':
    main()# Import smtplib for the actual sending function
import smtplib
from os.path import basename
import urllib
# Import the email modules we'll need
from email.mime.text import MIMEText
from email.mime.application import MIMEApplication
from email.mime.multipart import MIMEMultipart
from email.mime.image import MIMEImage

# Open a plain text file for reading.  For this example, assume that
# the text file contains only ASCII characters.
fp = open('test_email.txt', 'rb')
# Create a text/plain message
msg = MIMEMultipart()
msg.attach(MIMEText(fp.read()))
fp.close()

me = 'rishabh.bhargava93@gmail.com'
you = 'cmtowle@gmail.com'

msg['Subject'] = 'Cat Digest 1'

msg['From'] = me
msg['To'] = you

urllib.urlretrieve('http://thecatapi.com/api/images/get?format=src&type=jpg', 'the_cat.jpeg')

img_data = open('the_cat.jpeg', 'rb').read()
image = MIMEImage(img_data, name=basename('the_cat.jpeg'))
msg.attach(image)
#with open('this.jpeg', 'rb') as fil:
#	msg.attach(MIMEApplication(fil.read(), Content_Disposition='attachment; filename="%s"' % basename('this.jpeg')))

# Send the message via our own SMTP server, but don't include the
# envelope header.
s = smtplib.SMTP('smtp.gmail.com', 587)
s.ehlo()
s.starttls()
s.ehlo()
s.login('rishabh.bhargava93@gmail.com','fuckbroad')
s.sendmail(me, [you], msg.as_string())
s.quit()import numpy as np


##########################################################################
# X is list of numpy arrays where each array is feature variable (input)
# Y is a list of numbers where each number is a target variable (output)
# alpha is the step size

# This is the batch gradient descent which converges slower than th incremental gradient descent

# returns theta, the linear estimate of the training set
##########################################################################
def least_mean_squares(x, y, alpha):

	x = np.concatenate((np.ones((len(x), 1)), x), axis = 1)

	size_x = x[0].shape[0]
	theta = np.zeros(size_x)

	#error = 0.0001
	iterate = 0 

	while iterate < 10000:
		for j in xrange(size_x):
			thetaj_sum = 0
			for i, point in enumerate(x):
				thetaj_sum += (np.dot(point, theta) - y[i]) * point[j]
			theta[j] -= alpha*thetaj_sum
		iterate+=1

	return theta

x_test = []
x_test.append(np.array([1]))
x_test.append(np.array([2]))
x_test.append(np.array([3]))
x_test.append(np.array([4]))

y_test = [1, 3, 3, 7]

alpha = 0.01

print x_test

print least_mean_squares(x_test, y_test, alpha)def coin_partition(x):
	return ways(x,x)

d = {}

def ways(n,m):
	if (n,m) in d:
		return d[(n,m)]
	else:
		if n<0 or m < 1:
			d[(n,m)] = 0
			return 0
		elif n ==0:
			d[(n,m)] = 1
			return 1
		else:
			d[(n,m)] = ways(n-m, m) + ways(n, m-1)
			return d[(n,m)]

i = 62

while i<1000:
	if coin_partition(i) % 1000000 == 0:
		print i
		break
	i += 1


#for i in xrange(201,1001):
#	print coin_partition(i)

def ways_loop(n):
	d = {}
	for i in xrange(1,n):
		d[(0,i)] = 1

	for i in (0, n+1):
		d[i, 0] = 0

	for i in xrange(1,n):
		for j in xrange(1,n):
			if i - j < 0:
				if j == 1:
					d[(i, j)] = 0
				else:
					d[(i,j)] = d[(i, j-1)]
			else:
				d[(i,j)] = d[(i-j, j)] + d[(i,j-1)]

	print d[(n,n)]

	for i in xrange(1,n):
		if d[(i,i)] % 1000000 == 0:
			print i

	print "done"


ways_loop(5)coins = [200,100,50,20,10,5,2,1]

def ways(n,m):
	if n < 0 or m >= len(coins):
		return 0
	elif n == 0:
		return 1
	else:
		return ways(n-coins[m], m) + ways(n, m+1)

print ways(200,0)def combinations(string):
  if len(string) == 1:
    return ["", string]
  else:
    b = combinations(string[1:])
    return b + [string[0] + x for x in b]

print combinations("absdc")
import sys
    
def print100(a):
  try:
    b = 1/(101-a)
  except ZeroDivisionError:
    sys.exit()
  print a
  print100(a+1)

print100(1)
def median_of_medians(arr, index):
	
	# compute the median of medians using the helper function given below
	mom =  get_median_of_medians(arr)
	
	# partition aroun the median of medians calculated. Thhis sould be done in place, but we have done 
	# this using O(n) space. This is also O(n) time.
	new_arr = []
	for el in arr:
		if el < mom:
			new_arr.append(el)
	k = len(new_arr)
	new_arr.append(mom)
	for el in arr:
		if el > mom:
			new_arr.append(el)

	# Cases we could have
	if k == index:
		return mom
	elif k > index:
		return median_of_medians(new_arr[0:k], index)
	else:
		return median_of_medians(new_arr[k+1: len(new_arr)], index - k)

def get_median_of_medians(arr):

	# Base case: lenght of array is 1
	if len(arr) == 1:
		return arr[0]

	#Otherwise break array into chunks of n/5 and calculate for each o them separately
	else:
		medians = []
		for i in xrange((len(arr)-1)/5 +1):
			if i*5+5 > len(arr):
				b = arr[i*5:len(arr)]
			else:
				b = arr[i*5:i*5+5]

			b.sort()
			medians.append(b[len(b)/2])

		return get_median_of_medians(medians)

# Test case
a = [2,4,7,10,1,3,5,9,8,6]
print a
print median_of_medians(a, 0)# 10001st prime

def nthprime(n):
	if n == 1:
		return 2
	else:
		count = 2
		number = 3
		while True:
			if prime(number):
				count += 1
				if count > n:
					break
			number += 2
		return number

def prime(number):
	if number <= 1:
		return False
	i = 2
	while i <= number** 0.5:
		if number%i == 0:
			return False
		i += 1

	return True

print nthprime(10001)

## 10001st prime is 104743def permutations(string):
	if len(string) == 1:
		return [string]
	else:
		b = []
		for index, char in enumerate(string):
			b += [char + x for x in permutations(string[0:index] + string[index + 1: len(string)])]
		return b

print permutations("abc")dic= {}

def isMatch(s, p):
    global dic
    if (len(s),len(p)) in  dic:
        return dic[(len(s),len(p))]
    if len(s) == 0 and len(p) == 0:
        return True
    if len(s) == 0:
        if p[0] == '*':
            result =  isMatch(s, p[1:])
            dic[(len(s),len(p))] = result
            return result
        else:
            return False
    if len(p) == 0:
        return False
        
    if p[0] == '?':
        result =  isMatch(s[1:], p[1:])
    elif p[0] == '*':
            result =  isMatch(s[1:], p) or  isMatch(s, p[1:])
    else:
        result =  isMatch(s[1:], p[1:]) and s[0] == p[0]
            
    dic[(len(s),len(p))] = result
    return result

print isMatch("ssadd", "*")

def lengthOfLongestSubstring(self, s):
    lengths = []
    dics = []
        
    if s == "":
        return 0
    lengths.append(1)
    dics.append({s[0]: 1}
        
    for i, ch in enumerate(s):
        if ch in dics[i-1]:
            lengths.append(1)
            dics[i] = {ch: 1}
        else:
            lengths.append(length(i-1) + 1)
            dics[i] = dics[i-1]
            dics[i][ch] = 1
        
    return max(lengths)def stairs (n):
	if n <0 :
		return 0
	elif n == 0:
		return 1
	else:
		return stairs(n-1) + stairs(n-2)

print stairs(5)no_tests = int(raw_input())

def combinations(n, r):
    if n < r:
        return 1
    elif r <= 0:
        return 1
    else:
        return combinations(n-1, r-1)*n/r

numbers = []
for i in xrange(no_tests):
    numbers.append(int(raw_input()))
    
for i in xrange(len(numbers)):
    if numbers[i] < 2:
        print 0
    else:
        print combinations(numbers[i], 2)
    
    
import numpy as np
import project1_code as p1
import math

def extract_dictionary(file):
    """
      Given a text file, returns a dictionary of unique words.
      Each line is passed into extract_words, and a list on unique
      words is maintained. 
    """
    dict = []
    
    f = open(file, 'r')
    for line in f:
        flist = p1.extract_words(line)
        
        for word in flist:
            is_new_word = True
            for entry in dict:
            	if word == entry[0]:
            		entry[1]+=1
            		is_new_word = False
            if is_new_word:
            	dict.append([word, 1])

    f.close()

    return dict

def extract_feature_vectors(file, dict): 
    """
      Returns a bag-of-words representation of a text file, given a dictionary.
      The returned matrix is of shape (m, n), where the text file has m non-blank
      lines, and the dictionary has n entries. 
    """
    f = open(file, 'r') 
    num_lines = 0

    for line in f:
    	if(line.strip()):
        	num_lines = num_lines + 1

    f.close()

    feature_matrix = np.zeros([num_lines, len(dict)])

    f = open(file, 'r')
    pos = 0
    
    for line in f:
    	if(line.strip()):
    		flist = p1.extract_words(line)
    		for word in flist: 
    			for entry in dict:
    				if word == entry[0]:
    					feature_matrix[pos, dict.index(entry)] = math.floor(math.log(1+entry[1]))
    		pos = pos + 1
            
    f.close()
    
    return feature_matrix

dictionary = extract_dictionary('train-tweet.txt')
labels = p1.read_vector_file('train-answer.txt')
feature_matrix = extract_feature_vectors('train-tweet.txt', dictionary)

theta = p1.perceptron(feature_matrix, labels)
theta_0 = theta[len(theta)-1]
theta = np.delete(theta, len(theta)-1)
label_output = p1.perceptron_classify(feature_matrix, theta_0, theta)

correct = 0
for i in xrange(0, len(label_output)):
    if(label_output[i] == labels[i]):
        correct = correct + 1

percentage_correct = 100.0 * correct / len(label_output)
print("Augmented perceptron gets " + str(percentage_correct) + "% correct (" + str(correct) + " out of " + str(len(label_output)) + ").")

test = p1.cross_validation_perceptron(feature_matrix, labels)
print test

#returned 508import project1_code as p1
import numpy as np



dictionary = p1.extract_dictionary('train-tweet.txt')
labels = p1.read_vector_file('train-answer.txt')
feature_matrix = p1.extract_feature_vectors('train-tweet.txt', dictionary)
#print feature_matrix.shape

#test = p1.cross_validation_perceptron(feature_matrix, labels)
#print test
#returned 519

test = p1.cross_validator(feature_matrix, labels, True)
print test

from string import punctuation, digits
import numpy as np
import matplotlib.pyplot as plt

def extract_words(input_string):
    """
      Returns a list of lowercase words in a strong.
      Punctuation and digits are separated out into their own words.
    """


    for c in punctuation.replace('@', "") + digits :
        input_string = input_string.replace(c, "")

    splitted_string = input_string.lower().split()

    return [x for x in splitted_string if not (x.startswith("http") or x.startswith("@"))]

def extract_dictionary(file):
    """
      Given a text file, returns a dictionary of unique words.
      Each line is passed into extract_words, and a list on unique
      words is maintained. 
    """
    dict = []
    
    f = open(file, 'r')
    for line in f:
        flist = extract_words(line)
        
        for word in flist:
            if(word not in dict):
                dict.append(word)

    f.close()

    return dict

def extract_feature_vectors(file, dict):
    """
      Returns a bag-of-words representation of a text file, given a dictionary.
      The returned matrix is of shape (m, n), where the text file has m non-blank
      lines, and the dictionary has n entries. 
    """
    f = open(file, 'r')
    num_lines = 0

    for line in f:
        if(line.strip()):
            num_lines = num_lines + 1

    f.close()

    feature_matrix = np.zeros([num_lines, len(dict)])

    f = open(file, 'r')
    pos = 0
    
    for line in f:
        if(line.strip()):
            flist = extract_words(line)
            for word in flist:
                if(word in dict):
                    feature_matrix[pos, dict.index(word)] = 1
            pos = pos + 1
            
    f.close()
    
    return feature_matrix

def averager(feature_matrix, labels):
    """
      Implements a very simple classifier that averages the feature vectors multiplied by the labels.
      Inputs are an (m, n) matrix (m data points and n features) and a length m label vector. 
      Returns a length-n theta vector (theta_0 is 0). 
    """
    (nsamples, nfeatures) = feature_matrix.shape
    theta_vector = np.zeros([nfeatures])
    theta_0 = 0

    for i in xrange(0, nsamples):
        label = labels[i]
        sample_vector = feature_matrix[i, :]
        theta_vector = theta_vector + label*sample_vector
        theta_0 +=label
    
    #print theta_0, "is theta_0"
    theta_vector = np.append(theta_vector, theta_0)
    return theta_vector

def read_vector_file(fname):
    """
      Reads and returns a vector from a file. 
    """
    return np.genfromtxt(fname)

def perceptron_classify(feature_matrix, theta_0, theta_vector):
    """
      Classifies a set of data points given a weight vector and offset.
      Inputs are an (m, n) matrix of input vectors (m data points and n features),
      a real number offset, and a length n parameter vector.
      Returns a length m label vector. 
    """
    (nsamples, nfeatures) = feature_matrix.shape
    label_output = np.zeros([nsamples])
    
    
    for i in xrange(0, nsamples):   
        sample_features = feature_matrix[i, :]
        perceptron_output = theta_0 + np.dot(theta_vector, sample_features)

        if(perceptron_output > 0):
            label_output[i] = 1
        else:
            label_output[i] = -1

    return label_output
    
def perceptron(feature_matrix, labels):
    (nsamples, nfeatures) = feature_matrix.shape
    theta = np.zeros([nfeatures])
    theta_0 = 0
    solved = False
    while not solved:
        flag = True
        for i in xrange(0, nsamples):
            label = labels[i]
            sample_features = feature_matrix[i,:]
            if (np.dot(theta, sample_features) + theta_0)*label <= 0 :
                theta = theta + label*sample_features
                theta_0 = theta_0 + label
                flag = False
        if flag:
            solved = True
    #print theta_0, "is the new theta0"
    theta = np.append(theta, [theta_0])
    return theta
    
def passive_agressive(feature_matrix, labels):
    (nsamples, nfeatures) = feature_matrix.shape
    theta = np.zeros([nfeatures])
    solved = False
    while not solved :
        old_theta = theta
        count = 0
        for i in xrange(0, nsamples):
            label = labels[i]
            sample_features = feature_matrix[i,:]
            loss = 0
            if np.dot(theta, sample_features) * label <=1:
                loss = 1- np.dot(theta, sample_features) * label
                count = 0
            eta = loss/((np.linalg.norm(sample_features))**2)
            theta = theta + sample_features*eta*label
            count+=1
        if np.array_equal(theta, old_theta) or count == nsamples:
            solved = True  
    return theta    

def chunks(l, n):
    return [l[i:i+n] for i in range(0, len(l), n)]
def cross_validator(feature_matrix, labels, isPerceptron):
    """
    perceptron is true if you want to cross_validate perceptron
    """
    foldAmount = 10
    foldedMatrix = np.array_split(feature_matrix,foldAmount)
    foldedLabels = chunks(labels,len(labels)/foldAmount)
    #print foldedLabels
    correct = 0

    for i in xrange(foldAmount):
        #print str(i) + "out of" + str(foldAmount-1)
        unfoldedMatrix = None
        unfoldedLabels = None
        for k in xrange(foldAmount):
            if k != i:
                if(unfoldedMatrix == None):
                    unfoldedMatrix = foldedMatrix[k]
                else:
                    unfoldedMatrix = np.vstack((unfoldedMatrix, foldedMatrix[k]))
                if(unfoldedLabels == None):
                    unfoldedLabels = foldedLabels[k]
                else: 
                    unfoldedLabels = np.vstack((unfoldedLabels,foldedLabels[k]))
        unfoldedLabels = np.ravel(unfoldedLabels)
        foldPart = foldedMatrix[i]
        foldPartLabels = foldedLabels[i]
        thetaList = None
        #print unfoldedMatrix
        #print unfoldedLabels
        if(isPerceptron):
            thetaList = perceptron(unfoldedMatrix,unfoldedLabels)
            theta_0 = thetaList[len(thetaList)-1]
            thetaList = np.delete(thetaList, len(thetaList)-1)
        else:
            thetaList = passive_agressive(unfoldedMatrix,unfoldedLabels)
            theta_0 = 0
        #print thetaList.shape
        #print foldPart.shape

        label_output = perceptron_classify(foldPart, theta_0,thetaList)

        for j in xrange(0, len(label_output)):
            if(label_output[j] == labels[j]):
                correct = correct + 1

    return correct
    
def cross_validation_perceptron(feature_matrix, labels):
    (nsamples, nfeatures) = feature_matrix.shape
    count = 0
    for i in xrange(0, nsamples):
        label = labels[i]
        labels1 = np.delete(labels, i)
        sample_feature = feature_matrix[i, :]
        feature_matrix1 = np.delete(feature_matrix, i,0)
        
        theta = perceptron(feature_matrix1, labels1)
        theta_0 = theta[len(theta)-1]
        theta = np.delete(theta, len(theta)-1)
        
        output = 1 if (np.dot(theta, np.transpose(sample_feature))+theta_0)>0 else -1
        if output == label:
            count+=1
    return count

def cross_validation_passive_agressive(feature_matrix, labels):
    (nsamples, nfeatures) = feature_matrix.shape
    count = 0
    for i in xrange(0, nsamples):
        #print(i,"out of",nsamples)
        label = labels[i]
        labels1 = np.delete(labels, i)
        sample_feature = feature_matrix[i, :]
        feature_matrix1 = np.delete(feature_matrix, i,0)
        
        theta = passive_agressive(feature_matrix1, labels1)
        
        output = 1 if np.dot(theta, np.transpose(sample_feature))>0 else -1
        #print output
        if output == label:
            count+=1
    return count

def write_label_answer(vec, outfile):
    """
      Outputs your label vector the a given file.
      The vector must be of shape (70, ) or (70, 1),
      i.e., 70 rows, or 70 rows and 1 column.
    """
    
    if(vec.shape[0] != 180):
        print("Error - output vector should have 182 rows.")
        print(vec.shape[0])
        print("Aborting write.")
        return

    for v in vec:
        if((v != -1.0) and (v != 1.0)):
            print("Invalid value in input vector.")
            print("Aborting write.")
            return
        
    np.savetxt(outfile, vec)
        

def plot_2d_examples(feature_matrix, labels, theta_0, theta):
    """
      Uses Matplotlib to plot a set of labeled instances, and
      a decision boundary line.
      Inputs: an (m, 2) feature_matrix (m data points each with
      2 features), a length-m label vector, and hyper-plane
      parameters theta_0 and length-2 vector theta. 
    """
    
    cols = []
    xs = []
    ys = []
    
    for i in xrange(0, len(labels)):
        if(labels[i] == 1):
            cols.append('b')
        else:
            cols.append('r')
        xs.append(feature_matrix[i][0])
        ys.append(feature_matrix[i][1])

    plt.scatter(xs, ys, s=40, c=cols)

    [xmin, xmax, ymin, ymax] = plt.axis()

    linex = []
    liney = []
    for x in np.linspace(xmin, xmax):
        linex.append(x)
        if(theta[1] != 0.0):
            y = (-theta_0 - theta[0]*x) / (theta[1])
            liney.append(y)
        else:
            liney.append(0)
    
    plt.plot(linex, liney, 'k-')

    plt.show()
import numpy as np
import project1_code as p1

dictionary = p1.extract_dictionary('train-tweet.txt')
labels = p1.read_vector_file('train-answer.txt')
feature_matrix = p1.extract_feature_vectors('train-tweet.txt', dictionary)
feature_matrix_real = p1.extract_feature_vectors('sample_from_tweepy.txt', dictionary)


average_without_offset_theta = p1.averager(feature_matrix, labels)
theta_0 = average_without_offset_theta[len(average_without_offset_theta)-1]
average_without_offset_theta = np.delete(average_without_offset_theta, len(average_without_offset_theta)-1)

label_output = p1.perceptron_classify(feature_matrix, 0, average_without_offset_theta)

correct = 0
for i in xrange(0, len(label_output)):
    if(label_output[i] == labels[i]):
        correct = correct + 1

percentage_correct = 100.0 * correct / len(label_output)
print("Averager without offset gets " + str(percentage_correct) + "% correct (" + str(correct) + " out of " + str(len(label_output)) + ").")


average_theta = p1.averager(feature_matrix, labels)
theta_0 = average_theta[len(average_theta)-1]
average_theta = np.delete(average_theta, len(average_theta)-1)

label_output = p1.perceptron_classify(feature_matrix, theta_0, average_theta)

correct = 0
for i in xrange(0, len(label_output)):
    if(label_output[i] == labels[i]):
        correct = correct + 1

percentage_correct = 100.0 * correct / len(label_output)
print("Averager gets " + str(percentage_correct) + "% correct (" + str(correct) + " out of " + str(len(label_output)) + ").")


perceptron_theta = p1.perceptron(feature_matrix, labels)
theta_0 = perceptron_theta[len(perceptron_theta)-1]
perceptron_theta = np.delete(perceptron_theta, len(perceptron_theta)-1)

label_output = p1.perceptron_classify(feature_matrix, theta_0, perceptron_theta)
real_label_output = p1.perceptron_classify(feature_matrix_real, theta_0,perceptron_theta)
p1.write_label_answer(real_label_output,"sample_from_tweepy_answer.txt")
correct = 0
for i in xrange(0, len(label_output)):
    if(label_output[i] == labels[i]):
        correct = correct + 1

percentage_correct = 100.0 * correct / len(label_output)
print("Perceptron gets " + str(percentage_correct) + "% correct (" + str(correct) + " out of " + str(len(label_output)) + ").")



passive_agressive_theta = p1.passive_agressive(feature_matrix, labels)

label_output = p1.perceptron_classify(feature_matrix, 0, passive_agressive_theta)

correct = 0
for i in xrange(0, len(label_output)):
    if(label_output[i] == labels[i]):
        correct = correct + 1

percentage_correct = 100.0 * correct / len(label_output)
print("Passive Agressive gets " + str(percentage_correct) + "% correct (" + str(correct) + " out of " + str(len(label_output)) + ").")

import project1_code as p1
from string import punctuation, digits


def extract_words(input_string):
    """
      Returns a list of lowercase words in a strong.
      Punctuation and digits are separated out into their own words.
    """


    for c in punctuation.replace('@', "") + digits :
        input_string = input_string.replace(c, "")

    print input_string
    splitted_string = input_string.lower().split()

    return [x for x in splitted_string if not (x.startswith("http") or x.startswith("@"))]

print extract_words("Lego Movie' Blocks Rivals to Lead Box Office @jhsdffor Third Weekend - Bloomberg http://t.co/YgpH0ZX3Nc")import tweepy
import time
from getpass import getpass
from textwrap import TextWrapper
import project1_code as p1

consumer_key = '5N6WnewfTr4TXiMNe7qqA'
consumer_secret = 'sF7duztDHCwtH3eeCsuAqQEp8GzqpnJp7S9UyndpKgA'
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)

access_token = '2321411072-yifWz3g7X3pleFo5WAz7F2oBie6a4U6CDojDtZJ'
access_secret = 'VlE0NzNaob6qCYbcLJNg6D93LSi4seX9eD7GSyEdmeYWb'

auth.set_access_token(access_token, access_secret)


api = tweepy.API(auth)

#user = api.me()

#print api.friends_timeline()

class StreamWatcherListener(tweepy.StreamListener):

    status_wrapper = TextWrapper(width=60, initial_indent='    ', subsequent_indent='    ')

    def on_status(self, status):
        try:
            print self.status_wrapper.fill(status.text)
            print '\n %s  %s  via %s\n' % (status.author.screen_name, status.created_at, status.source)
            
            with open("sample_from_tweepy_v2.txt", "a") as file:
                file.write((status.text).replace('\n', '') + '\n')
        except:
            # Catch any unicode errors while printing to console
            # and just ignore them to avoid breaking application.
            pass

    def on_error(self, status_code):
        print 'An error has occured! Status code = %s' % status_code
        return True  # keep stream alive

    def on_timeout(self):
        print 'Snoozing Zzzzzz'



def main():
    # Prompt for login credentials and setup stream object
    stream = tweepy.Stream(auth, StreamWatcherListener(), timeout=10)

    # Prompt for mode of streaming
    valid_modes = ['sample', 'filter']
    while True:
        mode = raw_input('Mode? [sample/filter] ')
        if mode in valid_modes:
            break
        print 'Invalid mode! Try again.'

    if mode == 'sample':
        stream.sample()

    elif mode == 'filter':
        follow_list = raw_input('Users to follow (comma separated): ').strip()
        track_list = raw_input('Keywords to track (comma seperated): ').strip()
        if follow_list:
            follow_list = [u for u in follow_list.split(',')]
        else:
            follow_list = None
        if track_list:
            track_list = [k for k in track_list.split(',')]
        else:
            track_list = None

        stream.filter(follow_list, track_list)


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print '\nGoodbye!'import sys
import urllib
import urllib2
import oauth2

CONSUMER_KEY = "ASNpbiETl5cZeFnDGUWBdQ"
CONSUMER_SECRET = "WjYkFOjVgKiUAlwY_Ro63xgKmBs"
TOKEN = "oPh1ypflLnyPnAWz8W3A9icubsI5pCW-"
TOKEN_SECRET = "laH_-CA8ceh8nQhX2fsR2LfUjfY"

#url_params = {
 #       'term': term.replace(' ', '+'),
  #      'location': location.replace(' ', '+'),
   #     'limit': SEARCH_LIMIT
    #}

API_HOST = 'api.yelp.com'
SEARCH_PATH = '/v2/search/'

import numpy as np
import scipy.io
import os.path

for file_name in [a_file for a_file in sorted(os.listdir('./')) if a_file[-4:] == '.dat']:
    print 'Processing %s' % file_name
    data = np.genfromtxt(file_name, delimiter=',')
    X = data[:,:-1]
    y = data[:,-1]
    max_locations = y==max(y)
    min_locations = y==min(y)
    y[max_locations] = 1
    y[min_locations] = -1
    save_file_name = file_name[:-4] + '.mat'
    scipy.io.savemat(save_file_name, {'X' : X, 'y' : y})
import numpy as np
import scipy.io
import os.path

for file_name in [a_file for a_file in sorted(os.listdir('./')) if a_file[-4:] == '.dat']:
    print 'Processing %s' % file_name
    # Read file
    with open(file_name, 'r') as input_file:
        text = input_file.readlines()
    # Remove header
    text = [line for line in text if not line[0] == '@']
    # Convert labels into numbers
    IO_dict = {}
    new_text = []
    counter = 0
    for line in text:
        last_symbol = line.split(',')[-1]
        if not last_symbol in IO_dict:
            IO_dict[last_symbol] = '%d\n' % counter
            counter += 1
        new_text.append(','.join(line.split(',')[:-1] + [IO_dict[last_symbol]]))
    # Write to file
    with open(file_name, 'w') as output_file:
        output_file.write(''.join(new_text))
import numpy as np
import scipy.io

X = np.genfromtxt('arcene_train.data', delimiter=' ')
y = np.genfromtxt('arcene_train.labels', delimiter=' ')
scipy.io.savemat('arcene.mat', {'X' : X, 'y' : y})
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_biclusters
from sklearn.datasets import samples_generator as sg
from sklearn.cluster.bicluster import SpectralCoclustering
from sklearn.metrics import consensus_score
import os.path

my_file = open('../results/class/default/summary.csv', 'r')
data = []
for line in my_file:
	data.append([float(x) for x in line.split(',')])

data = np.array(data)
print data.shape
plt.imshow(np.array(data), cmap=plt.cm.Blues)
plt.title("Original dataset")

model = SpectralCoclustering(n_clusters=8, random_state=0)
model.fit(data)

fit_data = data[np.argsort(model.row_labels_)]
fit_data = fit_data[:, np.argsort(model.column_labels_)]

plt.matshow(fit_data, cmap=plt.cm.Blues)
plt.title("After biclustering; rearranged to show biclusters")

plt.show()

#TODO: Read the paper again, PCA, '''
Routines to evaluate results

James Robert Lloyd 2013
'''

import os
import numpy as np
import math
from sklearn.mixture import GMM, VBGMM, DPGMM
from sklearn.decomposition import PCA, FactorAnalysis

from pylab import *
from scipy import *

default_dir = '../results/class/default/'

#### Utilities

def permutation_indices(data):
    return sorted(range(len(data)), key = data.__getitem__)
    
def pretty_scatter(x, y, color, radii, labels):
    for i in range(len(x)):
        text(x[i], y[i], labels[i], size=8, horizontalalignment='center')
    sct = scatter(x, y, c=color, s=radii, linewidths=1, edgecolor='w')
    sct.set_alpha(0.75)

#### Interface

def create_csv_summary(results_dir):
    # Loop over model folders
    method_descriptions = [adir for adir in sorted(os.listdir(results_dir)) if os.path.isdir(os.path.join(results_dir, adir)) and not (adir.startswith('GBM 100')) and not (adir.startswith('GBM 300'))]# and not (adir.startswith('Kurt'))]
    data_names = []
    data_dictionary = {method_description : {} for method_description in method_descriptions}
    for method_description in method_descriptions:
        print 'Reading %s' % method_description
        data_names = sorted(list(set(data_names + [os.path.splitext(file_name)[0] for file_name in [full_path for full_path in sorted(os.listdir(os.path.join(results_dir, method_description))) if full_path[-6:] == '.score']])))# and not (full_path.startswith('rand'))]])))
        for data_name in [file_name for file_name in sorted(os.listdir(os.path.join(results_dir, method_description))) if file_name[-6:] == '.score']:
            with open(os.path.join(results_dir, method_description, data_name), 'rb') as score_file:
                score = float(score_file.read())
            data_dictionary[method_description][os.path.splitext(data_name)[0]] = score
    # Create array
    print 'Creating array'
    data_array = -0.01 * np.ones((len(method_descriptions), len(data_names)))
    for (i, method_description) in enumerate(method_descriptions):
        for (j, data_name) in enumerate(data_names):
            if (method_description in data_dictionary) and (data_name in data_dictionary[method_description]):
                data_array[i, j] = data_dictionary[method_description][data_name]
            else:
                data_array[i, j] = np.NAN
    print 'Saving array'
    np.savetxt(os.path.join(results_dir, 'summary_complete_ish.csv'), data_array, delimiter=',')
    with open(os.path.join(results_dir, 'methods_complete_ish.csv'), 'w') as save_file:
        save_file.write('\n'.join(method_descriptions))
    with open(os.path.join(results_dir, 'datasets_complete_ish.csv'), 'w') as save_file:
        save_file.write('\n'.join(data_names))
    return data_array
    
def plot_ordered_array(results_dir):
    # Load array
    data_array = np.genfromtxt(os.path.join(results_dir, 'summary.csv'), delimiter=',')
    # Mask the NANs
    mdat = np.ma.masked_array(data_array,np.isnan(data_array))
    # Display with ordered rows and columns
    imshow(data_array[permutation_indices(list(np.mean(mdat, axis=1).data))][:,permutation_indices(list(np.mean(mdat, axis=0).data))])
    show()
    
def save_GPLVM_data(results_dir):
    # Load array
    data_array = np.transpose(np.genfromtxt(os.path.join(results_dir, 'summary.csv'), delimiter=','))
    # Setup GPLVM
    (N, D) = data_array.shape
    Q = 2 # Latent dimensionality
    k = GPy.kern.rbf(Q, ARD=True) + GPy.kern.white(Q, 0.00001) #### TODO - is this good usage?
    # Fit model
    #m = GPy.models.Bayesian_GPLVM(Y=data_array, Q=Q, init='PCA', kernel = k, M=n_pseudo_points)
    m = GPy.models.GPLVM(Y=data_array, Q=Q, init='PCA', kernel = k)
    m.ensure_default_constraints()
    m.optimize_restarts(robust=True)
    # Save fit
    np.savetxt(os.path.join(results_dir, 'GPLVM-datasets-2.csv'), m.X, delimiter=',')

def save_PCA_data(results_dir):
	# Load array
	data_array = np.transpose(np.genfromtxt(os.path.join(results_dir,'summary.csv'),delimiter=','))
	print np.isnan(data_array)
	data_array = np.ma.masked_array(data_array,np.isnan(data_array))
	print np.isnan(data_array)
	#print data_array
	(N,D) = data_array.shape
	print "number of datasets %d, number of methods %d" %(N,D)
	pca = PCA(n_components = 2)
	x_new = pca.fit_transform(data_array)
	print (pca.explained_variance_ratio_)
	print x_new.shape
	np.savetxt(os.path.join(results_dir, 'PCA-datasets-4.csv'), x_new, delimiter=',')

def save_PCA_method(results_dir):
	# Load array
	data_array = np.genfromtxt(os.path.join(results_dir,'summary.csv'),delimiter=',')
	(N,D) = data_array.shape
	print "number of methods %d, number of datasets %d" %(N,D)
	pca = PCA(n_components = 2)
	x_new = pca.fit_transform(data_array)
	print (pca.explained_variance_ratio_)
	print x_new.shape
	np.savetxt(os.path.join(results_dir,'PCA-methods-4.csv'), x_new, delimiter=',')
    
def plot_GPLVM_data(results_dir, method_index=0):
    # Load relevant datasets
    data_array = np.genfromtxt(os.path.join(results_dir, 'summary.csv'), delimiter=',')
    X = (np.genfromtxt(os.path.join(results_dir, 'GPLVM-datasets-2.csv'), delimiter=','))
    datasets = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'datasets.csv'), 'r').readlines()]
    methods = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'methods.csv'), 'r').readlines()]
    # Plot
    clf()
    pretty_scatter(X[:,0], X[:,1], data_array[method_index,:], 200*np.ones(X[:,0].shape), datasets)
    xlabel('Dimension 1')
    ylabel('Dimension 2')
    title('Performance under %s' % methods[method_index])
    colorbar()
    show()

def plot_PCA_data(results_dir, method_index=[26,36,13,43]):
	# Load relevant datasets
	data_array = np.genfromtxt(os.path.join(results_dir,'summary.csv'),delimiter=',')
	X = (np.genfromtxt(os.path.join(results_dir, 'PCA-datasets-4.csv'), delimiter=','))
	datasets = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'datasets.csv'), 'r').readlines()]
	methods = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'methods.csv'), 'r').readlines()]
	# Plot
	figure()
	for i in xrange(len(method_index)):
		subplot(len(method_index)/2, len(method_index)/2, i)
		pretty_scatter(X[:,0], X[:,1], data_array[method_index[i]-1,:], 200*np.ones(X[:,0].shape), ['' for d in datasets])
		xlabel('Dimension 1')
		ylabel('Dimension 2')
		title('Performance under %s' % methods[method_index[i]-1])
		colorbar()
	show()

def plot_PCA_method_data(results_dir, dataset_index =[25]):
	data_array = np.transpose(np.genfromtxt(os.path.join(results_dir,'summary.csv'),delimiter=','))
	X = (np.genfromtxt(os.path.join(results_dir, 'PCA-methods-4.csv'), delimiter=','))
	datasets = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'datasets.csv'), 'r').readlines()]
	methods = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'methods.csv'), 'r').readlines()]
	figure()
	for i in xrange(len(dataset_index)):
		subplot(1, 1, i)
		pretty_scatter(X[:,0], X[:,1], data_array[dataset_index[i]-1,:], 200*np.ones(X[:,0].shape), methods)
		xlabel('Dimension 1')
		ylabel('Dimension 2')
		title('Performance under %s' % datasets[dataset_index[i]-1])
		colorbar()
	show()
    
def plot_GPLVM_data_cluster(results_dir, n_clusters=None, VB=False):
    # Load relevant datasets
    data_array = np.genfromtxt(os.path.join(results_dir, 'summary.csv'), delimiter=',')
    X = (np.genfromtxt(os.path.join(results_dir, 'GPLVM-datasets-2.csv'), delimiter=','))
    datasets = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'datasets.csv'), 'r').readlines()]
    methods = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'methods.csv'), 'r').readlines()]
    # Fit a mixture model
    if n_clusters is None:
        m = DPGMM()
    elif VB:
        m = VBGMM(alpha = 10, n_components=n_clusters)
    else:
        m = GMM(n_components=n_clusters, n_init=100)
    m.fit(data_array.T)
    clusters = m.predict(data_array.T)
    # Plot
    #clf()
    figure(1)
    pretty_scatter(X[:,0], X[:,1], clusters, 200*np.ones(X[:,0].shape), datasets)
    xlabel('Dimension 1')
    ylabel('Dimension 2')
    if n_clusters is None:
        title('CRP MoG')
    elif VB:
        title('%d clusters with VB' % n_clusters)
    else:
        title('%d clusters with EM' % n_clusters)
    show()
    
def produce_co_clustering_table(results_dir=default_dir, model_clusters=6, data_clusters=6):
    # Load relevant datasets
    data_array = np.genfromtxt(os.path.join(results_dir, 'summary.csv'), delimiter=',')
    X = (np.genfromtxt(os.path.join(results_dir, 'GPLVM-datasets-2.csv'), delimiter=','))
    datasets = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'datasets.csv'), 'r').readlines()]
    methods = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'methods.csv'), 'r').readlines()]
    # Fit mixture models
    m_model = GMM(n_components=model_clusters, n_init=20)
    m_model.fit(data_array)
    model_C = m_model.predict(data_array)
    m_data = GMM(n_components=data_clusters, n_init=20)
    m_data.fit(data_array.T)
    data_C = m_data.predict(data_array.T)
    # Produce table
    performance_table = np.zeros((model_clusters, data_clusters))
    for model_cluster in range(model_clusters):
        # print model clusters
        print 'Model cluster %d' % (model_cluster + 1)
        for (i, method) in enumerate(methods):
            if model_C[i] == model_cluster:
                print method
        for data_cluster in range(data_clusters):
            performance_table[model_cluster, data_cluster] = np.mean(data_array[model_C==model_cluster][:,data_C==data_cluster])
    # Print table
    print performance_table

def factor_analysis(results_dir):
	data_array = np.transpose(np.genfromtxt(os.path.join(results_dir,'summary.csv'),delimiter=','))
	fa = FactorAnalysis(n_components = 2)
	new_array = fa.fit_transform(data_array)
	print fa.get_covariance().shape
	print new_array
	np.savetxt(os.path.join(results_dir,'FA-datasets-2.csv'), new_array, delimiter=',')

def factor_analyses(results_dir):
	data_array = np.genfromtxt(os.path.join(results_dir,'summary.csv'),delimiter=',')
	fa1 = FactorAnalysis(n_components = 1)
	new_array_gbm = fa1.fit_transform(np.transpose(data_array[range(15)]))
	print new_array_gbm.shape
	fa2 = FactorAnalysis(n_components = 1)
	new_array_tree = fa2.fit_transform(np.transpose(data_array[range(41,51) + range(54,64)]))
	print new_array_tree.shape

	fa3 = FactorAnalysis(n_components = 1)
	new_array_lin = fa3.fit_transform(np.transpose(data_array[range(27,41) + range(51,54)]))

	fa4 = FactorAnalysis(n_components = 1)
	new_array_knn = fa4.fit_transform(np.transpose(data_array[range(16,27)]))

	datasets = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'datasets.csv'), 'r').readlines()]
	methods = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'methods.csv'), 'r').readlines()]
	figure()
	pretty_scatter(new_array_tree, [1 for x in range(115)], data_array[46], 200*np.ones(new_array_tree.shape), ['' for d in datasets])
	xlabel('Dimension 1')
	ylabel('Arbitrary Dimension 2')
	colorbar()

	figure()

	plot(new_array_lin, new_array_tree, 'bo')
	xlabel('Linear')
	ylabel('Tree + RF')

	figure()
	subplot(2,2,1)
	scatter(new_array_gbm, new_array_tree)
	xlabel('GBM')
	ylabel('Tree + RF')

	#figure()
	subplot(2,2,2)
	scatter(new_array_knn, new_array_tree)
	xlabel('KNN')
	ylabel('Tree + RF')

	#figure()
	subplot(2,2,3)
	scatter(new_array_knn, new_array_lin)
	xlabel('KNN')
	ylabel('Linear')

	subplot(2,2,4)
	scatter(new_array_gbm, new_array_lin)
	xlabel('GBM')
	ylabel('Linear')
	show()

def plot_FA_data(results_dir, method_index=1):
	data_array = np.genfromtxt(os.path.join(results_dir,'summary.csv'),delimiter=',')
	X = (np.genfromtxt(os.path.join(results_dir, 'FA-datasets-2.csv'), delimiter=','))
	datasets = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'datasets.csv'), 'r').readlines()]
	methods = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'methods.csv'), 'r').readlines()]

	figure()
	pretty_scatter(X[:,0], X[:,1], data_array[method_index-1,:], 200*np.ones(X[:,0].shape), datasets)
	xlabel('Dimension 1')
	ylabel('Dimension 2')
	title('Performance under %s' % methods[method_index-1])
	colorbar()
	show()

def get_statistics(results_dir):
	data_array = np.genfromtxt(os.path.join(results_dir,'summary.csv'),delimiter=',')
	datasets = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'datasets.csv'), 'r').readlines()]
	methods = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'methods.csv'), 'r').readlines()]
	nr = []
	nc = []
	for dataset in datasets:
		with open(results_dir + 'Num_rows/' + dataset + '.score', 'r') as rowfile:
			r = float(rowfile.read())
			#if r > 10000:
			#	print dataset
			nr.append(r)

		with open(results_dir + 'Num_colms/' + dataset + '.score', 'r') as colfile:
			c = float(colfile.read())
			#if c > 1000:
			#	c = 10
			nc.append(c)


	means = np.mean(data_array,axis=0)
	figure()
	pretty_scatter([math.log10(n) for n in nr], [math.log10(nn) for nn in nc], means, 200*np.ones(len(nc)), ['' for d in datasets])
	colorbar()
	xlabel('Number of rows in dataset (log)')
	ylabel('Number of columns in dataset (log)')
	show()


def exp_stats(results_dir):
	data_array = np.genfromtxt(os.path.join(results_dir,'summary_complete_ish.csv'),delimiter=',')
	datasets = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'datasets_complete_ish.csv'), 'r').readlines()]
	methods = [line.rstrip('\n') for line in open(os.path.join(results_dir, 'methods_complete_ish.csv'), 'r').readlines()]

	means_datasets = np.mean(data_array,axis=0)
	means_methods = np.mean(data_array,axis=1)

	with open(os.path.join(results_dir, 'avg_datasets_score.csv'), 'w') as save_file:
		for i,d in enumerate(datasets):
			save_file.write(d + ',' + str(means_datasets[i]) + '\n')

	with open(os.path.join(results_dir, 'avg_methods_score.csv'), 'w') as save_file:
		for i,d in enumerate(methods):
			save_file.write(d + ',' + str(means_methods[i]) + '\n')

#create_csv_summary(default_dir)
#plot_ordered_array(default_dir)
#print permutation_indices([3,4,0,1,2])

#factor_analysis(default_dir)
#plot_FA_data(default_dir)

#factor_analyses(default_dir)

#plot_GPLVM_data_cluster('../results/class/without_gbm/',method_index)
#save_PCA_data(default_dir)

#create_csv_summary(default_dir)
get_statistics(default_dir)
#exp_stats(default_dir)
#plot_PCA_data(default_dir, [41, 50, 53, 57])
#show()
#save_PCA_method(default_dir)
#plot_PCA_method_data(default_dir)

#method_index=[10,16,23,32]
#plot_PCA_data(default_dir, method_index)
#method_index=[6,16,26,36]

#plot_PCA_data(default_dir,method_index)
#method_index = [71,78,26,89]
#plot_PCA_data(default_dir,method_index)
import os
import shutil
import numpy as np
import scipy
import csv


path = '../data/class/raw/automatic_statistician/' + os.listdir('../data/class/raw/automatic_statistician/')[0]
print path

def load_dictionary(path):
	print path
	try:
		data_array = np.loadtxt(path, delimiter = ',', dtype = float)
	except ValueError:
		try:
			data_array = np.loadtxt(path, delimiter = ',', dtype = float, skiprows = 1)
		except ValueError:
			print path
	print data_array.shape
	print data_array

load_dictionary(path)
'''
Routines to coordinate model x dataset evaluation

James Robert Lloyd 2013
'''

#### TODO

# - Do I want to standardise inputs or might I risk destroying structure?
#   - If so, empirical quantiles or zero mean and standard deviation?

import numpy as np
import os.path
import time
import psutil
from multiprocessing import Process
from utils.pyroc import AUC
from utils.data import load_dictionary, split_into_folds, all_data_files, standardise_inputs, standardise_outputs
import models, properties

#### Utilities

def exp_param_defaults(exp_params={}):
    '''Sets all missing parameters to their default values'''
    defaults = {'methods' : models.list_of_classifiers,
                'properties' : properties.list_of_properties,
                'data_dir' : '../data/class',
                'sleep_time' : 2, # Sleep time between experiments, to prevent cloud communication bottlenecks
                'save_dir' : '../results/class/default',
                'multithread' : False,
                'overwrite' : False,
                'max_job_time' : 1,
                'max_cpu_percent' : 80
                }
    # Iterate through default key-value pairs, setting all unset keys
    for key, value in defaults.iteritems():
        if not key in exp_params:
            exp_params[key] = value
    return exp_params
    
def exp_params_to_str(exp_params):
    result = "Running experiment:\n"
    for key, value in exp_params.iteritems():
        result += "%s = %s,\n" % (key, value)
    return result
    
#### Experiment coordination

def evaluate_and_save(method, is_method, data_file, save_file_name):
    data = split_into_folds(standardise_outputs(standardise_inputs(load_dictionary(data_file))))
    #print data
    if is_method:
        score = np.mean([AUC(method.predict_p(fold['X_train'], fold['y_train'], fold['X_test']), fold['y_test']) for fold in data['folds']])
    else:
        score = np.mean([method.get_stat(fold['X_train']) for fold in data['folds']])
    save_file_dir = os.path.split(save_file_name)[0]
    if not os.path.isdir(save_file_dir):
        os.makedirs(save_file_dir)
    with open(save_file_name, 'w') as save_file:
        save_file.write('%s' % score)
    
def evaluate_all(exp_params):
    job_ids = []
    for data_file in sorted(all_data_files(exp_params['data_dir'])):
        #data = split_into_folds(standardise_inputs(load_dictionary(data_file)))
        data_name = os.path.splitext(os.path.basename(data_file))[0]
        for method in exp_params['methods']:
            save_file_name = os.path.join(exp_params['save_dir'], method.description(), data_name + '.score')
            if exp_params['overwrite'] or (not os.path.isfile(save_file_name)):
                while psutil.cpu_percent() > exp_params['max_cpu_percent']:
                    time.sleep(10)
                print 'Running %s %s' % (data_name, method.description())
                if exp_params['multithread']:
                    print "This is wrong"
                    #job_ids.append(cloud.mp.call(evaluate_and_save, method, data_file, save_file_name, _max_runtime=exp_params['max_job_time']))
                else:
                    evaluate_and_save(method, True, data_file, save_file_name)
                #time.sleep(exp_params['sleep_time'])
            else:
                print 'Skipping %s %s' % (data_name, method.description()) 
        for prop in exp_params['properties']:
            save_file_name = os.path.join(exp_params['save_dir'], prop.description(), data_name + '.score')
            if exp_params['overwrite'] or (not os.path.isfile(save_file_name)):
                while psutil.cpu_percent() > exp_params['max_cpu_percent']:
                    time.sleep(10)
                print "Running %s %s" % (data_name, prop.description())
                evaluate_and_save(prop, False, data_file, save_file_name)
            else:
                print "Skipping %s %s" % (data_name, prop.description())
    if exp_params['multithread']:
        print 'Waiting for all jobs to complete'
        #cloud.mp.join(job_ids, ignore_errors=True)
    print 'Finished'
            
#### Interface
            
def test():
    evaluate_all(exp_param_defaults({'methods' : models.list_of_classifiers, 'overwrite': False}))
            
def try_all_datasets():
    evaluate_all(exp_param_defaults({'methods' : [models.GaussianNaiveBayes_c()], 'sleep_time' : 0, 'multithread' : False}))

try_all_datasets()import scipy.io as sp
import numpy as np
import os

def create_random_dataset(path, dims, n):
	d = {}
	d['X'] = np.random.rand(n, dims)
	d['y'] = [2**(round(x)) - 1 for x in np.random.rand(n,1)]
	print d
	file_name = 'random' + '_' + str(n) + '_' + str(dims) + '.mat'
	sp.savemat(file_name, d)

create_random_dataset(0, 10,101)
'''
Simple interfaces to models

James Robert Lloyd 2013
Rishabh Bhargava 2014
'''

from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import LinearSVC, SVC

import numpy as np

class GaussianNaiveBayes_c():

    def __init__(self):
        self._model = GaussianNB()

    def description(self):
        return 'GNB'
        
    def predict_p(self, X_train, y_train, X_test):
        return self._model.fit(X_train, y_train).predict_proba(X_test)[:,-1]  
        
class Tree_c():

    def __init__(self, min_samples_leaf=5, criterion='gini'):
        self.min_samples_leaf = min_samples_leaf
        self.criterion = criterion
        self._model = DecisionTreeClassifier(min_samples_leaf=self.min_samples_leaf, criterion=self.criterion)

    def description(self):
        return 'Tree %s %s' % (self.min_samples_leaf, self.criterion)

    def predict_p(self, X_train, y_train, X_test): 
        return self._model.fit(X_train, y_train).predict_proba(X_test)[:,-1]   
        
class RandomForest_c():

    def __init__(self, n_tree=500, criterion='gini'):
        self.n_tree = n_tree
        self.criterion = criterion
        self._model = RandomForestClassifier(n_estimators=self.n_tree, criterion=self.criterion)

    def description(self):
        return 'RF %s %s' % (self.n_tree, self.criterion)

    def predict_p(self, X_train, y_train, X_test): 
        return self._model.fit(X_train, y_train).predict_proba(X_test)[:,-1]
        
class GBM_c():

    def __init__(self, n_estimators=500, learn_rate=0.1, max_depth=3):
        self.n_estimators = n_estimators
        self.learn_rate = learn_rate
        self.max_depth = max_depth
        self._model = GradientBoostingClassifier(loss='deviance', n_estimators=n_estimators, learning_rate=learn_rate, max_depth=max_depth)

    def description(self):
        return 'GBM %s %s %s' % (self.n_estimators, self.learn_rate, self.max_depth)

    def predict_p(self, X_train, y_train, X_test): 
        return self._model.fit(X_train, y_train).predict_proba(X_test)[:,-1]

class LogisticRegression_c():

    def __init__(self, C=None, penalty=None):
        self.C = C
        self.penalty = penalty
        self._model = LogisticRegression(C=C, penalty=penalty)

    def description(self):
        return 'LR %s %s' % (self.penalty, self.C)

    def predict_p(self, X_train, y_train, X_test): 
        return self._model.fit(X_train, y_train).predict_proba(X_test)[:,-1]

class SGD_c():

    def __init__(self, loss=None, penalty=None):
        self.loss = loss
        self.penalty = penalty
        self._model = SGDClassifier(loss=loss, penalty=penalty)

    def description(self):
        return 'SGD %s %s' % (self.penalty, self.loss)

    def predict_p(self, X_train, y_train, X_test): 
        return self._model.fit(X_train, y_train).predict_proba(X_test)[:,-1]

class Linear_SVM_c():

    def __init__(self, loss='l2', penalty='l2'):
        self.loss = loss
        self.penalty = penalty
        self._model = LinearSVC(penalty=penalty, loss=loss)

    def description(self):
        return 'Linear SVM %s %s' %(self.penalty, self.loss)

    def predict_p(self, X_train, y_train, X_test):
        return self._model.fit(X_train, y_train).predict(X_test)

class KNN_c():

    def __init__(self, k=5):
        self.k = k
        self._model = KNeighborsClassifier(n_neighbors=k)

    def description(self):
        return 'KNN %s' % (self.k)

    def predict_p(self, X_train, y_train, X_test): 
        self._model.fit(X_train, y_train)
        # Compute empirical probabilities
        return np.array([np.mean(y_train[self._model.kneighbors(X_test[i,:])[1]]==1) for i in range(X_test.shape[0])])

class SVM_c():

    def __init__(self, C = 1.0, kernel = 'rbf', degree = 3):
        self.C = C
        self.kernel = kernel
        self.degree = degree
        self.model = SVC(C=C, kernel = kernel, degree = degree)

    def description(self):
        return 'SVC %s %s %s' %(self.C, self.kernel, self.degree)

    def predict_p(self, X_train, y_train, X_test):
        self.model.fit(X_train, y_train).predict(X_test)

#################################
# KNN 50, 75, 100 removed for now
#################################

list_of_classifiers = [GaussianNaiveBayes_c()] + \
                      [KNN_c(k) for k in [4,8,12,16,20,24,28,32]] + \
                      [LogisticRegression_c(C=C, penalty=penalty) for C in [0.001, 0.01, 0.1, 1, 10, 100] for penalty in ['l1', 'l2']] + \
                      [SGD_c(loss=loss, penalty=penalty) for loss in ['log'] for penalty in ['l1', 'l2', 'elasticnet']] + \
                      [RandomForest_c(n_tree, criterion) for n_tree in [100,200,300,400,500] for criterion in ['gini', 'entropy']] + \
                      [Tree_c(min_samples_leaf, criterion) for min_samples_leaf in [1,5,10,20,50] for criterion in ['gini', 'entropy']] + \
                      [GBM_c(n_estimators, learn_rate, max_depth) for n_estimators in [100,300,500] for learn_rate in [0.0001,0.001,0.01,0.1,1] for max_depth in [1,3,5]] + \
                      [Linear_SVM_c(loss=loss, penalty='l2') for loss in ['l1', 'l2']]

GBM_classifiers = [GBM_c(n_estimators, learn_rate, max_depth) for n_estimators in [100,300,500] for learn_rate in [0.0001,0.001,0.01,0.1,1] for max_depth in [1,3,5]]
LSVM_classifiers = [Linear_SVM_c(loss=loss, penalty='l2') for loss in ['l1', 'l2']]
SVM_classifiers = [SVM_c(C=C, kernel = kernel) for C in [1,0.1,0.01] for kernel in ['rbf', 'sigmoid']] + [SVM_c(C=C, kernel = 'poly', degree= degree) for C in [1,0.1] for degree in [2,3,5]]

#print SVM_classifiers
#[GBM_c(n_estimators, learn_rate, max_depth) for n_estimators in [10,25,50] for learn_rate in [0.0001,0.01,1] for max_depth in [1,2]]"""
Some properties of datasets

Rishabh Bhargava 2014
"""

import numpy as np

from scipy.stats import kurtosis

class Kurtosis_p():

	def __init__(self, agg_method='avg'):
		self.agg_method = agg_method

	def description(self):
		return "Kurtosis %s" % self.agg_method

	def get_stat(self, X_train):
		if self.agg_method == 'avg':
			return np.mean([kurtosis(X_train[:,i]) for i in range(len(X_train[0]))])
		else:
			return max([kurtosis(X_train[:,i]) for i in range(len(X_train[0]))])

class Num_rows():

	def description(self):
		return "Num_rows"

	def get_stat(self, X_train):
		return len(X_train)

class Num_columns():

	def description(self):
		return "Num_colms"

	def get_stat(self, X_train):
		return len(X_train[0])

list_of_properties = [Kurtosis_p(agg) for agg in ['avg', 'max']] +[Num_rows(), Num_columns()]from pylab import * 

x = [900, 1200, 700, 1500, 850, 1410, 1000, 990,1150, 759, 815,800, 1400,1300, 1200, 1050, 1100]
y = [330, 575, 220, 800, 300, 745, 380, 360, 550, 260, 290, 289, 760,720, 600,450,500]

fit = polyfit(x,y,1)
fit_fn = poly1d(fit)
plot(x,y, 'ro', x, fit_fn(x), '-k')

xlabel('Size (area) of a London home in square feet')
ylabel('Cost of a London home in multiples of 1000 GBP')

xlim(680, 1550)
ylim(180, 830)

show()import os
from pylab import *
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn. tree import DecisionTreeClassifier
from utils.data import split_into_folds
from utils.pyroc import AUC
from models import *

'''
This method creates a k dimensional object in an n dimensional space and returns points on it
'''
def create_plane(n, k):
	b = np.random.rand(k,1)
	pars = np.random.rand(100,k)
	a = np.random.rand(n,k)
	points = []
	for i in xrange(100):
		temp = np.zeros(n)
		for j in xrange(k):
			temp = np.add(pars[i][j]*a[:,j], temp)
		points.append(temp)
	return points

'''
This method creates a k dimensional object in an n dimensional space and returns p points around it with correct labels
'''
def yield_points(n,k,p):
	a = np.ones(n)
	#print a
	order = np.random.permutation(n)[:k]
	X = []
	y = []
	A = np.random.rand(n,k)
	#A = np.ones((n,k))
	for i in xrange(p):
		# x = [round()(2*np.random.rand(n) - 1)[order]]
		x = [1 if np.random.rand() > 0.5 else -1 for _ in range(n)]
		temp = np.asarray([np.dot(A[j,:], x) for j in range(n)])
		if len(X) == 0:
			X = temp
		else:
			X = np.vstack((X, temp))
		#print X
		#temp = x
		#type(temp)
		if np.random.rand() < 0.1:
			temp = -temp
		if np.dot(temp,a) > 0:
			y = np.append(y, 1)
		else:
			y = np.append(y, -1)
	return (X,y)


'''
test = create_plane(2,1)
x = [a[0] for a in test]
y = [a[1] for a in test]
'''
##X,y=  yield_points(2,1,10)
#X,y = yield_points(3000,20,2000)
#print X
#print reduce(lambda x,y: x+ y, filter(lambda x: x > 0, y))
'''
for i in xrange(len(y)):
	if y[i] == 1:
		temp = 'o'
	else:
		temp = 'x'
	scatter(X[i][0], X[i][1], marker=temp)


show()
'''

def get_the_plots(tups):
	ratios = []
	rf_scores = []
	lr_scores = []
	tree_scores = []
	for i in xrange(len(tups)):
		n,k,p = tups[i]
		X,y = yield_points(n,k,p)
		d = {}
		d['X'] = X
		d['y'] = y
		data = split_into_folds(d)
		rfmodel = RandomForest_c(10, 'gini')
		score = np.mean([AUC(rfmodel.predict_p(fold['X_train'], fold['y_train'], fold['X_test']), fold['y_test']) for fold in data['folds']])
		rf_scores.append(score)
		print score

		logregmodel = LogisticRegression_c(C=0.1, penalty='l2')
		score = np.mean([AUC(logregmodel.predict_p(fold['X_train'], fold['y_train'], fold['X_test']), fold['y_test']) for fold in data['folds']])
		lr_scores.append(score)
		print score

		dtmodel = Tree_c(10, 'gini')
		score = np.mean([AUC(dtmodel.predict_p(fold['X_train'], fold['y_train'], fold['X_test']), fold['y_test']) for fold in data['folds']])
		tree_scores.append(score)
		print score

		ratios.append(p)
	line1, = plot(ratios,rf_scores,'r', label='RF')
	line1, = plot(ratios, lr_scores, 'g', label='LR')
	line1, = plot(ratios, tree_scores, 'k', label='Tree')
	legend()
	xlabel('Number of points in the dataset')
	ylabel('Performance score')
	show()


get_the_plots([(1000, 1000, 10),(1000, 1000, 20)] + [(1000,1000,21+2*x) for x in range(4)] + [(1000,1000,20+10*(x+1)) for x in range(8)])

#get_the_plots([(100,100,5+ 2*(x+1)) for x in range(30)])

'''d = {}
d['X'] = X
d['y'] = y
data = split_into_folds(d)
rfmodel = RandomForest_c(10, 'gini')
score = np.mean([AUC(rfmodel.predict_p(fold['X_train'], fold['y_train'], fold['X_test']), fold['y_test']) for fold in data['folds']])
print score

logregmodel = LogisticRegression_c(C=0.1, penalty='l2')
score = np.mean([AUC(logregmodel.predict_p(fold['X_train'], fold['y_train'], fold['X_test']), fold['y_test']) for fold in data['folds']])
print score
''''''Routines for I/O and creating data files'''

import os
import scipy.io
import numpy as np
import pickle

#### I/O

def load_dictionary(file_name):
    '''Loads either a .mat or .pickle'''
    extension = os.path.splitext(file_name)[-1]
    if extension == '.mat':
        return scipy.io.loadmat(file_name, squeeze_me=True)
    elif extension == '.pickle':
        pickle_file = open(file_name, 'rb')
        data = pickle.load(pickle_file)
        pickle_file.close()
        return data
    else:
        raise Exception('Unrecognised data file extension')
        
def all_data_files(data_dir):
    """Produces list of all .mat, .pickle and .csv files in a directory - returns absolute paths"""
    return [os.path.join(data_dir, file_name) for file_name in os.listdir(data_dir) if (file_name[-4:] == '.mat') or (file_name[-7:] == '.pickle') or (file_name[-4:] == '.csv')]
        
#### Processing

def split_into_folds(data, n=5, seed=0):
    np.random.seed(seed)
    perm = np.random.permutation(range(data['X'].shape[0]))
    
    folds = []
    for fold in range(n):
        if fold == 0:
            train_i = perm[0:np.floor((n-1)*len(perm)/n)]
            test_i  = perm[np.floor((n-1)*len(perm)/n):]
            folds.append({'X_train' : data['X'][train_i,], 'y_train' : data['y'][train_i], 'X_test' : data['X'][test_i,], 'y_test' : data['y'][test_i]})
        elif fold == n-1:
            train_i = perm[np.floor(len(perm)/n):]
            test_i  = perm[:np.floor(len(perm)/n)]
            folds.append({'X_train' : data['X'][train_i,], 'y_train' : data['y'][train_i], 'X_test' : data['X'][test_i,], 'y_test' : data['y'][test_i]})
        else:
            train_i = list(perm[:np.floor(fold*len(perm)/n)]) + list(perm[np.floor((fold+1)*len(perm)/n):])
            test_i  = perm[np.floor(fold*len(perm)/n):np.floor((fold+1)*len(perm)/n)]
            folds.append({'X_train' : data['X'][train_i,], 'y_train' : data['y'][train_i], 'X_test' : data['X'][test_i,], 'y_test' : data['y'][test_i]})
            
    return {'folds' : folds}
    
def standardise_inputs(data):
    #### TODO
    # - Do I even want this function
    #   - Should I return empirical quantiles, or should it just apply a linear transformation?
    return data
    
def standardise_outputs(data):
    max_locations = data['y'] == max(data['y'])
    min_locations = data['y'] == min(data['y'])
    data['y'][max_locations] = 1
    data['y'][min_locations] = -1
    return data
#!/usr/bin/env python
# encoding: utf-8
"""
PyRoc.py

Created by Marcel Caraciolo on 2009-11-16.
Copyright (c) 2009 Federal University of Pernambuco. All rights reserved.

IMPORTANT:
Based on the original code by Eithon Cadag (http://www.eithoncadag.com/files/pyroc.txt)

Python Module for calculating the area under the receive operating characteristic curve, given a dataset.

0.1  - First Release
0.2 - Updated the code by adding new metrics for analysis with the confusion matrix.

Some convenience functions added by James Robert Lloyd 2013
Small edit by Rishabh Bhargava Nov, 2014

"""

import random
import math
try:
	import pylab
except:
	print "error:\tcan't import pylab module, you must install the module:\n"
	print "\tmatplotlib to plot charts!'\n"

#### Start James Robert Lloyd convenience functions

def AUC(predictions, targets):
    '''Just computes the AUC'''
    # Standardise targets
    max_locations = targets==max(targets)
    min_locations = targets==min(targets)
    targets[max_locations] = 1
    targets[min_locations] = 0
    #print targets
    #print predictions
    return ROCData(zip(targets, predictions)).auc()
    
#### End James Robert Lloyd convenience functions

def random_mixture_model(pos_mu=.6,pos_sigma=.1,neg_mu=.4,neg_sigma=.1,size=200):
	pos = [(1,random.gauss(pos_mu,pos_sigma),) for x in xrange(size/2)]
	neg = [(0,random.gauss(neg_mu,neg_sigma),) for x in xrange(size/2)]
	return pos+neg


def plot_multiple_rocs_separate(rocList,title='', labels = None, equal_aspect = True):
	""" Plot multiples ROC curves as separate at the same painting area. """
	pylab.clf()
	pylab.title(title)
	for ix, r in enumerate(rocList):
		ax = pylab.subplot(4,4,ix+1)
		pylab.ylim((0,1))
		pylab.xlim((0,1))
		ax.set_yticklabels([])
		ax.set_xticklabels([])
		if equal_aspect:
			cax = pylab.gca()
			cax.set_aspect('equal')
		
		if not labels:
			labels = ['' for x in rocList]
		
		pylab.text(0.2,0.1,labels[ix],fontsize=8)
		pylab.plot([x[0] for x in r.derived_points],[y[1] for y in r.derived_points], 'r-',linewidth=2)
	
	pylab.show()
	


def _remove_duplicate_styles(rocList):
 	""" Checks for duplicate linestyles and replaces duplicates with a random one."""
	pref_styles = ['cx-','mx-','yx-','gx-','bx-','rx-']
	points = 'ov^>+xd'
	colors = 'bgrcmy'
	lines = ['-','-.',':']
	
	rand_ls = []
	
	for r in rocList:
		if r.linestyle not in rand_ls:
			rand_ls.append(r.linestyle)
		else:
			while True:
				if len(pref_styles) > 0:
					pstyle = pref_styles.pop()
					if pstyle not in rand_ls:
						r.linestyle = pstyle
						rand_ls.append(pstyle)
						break
				else:
					ls = ''.join(random.sample(colors,1) + random.sample(points,1)+ random.sample(lines,1))
					if ls not in rand_ls:
						r.linestyle = ls
						rand_ls.append(ls)
						break
						


def plot_multiple_roc(rocList,title='',labels=None, include_baseline=False, equal_aspect=True):
	""" Plots multiple ROC curves on the same chart. 
		Parameters:
			rocList: the list of ROCData objects
			title: The tile of the chart
			labels: The labels of each ROC curve
			include_baseline: if it's  True include the random baseline
			equal_aspect: keep equal aspect for all roc curves
	"""
	pylab.clf()
	pylab.ylim((0,1))
	pylab.xlim((0,1))
	pylab.xticks(pylab.arange(0,1.1,.1))
	pylab.yticks(pylab.arange(0,1.1,.1))
	pylab.grid(True)
	if equal_aspect:
		cax = pylab.gca()
		cax.set_aspect('equal')
	pylab.xlabel("1 - Specificity")
	pylab.ylabel("Sensitivity")
	pylab.title(title)
	if not labels:
		labels = [ '' for x in rocList]
	_remove_duplicate_styles(rocList)
	for ix, r in enumerate(rocList):
		pylab.plot([x[0] for x in r.derived_points], [y[1] for y in r.derived_points], r.linestyle, linewidth=1, label=labels[ix])
	if include_baseline:
		pylab.plot([0.0,1.0], [0.0, 1.0], 'k-', label= 'random')
	if labels:
		pylab.legend(loc='lower right')
		
	pylab.show()


def load_decision_function(path):
	""" Function to load the decision function (DataSet) 
		Parameters:
			path: The dataset file path
		Return:
			model_data: The data modeled
	"""
	fileHandler = open(path,'r')
	reader = fileHandler.readlines()
	reader = [line.strip().split() for line in reader]
	model_data = []
	for line in reader:
		if len(line) == 0: continue
		fClass,fValue = line
		model_data.append((int(fClass), float(fValue)))
	fileHandler.close()

	return model_data
	

class ROCData(object):
	""" Class that generates an ROC Curve for the data.
		Data is in the following format: a list l of tutples t
		where:
			t[0] = 1 for positive class and t[0] = 0 for negative class
			t[1] = score
			t[2] = label
	"""
	def __init__(self,data,linestyle='rx-'):
		""" Constructor takes the data and the line style for plotting the ROC Curve.
			Parameters:
				data: The data a listl of tuples t (l = [t_0,t_1,...t_n]) where:
					  t[0] = 1 for positive class and 0 for negative class
					  t[1] = a score
			 		  t[2] = any label (optional)
				lineStyle: THe matplotlib style string for plots.
				
			Note: The ROCData is still usable w/o matplotlib. The AUC is still available, 
			      but plots cannot be generated.
		"""
		self.data = sorted(data,lambda x,y: cmp(y[1],x[1]))
		self.linestyle = linestyle
		self.auc() #Seed initial points with default full ROC
	
	def auc(self,fpnum=0):
		""" Uses the trapezoidal rule to calculate the area under the curve. If fpnum is supplied, it will 
			calculate a partial AUC, up to the number of false positives in fpnum (the partial AUC is scaled
			to between 0 and 1). 		
			It assumes that the positive class is expected to have the higher of the scores (s(+) < s(-))
			Parameters:
				fpnum: The cumulative FP count (fps)
			Return:
			
		"""
		fps_count = 0
		relevant_pauc = []
		current_index = 0
		max_n = len([x for x in self.data if x[0] == 0])
		if fpnum == 0:
			relevant_pauc = [x for x in self.data]
		elif fpnum > max_n:
			fpnum = max_n
		#Find the upper limit of the data that does not exceed n FPs
		else:
			while fps_count < fpnum:
				relevant_pauc.append(self.data[current_index])
				if self.data[current_index][0] == 0:
					fps_count += 1
				current_index +=1
		total_n = len([x for x in relevant_pauc if x[0] == 0])
		total_p = len(relevant_pauc) - total_n
		
		#Convert to points in a ROC
		previous_df = -1000000.0
		current_index = 0
		points = []
		tp_count, fp_count = 0.0 , 0.0
		tpr, fpr = 0, 0
		while current_index < len(relevant_pauc):
			df = relevant_pauc[current_index][1]
			if previous_df != df:
				points.append((fpr,tpr,fp_count))
			if relevant_pauc[current_index][0] == 0:
				fp_count +=1
			elif relevant_pauc[current_index][0] == 1:
				tp_count +=1
			fpr = fp_count/total_n
			if total_p == 0:
				tpr = 0
			else:
				tpr = tp_count/total_p
			previous_df = df
			current_index +=1
		points.append((fpr,tpr,fp_count)) #Add last point
		points.sort(key=lambda i: (i[0],i[1]))
		self.derived_points = points
		
		return self._trapezoidal_rule(points)


	def _trapezoidal_rule(self,curve_pts):
		""" Method to calculate the area under the ROC curve"""
		cum_area = 0.0
		for ix,x in enumerate(curve_pts[0:-1]):
			cur_pt = x
			next_pt = curve_pts[ix+1]
			cum_area += ((cur_pt[1]+next_pt[1])/2.0) * (next_pt[0]-cur_pt[0])
		return cum_area
		
	def calculateStandardError(self,fpnum=0):
		""" Returns the standard error associated with the curve.
			Parameters:
				fpnum: The cumulativr FP count (fps)
			Return:
				the standard error.
		"""
		area = self.auc(fpnum)
		
		#real positive cases
		Na =  len([ x for x in self.data if x[0] == 1])
		
		#real negative cases
		Nn =  len([ x for x in self.data if x[0] == 0])
		
		
		Q1 = area / (2.0 - area)
		Q2 = 2 * area * area / (1.0 + area)
		
		return math.sqrt( ( area * (1.0 - area)  +   (Na - 1.0) * (Q1 - area*area) +
						(Nn - 1.0) * (Q2 - area * area)) / (Na * Nn))
							
	
	def plot(self,title='',include_baseline=False,equal_aspect=True):
		""" Method that generates a plot of the ROC curve 
			Parameters:
				title: Title of the chart
				include_baseline: Add the baseline plot line if it's True
				equal_aspect: Aspects to be equal for all plot
		"""
		
		pylab.clf()
		pylab.plot([x[0] for x in self.derived_points], [y[1] for y in self.derived_points], self.linestyle)
		if include_baseline:
			pylab.plot([0.0,1.0], [0.0,1.0],'k-.')
		pylab.ylim((0,1))
		pylab.xlim((0,1))
		pylab.xticks(pylab.arange(0,1.1,.1))
		pylab.yticks(pylab.arange(0,1.1,.1))
		pylab.grid(True)
		if equal_aspect:
			cax = pylab.gca()
			cax.set_aspect('equal')
		pylab.xlabel('1 - Specificity')
		pylab.ylabel('Sensitivity')
		pylab.title(title)
		
		pylab.show()
		
	
	def confusion_matrix(self,threshold,do_print=False):
		""" Returns the confusion matrix (in dictionary form) for a given threshold
			where all elements > threshold are considered 1 , all else 0.
			Parameters:
				threshold: threshold to check the decision function
				do_print:  if it's True show the confusion matrix in the screen
			Return:
				the dictionary with the TP, FP, FN, TN
		"""
		pos_points = [x for x in self.data if x[1] >= threshold]
		neg_points = [x for x in self.data if x[1] < threshold]
		tp,fp,fn,tn = self._calculate_counts(pos_points,neg_points)
		if do_print:
			print "\t Actual class"
			print "\t+(1)\t-(0)"
			print "+(1)\t%i\t%i\tPredicted" % (tp,fp)
			print "-(0)\t%i\t%i\tclass" % (fn,tn)
		return {'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn}
		

	
	def evaluateMetrics(self,matrix,metric=None,do_print=False):
		""" Returns the metrics evaluated from the confusion matrix.
			Parameters:
				matrix: the confusion matrix
				metric: the specific metric of the default value is None (all metrics).
				do_print:  if it's True show the metrics in the screen
			Return:
				the dictionary with the Accuracy, Sensitivity, Specificity,Efficiency,
				                        PositivePredictiveValue, NegativePredictiveValue, PhiCoefficient
		"""
		
		accuracy = (matrix['TP'] + matrix['TN'])/ float(sum(matrix.values()))
		
		sensitivity = (matrix['TP'])/ float(matrix['TP'] + matrix['FN'])
		
		specificity = (matrix['TN'])/float(matrix['TN'] + matrix['FP'])
		
		efficiency = (sensitivity + specificity) / 2.0
		
		positivePredictiveValue =  matrix['TP'] / float(matrix['TP'] + matrix['FP'])

		NegativePredictiveValue = matrix['TN'] / float(matrix['TN'] + matrix['FN'])
		
		PhiCoefficient = (matrix['TP'] * matrix['TN'] - matrix['FP'] * matrix['FN'])/(
							math.sqrt( (matrix['TP'] + matrix['FP']) *
							           (matrix['TP'] + matrix['FN']) *
									   (matrix['TN'] + matrix['FP']) *
									   (matrix['TN'] + matrix['FN']))) or 1.0
									
		if do_print:
			print 'Sensitivity: ' , sensitivity
			print 'Specificity: ' , specificity
			print 'Efficiency: ' , efficiency
			print 'Accuracy: ' , accuracy
			print 'PositivePredictiveValue: ' , positivePredictiveValue
			print 'NegativePredictiveValue' , NegativePredictiveValue
			print 'PhiCoefficient' , PhiCoefficient
			
		
		return {'SENS': sensitivity, 'SPEC': specificity, 'ACC': accuracy, 'EFF': efficiency,
				'PPV':positivePredictiveValue, 'NPV':NegativePredictiveValue , 'PHI':  PhiCoefficient}


	def _calculate_counts(self,pos_data,neg_data):
		""" Calculates the number of false positives, true positives, false negatives and true negatives """
		tp_count = len([x for x in pos_data if x[0] == 1])
		fp_count = len([x for x in pos_data if x[0] == 0])
		fn_count = len([x for x in neg_data if x[0] == 1])
		tn_count = len([x for x in neg_data if x[0] == 0])
		return tp_count,fp_count,fn_count, tn_count
		

		
if __name__ == '__main__':
	print "PyRoC - ROC Curve Generator"
	print "By Marcel Pinheiro Caraciolo (@marcelcaraciolo)"
	print "http://aimotion.bogspot.com\n"
	from optparse import OptionParser
	
	parser = OptionParser()
	parser.add_option('-f', '--file', dest='origFile', help="Path to a file with the class and decision function. The first column of each row is the class, and the second the decision score.")
	parser.add_option("-n", "--max fp", dest = "fp_n", default=0, help= "Maximum false positives to calculate up to (for partial AUC).")
	parser.add_option("-p","--plot", action="store_true",dest='plotFlag', default=False, help="Plot the ROC curve (matplotlib required)")
	parser.add_option("-t",'--title', dest= 'ptitle' , default='' , help = 'Title of plot.')
	
	(options,args) = parser.parse_args()


	if (not options.origFile):
		parser.print_help()
		exit()

	df_data = load_decision_function(options.origFile)
	roc = ROCData(df_data)
	roc_n = int(options.fp_n)
	print "ROC AUC: %s" % (str(roc.auc(roc_n)),)
	print 'Standard Error:  %s' % (str(roc.calculateStandardError(roc_n)),) 
	
	print ''
	for pt in roc.derived_points:
		print pt[0],pt[1]
		
	if options.plotFlag:
		roc.plot(options.ptitle,True,True)
		
"""
Basic demo showing how to instantiate a simple GP model, add data to it, and
optimize its hyperparameters.
"""

# global imports.
import os
import numpy as np
import matplotlib.pyplot as pl

# local imports
import pygp
import pygp.plotting as pp


if __name__ == '__main__':
    # load the data.
    cdir = os.path.abspath(os.path.dirname(__file__))
    data = np.load(os.path.join(cdir, 'xy.npz'))
    X = data['X']
    y = data['y']

    # create the model, add data, and optimize it.
    gp = pygp.BasicGP(sn=.1, sf=1, ell=.1, mu=0)
    gp.add_data(X, y)
    pygp.optimize(gp)

    # plot the posterior.
    pl.figure(1)
    pl.clf()
    pp.plot_posterior(gp)
    pl.legend(loc=2)
    pl.draw()
    pl.show()
"""
Interface to GP inference.
"""

# import the basic things by default
from . import inference
from . import kernels
from . import learning
from . import likelihoods
from . import meta
from . import priors

# import the basic things by default
from .inference import BasicGP
from .learning import optimize

# and make them available.
__all__ = ['BasicGP', 'optimize']
"""
Objects which implement GP inference.
"""

# pylint: disable=wildcard-import
from .exact import *
from .fitc import *
from .basic import *
from .dtc import *

from . import exact
from . import fitc
from . import basic
from . import dtc

__all__ = []
__all__ += exact.__all__
__all__ += fitc.__all__
__all__ += basic.__all__
__all__ += dtc.__all__
"""
Interface for latent function inference in Gaussian process models. These
models will assume that the hyperparameters are fixed and any optimization
and/or sampling of these parameters will be left to a higher-level wrapper.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import scipy.linalg as sla

# local imports
from ..utils.abc import abstractmethod
from ..utils.models import Parameterized, dot_params
from ..utils.random import rstate
from ._fourier import FourierSample

# exported symbols
__all__ = ['GP']


class GP(Parameterized):
    """
    GP inference interface.

    This class defines the GP interface. Although it implements the
    Parameterized interface it defines additional abstract methods and is still
    abstract as a result.

    The methods that must be implemented are:

        `_update`: update internal statistics given all the data.
        `_posterior`: compute the full posterior for use with sampling.
        `posterior`: compute the marginal posterior and its gradient.
        `loglikelihood`: compute the loglikelihood of observed data.

    Additionally, the following method can be implemented for improved
    performance in some circumstances:

        `_updateinc`: incremental update given new data.
    """
    def __init__(self, likelihood, kernel, mean):
        self._likelihood = likelihood
        self._kernel = kernel
        self._mean = float(mean)
        self._X = None
        self._y = None

        # record the number of hyperparameters. the additional +1 is due to the
        # mean hyperparameter.
        self.nhyper = (self._likelihood.nhyper +
                       self._kernel.nhyper + 1)

    def reset(self):
        """Remove all data from the model."""
        self._X = None
        self._y = None

    def __repr__(self):
        def indent(pre, text):
            return pre + ('\n' + ' '*len(pre)).join(text.splitlines())

        return indent(
            self.__class__.__name__ + '(',
            ',\n'.join([
                indent('likelihood=', repr(self._likelihood)),
                indent('kernel=', repr(self._kernel)),
                indent('mean=', str(self._mean))]) + ')')

    def _params(self):
        params = dot_params('like', self._likelihood._params())
        params += dot_params('kern', self._kernel._params())
        params += [('mean', 1, False)]
        return params

    @classmethod
    def from_gp(cls, gp, *args, **kwargs):
        """
        Create a new GP object given another. This allows one to make a "copy"
        of a GP using the same likelihood, kernel, etc. and using the same
        data, but possibly a different inference method.
        """
        args = (gp._likelihood.copy(), gp._kernel.copy(), gp._mean) + args
        newgp = cls(*args, **kwargs)
        if gp.ndata > 0:
            X, y = gp.data
            newgp.add_data(X, y)
        return newgp

    def get_hyper(self):
        # NOTE: if subclasses define any "inference" hyperparameters they can
        # implement their own get/set methods and call super().
        return np.r_[self._likelihood.get_hyper(),
                     self._kernel.get_hyper(),
                     self._mean]

    def set_hyper(self, hyper):
        # FIXME: should set_hyper check the number of hyperparameters?
        a = self._likelihood.nhyper
        b = self._kernel.nhyper

        self._likelihood.set_hyper(hyper[:a])
        self._kernel.set_hyper(hyper[a:a+b])
        self._mean = hyper[-1]

        if self.ndata > 0:
            self._update()

    @property
    def ndata(self):
        """The number of current input/output data pairs."""
        return 0 if (self._X is None) else self._X.shape[0]

    @property
    def data(self):
        """The current input/output data."""
        return (self._X, self._y)

    def add_data(self, X, y):
        """
        Add new data to the GP model.
        """
        X = self._kernel.transform(X)
        y = self._likelihood.transform(y)

        if self._X is None:
            self._X = X.copy()
            self._y = y.copy()
            self._update()

        else:
            try:
                self._updateinc(X, y)
                self._X = np.r_[self._X, X]
                self._y = np.r_[self._y, y]

            except NotImplementedError:
                self._X = np.r_[self._X, X]
                self._y = np.r_[self._y, y]
                self._update()

    def sample(self, X, m=None, latent=True, rng=None):
        """
        Sample values from the posterior at points `X`. Given an `(n,d)`-array
        `X` this will return an `n`-vector corresponding to the resulting
        sample.

        If `m` is not `None` an `(m,n)`-array will be returned instead,
        corresponding to `m` such samples. If `latent` is `False` the sample
        will instead be returned corrupted by the observation noise. Finally
        `rng` can be used to seed the randomness.
        """
        X = self._kernel.transform(X)

        # this boolean indicates whether we'll flatten the sample to return a
        # vector, or if we'll return a set of samples as an array.
        flatten = (m is None)

        # get the relevant sizes.
        m = 1 if flatten else m
        n = len(X)

        # if a seed or instantiated RandomState is given use that, otherwise
        # use the global object.
        rng = rstate(rng)

        # add a tiny amount to the diagonal to make the cholesky of Sigma
        # stable and then add this correlated noise onto mu to get the sample.
        mu, Sigma = self._full_posterior(X)
        Sigma += 1e-10 * np.eye(n)
        f = mu[None] + np.dot(rng.normal(size=(m, n)), sla.cholesky(Sigma))

        if not latent:
            f = self._likelihood.sample(f.ravel(), rng).reshape(m, n)

        return f.ravel() if flatten else f

    def posterior(self, X, grad=False):
        """
        Return the marginal posterior. This should return the mean and variance
        of the given points, and if `grad == True` should return their
        derivatives with respect to the input location as well (i.e. a
        4-tuple).
        """
        return self._marg_posterior(self._kernel.transform(X), grad)

    def sample_fourier(self, N, rng=None):
        """
        Approximately sample a function from the GP using a fourier-basis
        expansion with N bases. See the documentation on `FourierSample` for
        details on the returned function object.
        """
        return FourierSample(N,
                             self._likelihood, self._kernel, self._mean,
                             self._X, self._y, rng)

    @abstractmethod
    def _update(self):
        """
        Update any internal parameters (ie sufficient statistics) given the
        entire set of current data.
        """

    # NOTE: the following method is not abstract since we don't require that it
    # is implemented. if it is not implemented the full _update is performed
    # when new data is added.

    def _updateinc(self, X, y):
        """
        Update any internal parameters given additional data in the form of
        input/output pairs `X` and `y`. This method is called before data is
        appended to the internal data-store and no subsequent call to `_update`
        is performed.
        """
        raise NotImplementedError

    @abstractmethod
    def _full_posterior(self, X):
        """
        Compute the full posterior at points `X`. Return the mean vector and
        full covariance matrix for the given inputs.
        """

    @abstractmethod
    def _marg_posterior(self, X, grad=False):
        """
        Compute the marginal posterior at points `X`. Return the mean and
        variance vectors for the given inputs. If `grad` is True return the
        gradients with respect to the inputs as well.
        """

    @abstractmethod
    def loglikelihood(self, grad=False):
        """
        Return the marginal loglikelihood of the data. If `grad == True` also
        return the gradient with respect to the hyperparameters.
        """
"""
Approximations to the GP using random Fourier features.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import scipy.linalg as sla

# local imports
from ..utils.random import rstate
from ..utils.exceptions import ModelError
from ..likelihoods import Gaussian

# exported symbols
__all__ = ['FourierSample']


class FourierSample(object):
    def __init__(self, N, likelihood, kernel, mean, X, y, rng=None):
        # if given a seed or an instantiated RandomState make sure that we use
        # it here, but also within the sample_spectrum code.
        rng = rstate(rng)

        if not isinstance(likelihood, Gaussian):
            raise ModelError('Fourier samples only defined for Gaussian'
                             'likelihoods')

        # this randomizes the feature.
        W, alpha = kernel.sample_spectrum(N, rng)

        self._W = W
        self._b = rng.rand(N) * 2 * np.pi
        self._a = np.sqrt(2 * alpha / N)
        self._mean = mean
        self._theta = None

        if X is not None:
            Phi = self.phi(X)
            A = np.dot(Phi.T, Phi) + likelihood.s2 * np.eye(Phi.shape[1])
            R = sla.cholesky(A)
            r = y - mean

            # FIXME: we can do a smarter update here when the number of points
            # is less than the number of features.

            rnd = np.sqrt(likelihood.s2) * rng.randn(N)

            self._theta = sla.cho_solve((R, False), np.dot(Phi.T, r))
            self._theta += sla.solve_triangular(R, rnd)

        else:
            self._theta = rng.randn(N)

    def phi(self, X):
        """
        Evaluate the random features.
        """
        # x is n-by-D,
        # W is N-by-D,
        # Phi, the return value, should be n-by-N.
        rnd = np.dot(X, self._W.T) + self._b
        Phi = np.cos(rnd) * self._a
        return Phi

    def get(self, X):
        """
        Evaluate the function at a collection of points.
        """
        Phi = self.phi(np.array(X, ndmin=2, copy=False))
        return self._mean + np.dot(Phi, self._theta)

    def __call__(self, x):
        return self.get(x)[0]
"""
Simple wrapper class for a Basic GP.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# local imports
from ..utils.models import printable
from ..likelihoods import Gaussian
from ..kernels import SE, Matern
from .exact import ExactGP

# exported symbols
__all__ = ['BasicGP']


@printable
class BasicGP(ExactGP):
    """
    Basic GP frontend which assumes an ARD kernel and a Gaussian likelihood
    (and hence performs exact inference).
    """
    def __init__(self, sn, sf, ell, mu=0, ndim=None, kernel='se'):
        likelihood = Gaussian(sn)
        kernel = (
            SE(sf, ell, ndim) if (kernel == 'se') else
            Matern(sf, ell, 1, ndim) if (kernel == 'matern1') else
            Matern(sf, ell, 3, ndim) if (kernel == 'matern3') else
            Matern(sf, ell, 5, ndim) if (kernel == 'matern5') else None)

        if kernel is None:
            raise RuntimeError('Unknown kernel type')

        super(BasicGP, self).__init__(likelihood, kernel, mu)

    def _params(self):
        # replace the parameters for the base GP model with a simplified
        # structure and rename the likelihood's sigma parameter to sn (ie its
        # the sigma corresponding to the noise).
        params = [('sn', 1)]
        params += self._kernel._params()
        params += [('mu', 1, False)]
        return params
"""
Nystrom approximate inference in a Gaussian process model
for regression.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import scipy.linalg as sla
import itertools as it

# local imports
from ..utils.exceptions import ModelError
from ..likelihoods import Gaussian
from ._base import GP

# exported symbols
__all__ = ['DTC']


class DTC(GP):
    """Deterministic training conditional approximation to GP inference."""

    def __init__(self, likelihood, kernel, mean, U):
        # NOTE: exact inference will only work with Gaussian likelihoods.
        if not isinstance(likelihood, Gaussian):
            raise ModelError('exact inference requires a Gaussian likelihood')

        super(DTC, self).__init__(likelihood, kernel, mean)
        # save the pseudo-input locations.
        self._U = np.array(U, ndmin=2, dtype=float, copy=True)

        self._Ruu = None
        self._Rux = None
        self._a = None

    @property
    def pseudoinputs(self):
        """The pseudo-input points."""
        return self._U

    def _update(self):
        p = self._U.shape[0]
        su2 = self._likelihood.s2 * 1e-6

        # choleskies of Kuu and (Kuu + Kfu * Kuf / sn2), respectively,
        # see Eq 20b of (Quinonero-Candela and Rasmussen, 2005)
        Kuu = self._kernel.get(self._U)
        self._Ruu = sla.cholesky(Kuu + su2 * np.eye(p))

        # formulate data-dependent problem
        Kux = self._kernel.get(self._U, self._X)
        S = Kuu + np.dot(Kux, Kux.T) / self._likelihood.s2

        # compute cholesky of data dependent problem
        r = self._y - self._mean
        self._Rux = sla.cholesky(S + su2 * np.eye(p))
        self._a = sla.solve_triangular(self._Rux,
                                       np.dot(Kux, r),
                                       trans=True)

    def _full_posterior(self, X):
        # grab the prior mean and covariance.
        mu = np.full(X.shape[0], self._mean)
        Sigma = self._kernel.get(X)

        if self._X is not None:
            K = self._kernel.get(self._U, X)
            b = sla.solve_triangular(self._Ruu, K, trans=True)
            c = sla.solve_triangular(self._Rux, K, trans=True)

            # add the contribution to the mean coming from the posterior and
            # subtract off the information gained in the posterior from the
            # prior variance.
            mu += np.dot(c.T, self._a) / self._likelihood.s2
            Sigma += -np.dot(b.T, b) + np.dot(c.T, c)

        return mu, Sigma

    def _marg_posterior(self, X, grad=False):
        # grab the prior mean and variance.
        mu = np.full(X.shape[0], self._mean)
        s2 = self._kernel.dget(X)

        if self._X is not None:
            K = self._kernel.get(self._U, X)
            b = sla.solve_triangular(self._Ruu, K, trans=True)
            c = sla.solve_triangular(self._Rux, K, trans=True)

            # add the contribution to the mean coming from the posterior and
            # subtract off the information gained in the posterior from the
            # prior variance.
            mu += np.dot(c.T, self._a) / self._likelihood.s2
            s2 += -np.sum(b * b, axis=0) + np.sum(c * c, axis=0)

        if not grad:
            return (mu, s2)

        # Get the prior gradients. Note that this assumes a constant mean and
        # stationary kernel.
        dmu = np.zeros_like(X)
        ds2 = np.zeros_like(X)

        if self._X is not None:
            dK = self._kernel.grady(self._U, X)
            dK = dK.reshape(self._U.shape[0], -1)

            db = sla.solve_triangular(self._Ruu, dK, trans=True)
            db = np.rollaxis(np.reshape(db, (-1,) + X.shape), 2)

            dc = sla.solve_triangular(self._Rux, dK, trans=True)
            dmu += np.dot(dc.T, self._a).reshape(X.shape)

            dc = np.rollaxis(np.reshape(dc, (-1,) + X.shape), 2)
            ds2 += -2 * np.sum(db * b, axis=1).T + \
                    2 * np.sum(dc * c, axis=1).T

        return (mu, s2, dmu, ds2)

    def loglikelihood(self, grad=False):
        # noise hyperparameters
        sn2 = self._likelihood.s2
        su2 = sn2 * 1e-6
        ell = np.sqrt(sn2)

        # get the rest of the kernels and the residual.
        Kux = self._kernel.get(self._U, self._X)
        r = self._y.copy() - self._mean
        r /= ell

        # the cholesky of Q.
        V = sla.solve_triangular(self._Ruu, Kux, trans=True)
        V /= ell

        p = self._U.shape[0]
        A = sla.cholesky(np.eye(p) + np.dot(V, V.T))
        beta = sla.solve_triangular(A, V.dot(r), trans=True)

        lZ = -np.sum(np.log(np.diag(A))) - self.ndata * np.log(ell)
        lZ -= 0.5 * (np.inner(r, r) - np.inner(beta, beta))
        lZ -= 0.5 * self.ndata * np.log(2*np.pi)

        if not grad:
            return lZ

        alpha = (r - V.T.dot(sla.solve_triangular(A, beta)))
        B = sla.solve_triangular(self._Ruu, V)
        W = sla.solve_triangular(A, V, trans=True)
        VW = np.dot(V, W.T)
        BW = np.dot(B, W.T)
        w = B.dot(alpha)
        v = V.dot(alpha)

        # allocate space for the gradients.
        dlZ = np.zeros(self.nhyper)

        # gradient wrt the noise parameter.
        dlZ[0] = -(
            # gradient of the mahalanobis term
            - np.inner(r, r)
            + np.inner(beta, beta)
            + np.inner(v, v)
            + su2 * np.inner(w, w)
            # gradient of the log determinant term
            + self.ndata
            - np.sum(V**2)
            + np.sum(VW**2)
            - su2 * (np.sum(B**2) - np.sum(BW**2)))

        # iterator over gradients of the kernels
        dK = it.izip(
            self._kernel.grad(self._U),
            self._kernel.grad(self._U, self._X))

        # gradient wrt the kernel hyperparameters.
        i = 1
        for i, (dKuu, dKux) in enumerate(dK, i):
            M = 2 * dKux / ell - dKuu.dot(B)
            dlZ[i] = -0.5 * (
                - np.inner(w, np.dot(M, alpha))
                + np.sum(M*B)
                - np.sum(M.dot(W.T) * B.dot(W.T)))

        # gradient wrt the constant mean.
        dlZ[-1] = np.sum(alpha) / ell

        return lZ, dlZ
"""
Implementation of exact latent-function inference in a Gaussian process model
for regression.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import scipy.linalg as sla

# local imports
from ..utils.exceptions import ModelError
from ..likelihoods import Gaussian
from ._base import GP

# exported symbols
__all__ = ['ExactGP']


class ExactGP(GP):
    """
    Exact GP inference.

    This class implements exact inference for GPs. Note that exact inference
    only works with regression so an exception will be thrown if the given
    likelihood is not Gaussian.
    """
    def __init__(self, likelihood, kernel, mean):
        # NOTE: exact inference will only work with Gaussian likelihoods.
        if not isinstance(likelihood, Gaussian):
            raise ModelError('exact inference requires a Gaussian likelihood')

        super(ExactGP, self).__init__(likelihood, kernel, mean)
        self._R = None
        self._a = None

    def reset(self):
        for attr in 'Ra':
            setattr(self, '_' + attr, None)
        super(ExactGP, self).reset()

    def _update(self):
        sn2 = self._likelihood.s2
        K = self._kernel.get(self._X) + sn2 * np.eye(len(self._X))
        r = self._y - self._mean
        self._R = sla.cholesky(K)
        self._a = sla.solve_triangular(self._R, r, trans=True)

    def _updateinc(self, X, y):
        sn2 = self._likelihood.s2
        Kss = self._kernel.get(X) + sn2 * np.eye(len(X))
        Kxs = self._kernel.get(self._X, X)
        r = y - self._mean
        self._R, self._a = chol_update(self._R, Kxs, Kss, self._a, r)

    def _full_posterior(self, X):
        # grab the prior mean and covariance.
        mu = np.full(X.shape[0], self._mean)
        Sigma = self._kernel.get(X)

        if self._X is not None:
            K = self._kernel.get(self._X, X)
            V = sla.solve_triangular(self._R, K, trans=True)

            # add the contribution to the mean coming from the posterior and
            # subtract off the information gained in the posterior from the
            # prior variance.
            mu += np.dot(V.T, self._a)
            Sigma -= np.dot(V.T, V)

        return mu, Sigma

    def _marg_posterior(self, X, grad=False):
        # grab the prior mean and variance.
        mu = np.full(X.shape[0], self._mean)
        s2 = self._kernel.dget(X)

        if self._X is not None:
            K = self._kernel.get(self._X, X)
            RK = sla.solve_triangular(self._R, K, trans=True)

            # add the contribution to the mean coming from the posterior and
            # subtract off the information gained in the posterior from the
            # prior variance.
            mu += np.dot(RK.T, self._a)
            s2 -= np.sum(RK**2, axis=0)

        if not grad:
            return (mu, s2)

        # Get the prior gradients.
        dmu = np.zeros_like(X)
        ds2 = np.zeros_like(X)

        # NOTE: the above assumes a constant mean and stationary kernel (which
        # we satisfy, but should we change either assumption...).

        if self._X is not None:
            dK = self._kernel.grady(self._X, X)
            dK = dK.reshape(self.ndata, -1)

            RdK = sla.solve_triangular(self._R, dK, trans=True)
            dmu += np.dot(RdK.T, self._a).reshape(X.shape)

            RdK = np.rollaxis(np.reshape(RdK, (-1,) + X.shape), 2)
            ds2 -= 2 * np.sum(RdK * RK, axis=1).T

        return (mu, s2, dmu, ds2)

    def loglikelihood(self, grad=False):
        lZ = -0.5 * np.inner(self._a, self._a)
        lZ -= 0.5 * np.log(2 * np.pi) * self.ndata
        lZ -= np.sum(np.log(self._R.diagonal()))

        # bail early if we don't need the gradient.
        if not grad:
            return lZ

        # intermediate terms.
        alpha = sla.solve_triangular(self._R, self._a, trans=False)
        Q = sla.cho_solve((self._R, False), np.eye(self.ndata))
        Q -= np.outer(alpha, alpha)

        dlZ = np.r_[
            # derivative wrt the likelihood's noise term.
            -self._likelihood.s2 * np.trace(Q),

            # derivative wrt each kernel hyperparameter.
            [-0.5*np.sum(Q*dK)
             for dK in self._kernel.grad(self._X)],

            # derivative wrt the mean.
            np.sum(alpha)]

        return lZ, dlZ


def chol_update(A, B, C, a, b):
    """
    Update the cholesky decomposition of a growing matrix.

    Let `A` denote a cholesky decomposition of some matrix and `a` the inverse
    of `A` applied to some vector `y`. This computes the cholesky to a new
    matrix which has additional elements `B` and the non-diagonal and `C` on
    the diagonal block. It also computes the solution to the application of the
    inverse where the vector has additional elements `b`.
    """
    n = A.shape[0]
    m = C.shape[0]

    B = sla.solve_triangular(A, B, trans=True)
    C = sla.cholesky(C - np.dot(B.T, B))
    c = np.dot(B.T, a)

    # grow the new cholesky and use then use this to grow the vector a.
    A = np.r_[np.c_[A, B], np.c_[np.zeros((m, n)), C]]
    a = np.r_[a, sla.solve_triangular(C, b-c, trans=True)]

    return A, a
"""
FITC approximation for sparse pseudo-input GPs.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import scipy.linalg as sla
import itertools as it

# local imports
from ..utils.exceptions import ModelError
from ..likelihoods import Gaussian
from ._base import GP

# exported symbols
__all__ = ['FITC']


class FITC(GP):
    """
    GP inference using sparse pseudo-inputs.
    """
    def __init__(self, likelihood, kernel, mean, U):
        # NOTE: exact FITC inference will only work with Gaussian likelihoods.
        if not isinstance(likelihood, Gaussian):
            raise ModelError('exact inference requires a Gaussian likelihood')

        super(FITC, self).__init__(likelihood, kernel, mean)

        # save the pseudo-input locations.
        self._U = np.array(U, ndmin=2, dtype=float, copy=True)

        # sufficient statistics that we'll need.
        self._L = None
        self._R = None
        self._b = None

        # these are useful in computing the loglikelihood and updating the
        # sufficient statistics.
        self._A = None
        self._a = None

    def reset(self):
        for attr in 'LRbAa':
            setattr(self, '_' + attr, None)
        super(FITC, self).reset()

    @property
    def pseudoinputs(self):
        """The pseudo-input points."""
        return self._U

    def _update(self):
        sn2 = self._likelihood.s2
        su2 = sn2 / 1e6

        # kernel wrt the inducing points.
        Kuu = self._kernel.get(self._U)
        p = self._U.shape[0]

        # cholesky for the information gain. note that we only need to compute
        # this once as it is independent from the data.
        self._L = sla.cholesky(Kuu + su2*np.eye(p))

        # evaluate the kernel and residuals at the new points
        Kux = self._kernel.get(self._U, self._X)
        kxx = self._kernel.dget(self._X)
        r = self._y - self._mean

        # the cholesky of Q.
        V = sla.solve_triangular(self._L, Kux, trans=True)

        # rescale everything by the diagonal matrix ell.
        ell = np.sqrt(kxx + sn2 - np.sum(V**2, axis=0))
        Kux /= ell
        V /= ell
        r /= ell

        # NOTE: to update things incrementally all we need to do is store these
        # components. A just needs to be initialized at the identity and then
        # we just accumulate here.
        self._A = np.eye(p) + np.dot(V, V.T)
        self._a = np.dot(Kux, r)

        # update the posterior.
        self._R = np.dot(sla.cholesky(self._A), self._L)
        self._b = sla.solve_triangular(self._R, self._a, trans=True)

    def _full_posterior(self, X):
        mu = np.full(X.shape[0], self._mean)
        Sigma = self._kernel.get(X)

        if self._X is not None:
            # get the kernel and do two backsolves by the lower-dimensional
            # choleskys that we've stored.
            K = self._kernel.get(self._U, X)
            LK = sla.solve_triangular(self._L, K, trans=True)
            RK = sla.solve_triangular(self._R, K, trans=True)

            # add on the posterior mean contribution and reduce the variance
            # based on the information that we gain from the posterior but add
            # additional uncertainty the further away we are from the inducing
            # points.
            mu += np.dot(RK.T, self._b)
            Sigma += np.dot(RK.T, RK) - np.dot(LK.T, LK)

        return mu, Sigma

    def _marg_posterior(self, X, grad=False):
        # grab the prior mean and variance.
        mu = np.full(X.shape[0], self._mean)
        s2 = self._kernel.dget(X)

        if self._X is not None:
            # get the kernel and do two backsolves by the lower-dimensional
            # choleskys that we've stored.
            K = self._kernel.get(self._U, X)
            LK = sla.solve_triangular(self._L, K, trans=True)
            RK = sla.solve_triangular(self._R, K, trans=True)

            # add on the posterior mean contribution and reduce the variance
            # based on the information that we gain from the posterior but add
            # additional uncertainty the further away we are from the inducing
            # points.
            mu += np.dot(RK.T, self._b)
            s2 += np.sum(RK**2, axis=0) - np.sum(LK**2, axis=0)

        if not grad:
            return (mu, s2)

        # Get the prior gradients. Note that this assumes a constant mean and
        # stationary kernel.
        dmu = np.zeros_like(X)
        ds2 = np.zeros_like(X)

        if self._X is not None:
            p = self._U.shape[0]
            dK = self._kernel.grady(self._U, X)
            dK = dK.reshape(p, -1)

            LdK = sla.solve_triangular(self._L, dK, trans=True)
            RdK = sla.solve_triangular(self._R, dK, trans=True)

            dmu += np.dot(RdK.T, self._b).reshape(X.shape)

            LdK = np.rollaxis(np.reshape(LdK, (p,) + X.shape), 2)
            RdK = np.rollaxis(np.reshape(RdK, (p,) + X.shape), 2)

            ds2 += 2 * np.sum(RdK * RK, axis=1).T
            ds2 -= 2 * np.sum(LdK * LK, axis=1).T

        return (mu, s2, dmu, ds2)

    def loglikelihood(self, grad=False):
        # noise hyperparameters
        sn2 = self._likelihood.s2
        su2 = sn2 / 1e6

        # get the rest of the kernels and the residual.
        Kux = self._kernel.get(self._U, self._X)
        kxx = self._kernel.dget(self._X)
        r = self._y - self._mean

        # the cholesky of Q.
        V = sla.solve_triangular(self._L, Kux, trans=True)

        # rescale everything by the diagonal matrix ell.
        ell = np.sqrt(kxx + sn2 - np.sum(V**2, axis=0))
        V /= ell
        r /= ell

        # Note this A corresponds to chol(A) from _update.
        A = sla.cholesky(self._A)
        beta = sla.solve_triangular(A, V.dot(r), trans=True)
        alpha = (r - V.T.dot(sla.solve_triangular(A, beta))) / ell

        lZ = -np.sum(np.log(np.diag(A))) - np.sum(np.log(ell))
        lZ -= 0.5 * (np.inner(r, r) - np.inner(beta, beta))
        lZ -= 0.5 * ell.shape[0] * np.log(2*np.pi)

        if not grad:
            return lZ

        B = sla.solve_triangular(self._L, V*ell)
        W = sla.solve_triangular(A, V/ell, trans=True)
        w = B.dot(alpha)
        v = 2*su2*np.sum(B**2, axis=0)

        # allocate space for the gradients.
        dlZ = np.zeros(self.nhyper)

        # gradient wrt the noise parameter.
        dlZ[0] = (
            - sn2 * (np.sum(1/ell**2) - np.sum(W**2) - np.inner(alpha, alpha))
            - su2 * (np.sum(w**2) + np.sum(B.dot(W.T)**2))
            + 0.5 * (
                np.inner(alpha, v*alpha) + np.inner(np.sum(W**2, axis=0), v)))

        # iterator over gradients of the kernels
        dK = it.izip(
            self._kernel.grad(self._U),
            self._kernel.grad(self._U, self._X),
            self._kernel.dgrad(self._X))

        # gradient wrt the kernel hyperparameters.
        i = 1
        for i, (dKuu, dKux, dkxx) in enumerate(dK, i):
            M = 2*dKux - dKuu.dot(B)
            v = dkxx - np.sum(M*B, axis=0)
            dlZ[i] = (
                - np.sum(dkxx/ell**2)
                - np.inner(w, dKuu.dot(w) - 2*dKux.dot(alpha))
                + np.inner(alpha, v*alpha) + np.inner(np.sum(W**2, axis=0), v)
                + np.sum(M.dot(W.T) * B.dot(W.T))) / 2.0

        # gradient wrt the constant mean.
        dlZ[-1] = np.sum(alpha)

        return lZ, dlZ
"""
Objects which implement the kernel interface.
"""

# pylint: disable=wildcard-import
from .se import *
from .periodic import *
from .rq import *
from .matern import *

from . import se
from . import periodic
from . import rq
from . import matern

__all__ = []
__all__ += se.__all__
__all__ += periodic.__all__
__all__ += rq.__all__
__all__ += matern.__all__
"""
Definition of the kernel interface.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# local imports
from ..utils.abc import abstractmethod
from ..utils.models import Parameterized

# exported symbols
__all__ = ['Kernel']


### BASE KERNEL INTERFACE #####################################################

class Kernel(Parameterized):
    """
    The base Kernel interface.
    """
    def __call__(self, x1, x2):
        return self.get(x1[None], x2[None])[0]

    @abstractmethod
    def get(self, X1, X2=None):
        """
        Evaluate the kernel.

        Returns the matrix of covariances between points in `X1` and `X2`. If
        `X2` is not given this will return the pairwise covariances between
        points in `X1`.
        """

    @abstractmethod
    def dget(self, X):
        """Evaluate the self covariances."""

    @abstractmethod
    def grad(self, X1, X2=None):
        """
        Evaluate the gradient of the kernel.

        Returns an iterator over the gradients of the covariances between
        points in `X1` and `X2`. If `X2` is not given this will iterate over
        the the gradients of the pairwise covariances.
        """

    @abstractmethod
    def dgrad(self, X):
        """Evaluate the gradients of the self covariances."""

    @abstractmethod
    def transform(self, X):
        """Format the inputs X as arrays."""
"""
Combination classes.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import itertools as it
import functools as ft
import operator as op

# local imports
from ._base import Kernel
from ..utils.models import dot_params

# exported symbols
__all__ = ['ComboKernel', 'SumKernel', 'ProductKernel', 'combine']


### HELPER METHODS ############################################################

def product(fiterable):
    """
    The equivalent object to sum but for products.
    """
    return ft.reduce(op.mul, fiterable, 1)


def product_but(fiterable):
    """
    Given an iterator over function evaluations return an array such that
    `M[i]` is the product of every evaluation except for the ith one.
    """
    A = list(fiterable)

    # allocate memory for M and fill everything but the last element with
    # the product of A[i+1:]. Note that we're using the cumprod in place.
    M = np.empty_like(A)
    np.cumprod(A[:0:-1], axis=0, out=M[:-1][::-1])

    # use an explicit loop to iteratively set M[-1] equal to the product of
    # A[:-1]. While doing this we can multiply M[i] by A[:i].
    M[-1] = A[0]
    for i in xrange(1, len(A)-1):
        M[i] *= M[-1]
        M[-1] *= A[i]

    return M


### GENERAL COMBINATION KERNEL ################################################

class ComboKernel(Kernel):
    """
    Implementation of mixin methods for kernels that are themselves
    combinations of other kernels.
    """
    def __init__(self, *parts):
        self._parts = [part.copy() for part in parts]
        self.nhyper = sum(p.nhyper for p in self._parts)

    def __repr__(self):
        string = self.__class__.__name__ + '('
        indent = len(string) * ' '
        substrings = [repr(p) for p in self._parts]
        string += (',\n').join(substrings) + ')'
        string = ('\n'+indent).join(string.splitlines())
        return string

    def _params(self):
        # this is complicated somewhat because I want to return a flat list of
        # parts. so I avoid calling _params() recursively since we could also
        # contain combo objects.
        params = []
        nparts = 0
        parts = list(reversed(self._parts))
        while len(parts) > 0:
            part = parts.pop()
            if isinstance(part, ComboKernel):
                parts.extend(reversed(part._parts))
            else:
                params.extend(dot_params('part%d' % nparts, part._params()))
                nparts += 1
        return params

    def get_hyper(self):
        return np.hstack(p.get_hyper() for p in self._parts)

    def set_hyper(self, hyper):
        a = 0
        for p in self._parts:
            b = a + p.nhyper
            p.set_hyper(hyper[a:b])
            a = b


### SUM AND PRODUCT KERNELS ###################################################

class SumKernel(ComboKernel):
    """Kernel representing a sum of other kernels."""

    def get(self, X1, X2=None):
        fiterable = (p.get(X1, X2) for p in self._parts)
        return sum(fiterable)

    def dget(self, X):
        fiterable = (p.dget(X) for p in self._parts)
        return sum(fiterable)

    def grad(self, X1, X2=None):
        giterable = (p.grad(X1, X2) for p in self._parts)
        return it.chain.from_iterable(giterable)

    def dgrad(self, X):
        giterable = (p.dgrad(X) for p in self._parts)
        return it.chain.from_iterable(giterable)


class ProductKernel(ComboKernel):
    """Kernel representing a product of other kernels."""

    def get(self, X1, X2=None):
        fiterable = (p.get(X1, X2) for p in self._parts)
        return product(fiterable)

    def dget(self, X):
        fiterable = (p.dget(X) for p in self._parts)
        return product(fiterable)

    def grad(self, X1, X2=None):
        fiterable = (p.get(X1, X2) for p in self._parts)
        giterable = (p.grad(X1, X2) for p in self._parts)
        for Mi, grads in zip(product_but(fiterable), giterable):
            for dM in grads:
                yield Mi*dM

    def dgrad(self, X):
        fiterable = (p.dget(X) for p in self._parts)
        giterable = (p.dgrad(X) for p in self._parts)
        for Mi, grads in zip(product_but(fiterable), giterable):
            for dM in grads:
                yield Mi*dM


### HELPER FOR ASSOCIATIVE OPERATIONS #########################################

def combine(cls, *parts):
    """
    Given a list of kernels return another list of kernels where objects of
    type cls have been "combined". This applies to ComboKernel objects which
    represent associative operations.
    """
    combined = []
    for part in parts:
        combined += part._parts if isinstance(part, cls) else [part]
    return combined
"""
Implementation of distance computations.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import scipy.spatial.distance as ssd

# exported symbols
__all__ = ['rescale', 'sqdist', 'sqdist_foreach']


def rescale(ell, X1, X2):
    """
    Rescale the two sets of vectors by `ell`.
    """
    X1 = X1 / ell
    X2 = X2 / ell if (X2 is not None) else None
    return X1, X2


def diff(X1, X2=None):
    """
    Return the differences between vectors in `X1` and `X2`. If `X2` is not
    given this will return the pairwise differences in `X1`.
    """
    X2 = X1 if (X2 is None) else X2
    return X1[:, None, :] - X2[None, :, :]


def sqdist(X1, X2=None):
    """
    Return the squared-distance between two sets of vector. If `X2` is not
    given this will return the pairwise squared-distances in `X1`.
    """
    X2 = X1 if (X2 is None) else X2
    return ssd.cdist(X1, X2, 'sqeuclidean')


def sqdist_foreach(X1, X2=None):
    """
    Return an iterator over each dimension returning the squared-distance
    between two sets of vector. If `X2` is not given this will iterate over the
    pairwise squared-distances in `X1` in each dimension.
    """
    X2 = X1 if (X2 is None) else X2
    for i in xrange(X1.shape[1]):
        yield ssd.cdist(X1[:, i, None], X2[:, i, None], 'sqeuclidean')
"""
Base class for real-valued kernels.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ._base import Kernel
from ..utils.abc import abstractmethod

# import the generic sum/product kernels and change their names. We'll call the
# real-valued versions SumKernel and ProductKernel as well since they really
# shouldn't be used outside of this module anyway.
from ._combo import SumKernel as SumKernel_
from ._combo import ProductKernel as ProductKernel_
from ._combo import combine
from ._combo import product_but

# exported symbols
__all__ = ['RealKernel']


class RealKernel(Kernel):
    """Kernel whose inputs are real-valued vectors."""

    def __add__(self, other):
        return SumKernel(*combine(SumKernel, self, other))

    def __mul__(self, other):
        return ProductKernel(*combine(ProductKernel, self, other))

    def transform(self, X):
        return np.array(X, ndmin=2, dtype=float, copy=False)

    @abstractmethod
    def gradx(self, X1, X2=None):
        """
        Derivatives of the kernel with respect to its first argument. Returns
        an (m,n,d)-array.
        """

    @abstractmethod
    def grady(self, X1, X2=None):
        """
        Derivatives of the kernel with respect to its second argument. Returns
        an (m,n,d)-array.
        """

    @abstractmethod
    def gradxy(self, X1, X2=None):
        """
        Derivatives of the kernel with respect to both its first and second
        arguments. Returns an (m,n,d,d)-array. The (a,b,i,j)th element
        corresponds to the derivative with respect to `X1[a,i]` and `X2[b,j]`.
        """

    @abstractmethod
    def sample_spectrum(self, N, rng=None):
        """
        Sample N values from the spectral density of the kernel, returning a
        set of weights W of size (n,d) and a scalar value representing the
        normalizing constant.
        """


def _can_combine(*parts):
    """
    Return whether a set of real-valued kernels can be combined. Here this
    requires them to all be RealKernel objects and have the same number of
    input dimensions.
    """
    return (all(isinstance(_, RealKernel) for _ in parts) and
            all(_.ndim == parts[0].ndim for _ in parts))


class SumKernel(RealKernel, SumKernel_):
    """A sum of real-valued kernels."""

    def __init__(self, *parts):
        if not _can_combine(*parts):
            raise ValueError('cannot add mismatched kernels')

        super(SumKernel, self).__init__(*parts)
        self.ndim = self._parts[0].ndim

    def gradx(self, X1, X2=None):
        return sum(p.gradx(X1, X2) for p in self._parts)

    def grady(self, X1, X2=None):
        return sum(p.grady(X1, X2) for p in self._parts)

    def gradxy(self, X1, X2=None):
        return sum(p.gradxy(X1, X2) for p in self._parts)

    def sample_spectrum(self, N, rng=None):
        raise NotImplementedError


class ProductKernel(RealKernel, ProductKernel_):
    """A product of real-valued kernels."""

    def __init__(self, *parts):
        if not _can_combine(*parts):
            raise ValueError('cannot multiply mismatched kernels')

        super(ProductKernel, self).__init__(*parts)
        self.ndim = self._parts[0].ndim

    def gradx(self, X1, X2=None):
        fiterable = (p.get(X1, X2)[:, :, None] for p in self._parts)
        giterable = (p.gradx(X1, X2) for p in self._parts)
        return sum(f*g for f, g in zip(product_but(fiterable), giterable))

    def grady(self, X1, X2=None):
        fiterable = (p.get(X1, X2)[:, :, None] for p in self._parts)
        giterable = (p.grady(X1, X2) for p in self._parts)
        return sum(f*g for f, g in zip(product_but(fiterable), giterable))

    def gradxy(self, X1, X2=None):
        # the kernel evaluations.
        K = [p.get(X1, X2) for p in self._parts]
        Kn = product_but(K)

        # the gradients we need.
        Gx = [p.gradx(X1, X2) for p in self._parts]
        Gy = [p.grady(X1, X2) for p in self._parts]
        Gxy = [p.gradxy(X1, X2) for p in self._parts]

        # the part of the gradient corresponding to the two partial derivatives
        # with respect to xy.
        grad = sum(Kni[:, :, None, None] * dKi for Kni, dKi in zip(Kn, Gxy))

        # this is the combination of partials for different kernels.
        # multiplying in this way lets us avoid an explicit double-loop, but we
        # overcount.
        xpart = sum(dKi * Ki[:, :, None] for dKi, Ki in zip(Gx, Kn))
        ypart = sum(dKi / Ki[:, :, None] for dKi, Ki in zip(Gy, K))
        grad += xpart[:, :, :, None] * ypart[:, :, None, :]

        # get rid of the overcount.
        grad -= sum((Kni / Ki)[:, :, None, None]
                    * dKx[:, :, :, None]
                    * dKy[:, :, None, :]
                    for Kni, Ki, dKx, dKy in zip(Kn, K, Gx, Gy))

        return grad

    def sample_spectrum(self, N, rng=None):
        raise NotImplementedError
"""
Implementation of the squared-exponential kernels.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ._real import RealKernel
from ._distances import rescale, diff, sqdist, sqdist_foreach

from ..utils.random import rstate
from ..utils.models import printable

# exported symbols
__all__ = ['Matern']


@printable
class Matern(RealKernel):
    def __init__(self, sf, ell, d=3, ndim=None):
        self._logsf = np.log(float(sf))
        self._logell = np.log(ell)
        self._d = d
        self._iso = False
        self.ndim = np.size(self._logell)
        self.nhyper = 1 + np.size(self._logell)

        if ndim is not None:
            if np.size(self._logell) == 1:
                self._logell = float(self._logell)
                self._iso = True
                self.ndim = ndim
            else:
                raise ValueError('ndim only usable with scalar lengthscales')

        if self._d not in {1, 3, 5}:
            raise ValueError('d must be one of 1, 3, or 5')

    def _f(self, r):
        return (
            1 if (self._d == 1) else
            1+r if (self._d == 3) else
            1+r*(1+r/3.))

    def _df(self, r):
        return (
            1 if (self._d == 1) else
            r if (self._d == 3) else
            r*(1+r)/3.)

    def _params(self):
        return [
            ('sf', 1),
            ('ell', self.nhyper-1),
        ]

    def get_hyper(self):
        return np.r_[self._logsf, self._logell]

    def set_hyper(self, hyper):
        self._logsf = hyper[0]
        self._logell = hyper[1] if self._iso else hyper[1:]

    def get(self, X1, X2=None):
        X1, X2 = rescale(np.exp(self._logell)/np.sqrt(self._d), X1, X2)
        D = np.sqrt(sqdist(X1, X2))
        S = np.exp(self._logsf*2 - D)
        K = S * self._f(D)
        return K

    def grad(self, X1, X2=None):
        X1, X2 = rescale(np.exp(self._logell)/np.sqrt(self._d), X1, X2)
        D = np.sqrt(sqdist(X1, X2))
        S = np.exp(self._logsf*2 - D)
        K = S * self._f(D)
        M = S * self._df(D)

        yield 2*K           # derivative wrt logsf
        if self._iso:
            yield M*D       # derivative wrt logell (iso)
        else:
            for D_ in sqdist_foreach(X1, X2):
                            # derivative(s) wrt logell (ard)
                with np.errstate(invalid='ignore'):
                    yield np.where(D < 1e-12, 0, M*D_/D)

    def dget(self, X1):
        return np.exp(self._logsf*2) * np.ones(len(X1))

    def dgrad(self, X1):
        yield 2 * self.dget(X1)
        for _ in xrange(self.nhyper-1):
            yield np.zeros(len(X1))

    def gradx(self, X1, X2=None):
        ell = np.exp(self._logell) / np.sqrt(self._d)
        X1, X2 = rescale(ell, X1, X2)
        D1 = diff(X1, X2)

        D = np.sqrt(np.sum(D1**2, axis=-1))
        S = np.exp(self._logsf*2 - D)
        with np.errstate(invalid='ignore'):
            M = np.where(D < 1e-12, 0, S * self._df(D) / D)
        G = -M[:, :, None] * D1 / ell

        return G

    def grady(self, X1, X2=None):
        return -self.gradx(X1, X2)

    def gradxy(self, X1, X2=None):
        raise NotImplementedError

    def sample_spectrum(self, N, rng=None):
        rng = rstate(rng)
        sf2 = np.exp(self._logsf*2)
        ell = np.exp(self._logell)
        a = self._d / 2.
        g = np.tile(rng.gamma(a, 1/a, N), (self.ndim, 1)).T
        W = (rng.randn(N, self.ndim) / ell) / np.sqrt(g)
        return W, sf2
"""
Kernel which places a prior over periodic functions.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ._real import RealKernel
from ._distances import diff, sqdist
from ..utils.models import printable

# exported symbols
__all__ = ['Periodic']


@printable
class Periodic(RealKernel):
    """
    Covariance function for a 1-dimensional smooth periodic function with
    period p, lenthscale ell, and signal variance sf. The kernel function is
    given by::

        k(x, y) = sf^2 exp(-2 sin^2( ||x-y|| pi / p ) / ell^2)
    """
    def __init__(self, sf, ell, p):
        self._logsf = np.log(float(sf))
        self._logell = np.log(float(ell))
        self._logp = np.log(float(p))
        self.ndim = 1
        self.nhyper = 3

    def _params(self):
        return [
            ('sf', 1),
            ('ell', 1),
            ('p', 1),
        ]

    def get_hyper(self):
        return np.r_[self._logsf, self._logell, self._logp]

    def set_hyper(self, hyper):
        self._logsf = hyper[0]
        self._logell = hyper[1]
        self._logp = hyper[2]

    def get(self, X1, X2=None):
        sf2 = np.exp(self._logsf*2)
        ell = np.exp(self._logell)
        p = np.exp(self._logp)
        D = np.sqrt(sqdist(X1, X2)) * np.pi / p
        K = sf2 * np.exp(-2*(np.sin(D) / ell)**2)
        return K

    def grad(self, X1, X2=None):
        sf2 = np.exp(self._logsf*2)
        ell = np.exp(self._logell)
        p = np.exp(self._logp)

        # get the distance and a few transformations
        D = np.sqrt(sqdist(X1, X2)) * np.pi / p
        R = np.sin(D) / ell
        S = R**2
        E = 2 * sf2 * np.exp(-2*S)

        yield E
        yield 2*E*S
        yield 2*E*R*D * np.cos(D) / ell

    def dget(self, X1):
        return np.exp(self._logsf*2) * np.ones(len(X1))

    def dgrad(self, X):
        yield 2 * self.dget(X)
        yield np.zeros(len(X))
        yield np.zeros(len(X))

    def gradx(self, X1, X2=None):
        sf2 = np.exp(self._logsf*2)
        ell = np.exp(self._logell)
        p = np.exp(self._logp)

        # get the distance and a few transformations
        D = diff(X1, X2) * np.pi / p
        K = sf2 * np.exp(-2*(np.sin(D) / ell)**2)
        G = -2 * np.pi / ell**2 / p * K * np.sin(2*D)

        return G

    def grady(self, X1, X2=None):
        return -self.gradx(X1, X2)

    def gradxy(self, X1, X2=None):
        raise NotImplementedError

    def sample_spectrum(self, N, rng=None):
        raise NotImplementedError
"""
Kernel which places a prior over periodic functions.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ._real import RealKernel
from ..utils.models import printable
from ._distances import rescale, diff, sqdist, sqdist_foreach

# exported symbols
__all__ = ['RQ']


@printable
class RQ(RealKernel):
    def __init__(self, sf, ell, alpha, ndim=None):
        self._logsf = np.log(float(sf))
        self._logell = np.log(ell)
        self._logalpha = np.log(float(alpha))

        self._iso = False
        self.ndim = np.size(self._logell)
        self.nhyper = 2 + np.size(self._logell)

        if ndim is not None:
            if np.size(self._logell) == 1:
                self._logell = float(self._logell)
                self._iso = True
                self.ndim = ndim
            else:
                raise ValueError('ndim only usable with scalar lengthscales')

    def _params(self):
        return [
            ('sf', 1),
            ('ell', self.nhyper-2),
            ('alpha', 1),
        ]

    def get_hyper(self):
        return np.r_[self._logsf, self._logell, self._logalpha]

    def set_hyper(self, hyper):
        self._logsf = hyper[0]
        self._logell = hyper[1] if self._iso else hyper[1:-1]
        self._logalpha = hyper[-1]

    def get(self, X1, X2=None):
        sf2 = np.exp(self._logsf*2)
        ell = np.exp(self._logell)
        alpha = np.exp(self._logalpha)

        X1, X2 = rescale(ell, X1, X2)
        K = sf2 * (1 + 0.5*sqdist(X1, X2)/alpha) ** (-alpha)
        return K

    def grad(self, X1, X2=None):
        # hypers
        sf2 = np.exp(self._logsf*2)
        ell = np.exp(self._logell)
        alpha = np.exp(self._logalpha)

        # precomputations
        X1, X2 = rescale(ell, X1, X2)
        D = sqdist(X1, X2)
        E = 1 + 0.5*D/alpha
        K = sf2 * E**(-alpha)
        M = K*D/E

        yield 2*K                               # derivative wrt logsf
        if self._iso:
            yield M                             # derivative wrt logell (iso)
        else:
            for D in sqdist_foreach(X1, X2):
                yield K*D/E                     # derivative wrt logell (ard)
        yield 0.5*M - alpha*K*np.log(E)         # derivative wrt alpha

    def dget(self, X1):
        return np.exp(self._logsf*2) * np.ones(len(X1))

    def dgrad(self, X):
        yield 2 * self.dget(X)
        for _ in xrange(self.nhyper-2):
            yield np.zeros(len(X))
        yield np.zeros(len(X))

    def gradx(self, X1, X2=None):
        # hypers
        sf2 = np.exp(self._logsf*2)
        ell = np.exp(self._logell)
        alpha = np.exp(self._logalpha)

        # precomputations
        X1, X2 = rescale(ell, X1, X2)
        D = diff(X1, X2)
        E = 1 + np.sum(D**2, axis=-1) / 2 / alpha
        K = sf2 * E**(-alpha)
        G = -(K/E)[:, :, None] * D / ell

        return G

    def grady(self, X1, X2=None):
        return -self.gradx(X1, X2)

    def gradxy(self, X1, X2=None):
        raise NotImplementedError

    def sample_spectrum(self, N, rng=None):
        raise NotImplementedError
"""
Implementation of the squared-exponential kernels.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ._real import RealKernel
from ._distances import rescale, diff, sqdist, sqdist_foreach

from ..utils.random import rstate
from ..utils.models import printable

# exported symbols
__all__ = ['SE']


@printable
class SE(RealKernel):
    def __init__(self, sf, ell, ndim=None):
        self._logsf = np.log(float(sf))
        self._logell = np.log(ell)
        self._iso = False
        self.ndim = np.size(self._logell)
        self.nhyper = 1 + np.size(self._logell)

        if ndim is not None:
            if np.size(self._logell) == 1:
                self._logell = float(self._logell)
                self._iso = True
                self.ndim = ndim
            else:
                raise ValueError('ndim only usable with scalar lengthscales')

    def _params(self):
        return [
            ('sf', 1),
            ('ell', self.nhyper-1),
        ]

    def get_hyper(self):
        return np.r_[self._logsf, self._logell]

    def set_hyper(self, hyper):
        self._logsf = hyper[0]
        self._logell = hyper[1] if self._iso else hyper[1:]

    def get(self, X1, X2=None):
        X1, X2 = rescale(np.exp(self._logell), X1, X2)
        return np.exp(self._logsf*2 - sqdist(X1, X2)/2)

    def grad(self, X1, X2=None):
        X1, X2 = rescale(np.exp(self._logell), X1, X2)
        D = sqdist(X1, X2)
        K = np.exp(self._logsf*2 - D/2)
        yield 2*K                               # derivative wrt logsf
        if self._iso:
            yield K*D                           # derivative wrt logell (iso)
        else:
            for D in sqdist_foreach(X1, X2):
                yield K*D                       # derivatives wrt logell (ard)

    def dget(self, X1):
        return np.exp(self._logsf*2) * np.ones(len(X1))

    def dgrad(self, X):
        yield 2 * self.dget(X)
        for _ in xrange(self.nhyper-1):
            yield np.zeros(len(X))

    def gradx(self, X1, X2=None):
        ell = np.exp(self._logell)
        X1, X2 = rescale(ell, X1, X2)

        D = diff(X1, X2)
        K = np.exp(self._logsf*2 - np.sum(D**2, axis=-1)/2)
        G = -K[:, :, None] * D / ell
        return G

    def grady(self, X1, X2=None):
        return -self.gradx(X1, X2)

    def gradxy(self, X1, X2=None):
        ell = np.exp(self._logell)
        X1, X2 = rescale(ell, X1, X2)
        D = diff(X1, X2)
        _, _, d = D.shape

        K = np.exp(self._logsf*2 - np.sum(D**2, axis=-1)/2)
        D /= ell
        M = np.eye(d)/ell**2 - D[:, :, None] * D[:, :, :, None]
        G = M * K[:, :, None, None]

        return G

    def sample_spectrum(self, N, rng=None):
        rng = rstate(rng)
        sf2 = np.exp(self._logsf*2)
        ell = np.exp(self._logell)
        W = rng.randn(N, self.ndim) / ell
        return W, sf2
"""
Methods for learning the hyperparameters.
"""

# pylint: disable=wildcard-import
from .optimization import *
from .sampling import *

from . import optimization
from . import sampling

__all__ = []
__all__ += optimization.__all__
__all__ += sampling.__all__
"""
Perform type-II maximum likelihood to fit the hyperparameters of a GP model.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import scipy.optimize as so

# local imports
from ..utils.models import get_params

# exported symbols
__all__ = ['optimize']


def optimize(gp, priors=None):
    """
    Perform type-II maximum likelihood to fit GP hyperparameters.

    If given the priors object should be a dictionary mapping named parameters
    to an object which implements `prior.loglikelihood(hyper, grad)`. If a
    parameter is mapped to the `None` value then this will be assumed fixed.

    Note: nothing is returned by this function. Instead it will modify the
    hyperparameters of the given GP object in place.
    """
    hyper0 = gp.get_hyper()
    active = np.ones(gp.nhyper, dtype=bool)

    # this just manipulates a few lists so that we transform priors into a list
    # of tuples of the form (block, log, prior) for each named prior.
    params = dict((key, (block, log)) for (key, block, log) in get_params(gp))
    priors = dict() if (priors is None) else priors
    priors = [params[key] + (prior,) for (key, prior) in priors.items()]
    del params

    # remove from the active any block where the prior is None.
    for block, _, prior in priors:
        if prior is None:
            active[block] = False

    # get rid of these simple constraint priors.
    priors = [(b, l, p) for (b, l, p) in priors if p is not None]

    # FIXME: right now priors won't work because I am not dealing with the any
    # of the log transformed components.
    assert len(priors) == 0

    def objective(x):
        hyper = hyper0.copy()
        hyper[active] = x
        gp.set_hyper(hyper)
        lZ, dlZ = gp.loglikelihood(True)
        return -lZ, -dlZ[active]

    # optimize the model
    x, _, info = so.fmin_l_bfgs_b(objective, hyper0[active])

    # make sure that the gp is using the correct hypers
    hyper = hyper0.copy()
    hyper[active] = x
    gp.set_hyper(hyper)
"""
Perform hyperparameter sampling.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ..utils.models import get_params

# exported symbols
__all__ = ['sample']


#==============================================================================
# basic sampler(s) that don't know anything about GP objects.

def _slice_sample(logprob, x0, sigma=1.0, step_out=True, max_steps_out=1000):
    """
    Implementation of slice sampling taken almost directly from Snoek's
    spearmint package (with a few minor modifications).
    """
    def direction_slice(direction, x0):
        def dir_logprob(z):
            return logprob(direction*z + x0)

        upper = sigma*np.random.rand()
        lower = upper - sigma
        llh_s = np.log(np.random.rand()) + dir_logprob(0.0)

        l_steps_out = 0
        u_steps_out = 0
        if step_out:
            while dir_logprob(lower) > llh_s and l_steps_out < max_steps_out:
                l_steps_out += 1
                lower -= sigma
            while dir_logprob(upper) > llh_s and u_steps_out < max_steps_out:
                u_steps_out += 1
                upper += sigma

        while True:
            new_z = (upper - lower)*np.random.rand() + lower
            new_llh = dir_logprob(new_z)
            if np.isnan(new_llh):
                raise Exception("Slice sampler got a NaN")
            if new_llh > llh_s:
                break
            elif new_z < 0:
                lower = new_z
            elif new_z > 0:
                upper = new_z
            else:
                raise Exception("Slice sampler shrank to zero!")

        return new_z*direction + x0

    # FIXME: I've removed how blocks work because I want to rewrite that bit.
    # so right now this samples everything as one big block.
    direction = np.random.randn(x0.shape[0])
    direction = direction / np.sqrt(np.sum(direction**2))
    return direction_slice(direction, x0)


#==============================================================================
# interface for sampling hyperparameters from a GP.

def sample(gp, priors, n, raw=True):
    priors = dict(priors)
    active = np.ones(gp.nhyper, dtype=bool)
    logged = np.ones(gp.nhyper, dtype=bool)

    for (key, block, log) in get_params(gp):
        inactive = (key in priors) and (priors[key] is None)
        logged[block] = log
        active[block] = not inactive
        if inactive:
            del priors[key]
        else:
            priors[key] = (block, log, priors[key])

    # priors is now just a list of the form (block, log, prior).
    priors = priors.values()

    # get the initial hyperparameters and transform into the non-log space.
    hyper0 = gp.get_hyper()
    hyper0[logged] = np.exp(hyper0[logged])

    def logprob(x):
        # copy the initial hyperparameters and then assign the "active"
        # parameters that come from x.
        hyper = hyper0.copy()
        hyper[active] = x
        logprob = 0

        # compute the prior probabilities. we do this first so that if there
        # are any infs they'll be caught in the least expensive computations
        # first.
        for block, log, prior in priors:
            logprob += prior.logprior(hyper[block])
            if np.isinf(logprob):
                break

        # now compute the likelihood term. note that we'll have to take the log
        # of any logspace parameters before calling set_hyper.
        if not np.isinf(logprob):
            hyper[logged] = np.log(hyper[logged])
            gp.set_hyper(hyper)
            logprob += gp.loglikelihood()

        return logprob

    # create a big list of the hyperparameters so that we can just assign to
    # the components that are active. also get an initial sample x
    # corresponding only to the active parts of hyper0.
    hypers = np.tile(hyper0, (n, 1))
    x = hyper0.copy()[active]

    # do the sampling.
    for i in xrange(n):
        x = _slice_sample(logprob, x)
        hypers[i][active] = x

    # change the logspace components back into logspace.
    hypers[:, logged] = np.log(hypers[:, logged])

    # make sure the gp gets updated to the last sampled hyperparameter.
    gp.set_hyper(hypers[-1])

    if raw:
        return hypers
    else:
        return [gp.copy(h) for h in hypers]
"""
Objects implementing likelihoods.
"""

# pylint: disable=wildcard-import
from .gaussian import *

from . import gaussian

__all__ = []
__all__ += gaussian.__all__
"""
Implementation of the squared-exponential kernels.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ..utils.abc import abstractmethod
from ..utils.models import Parameterized

# exported symbols
__all__ = ['Likelihood', 'RealLikelihood']


class Likelihood(Parameterized):
    """
    Likelihood interface.
    """
    @abstractmethod
    def transform(self, y):
        pass

    @abstractmethod
    def sample(self, f, rng=None):
        pass


class RealLikelihood(Likelihood):
    def transform(self, y):
        return np.array(y, ndmin=1, dtype=float, copy=False)
"""
Implementation of the Gaussian likelihood model.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ._base import RealLikelihood
from ..utils.models import printable
from ..utils.random import rstate

# exported symbols
__all__ = ['Gaussian']


@printable
class Gaussian(RealLikelihood):
    """
    Likelihood model for standard Gaussian distributed errors.
    """
    def __init__(self, sigma):
        self._logsigma = np.log(float(sigma))
        self.nhyper = 1

    def _params(self):
        return [
            ('sigma', 1),
        ]

    @property
    def s2(self):
        """Simple access to the noise variance."""
        return np.exp(self._logsigma*2)

    def get_hyper(self):
        return np.r_[self._logsigma]

    def set_hyper(self, hyper):
        self._logsigma = hyper[0]

    def sample(self, f, rng=None):
        rng = rstate(rng)
        return f + rng.normal(size=len(f), scale=np.exp(self._logsigma))
"""
Meta-models which act like a GP object but also marginalize over the
hyperparameters.
"""

# pylint: disable=wildcard-import
from .mcmc import *
from .smc import *

from . import mcmc
from . import smc

__all__ = []
__all__ += mcmc.__all__
__all__ += smc.__all__
"""
Meta models which take care of hyperparameter marginalization whenever data is
added.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ..learning.sampling import sample

# exported symbols
__all__ = ['MCMC']


class MCMC(object):
    def __init__(self, model, prior, n=100, burn=100):
        self._model = model.copy()
        self._prior = prior
        self._samples = []
        self._n = n
        self._burn = burn

        if self._model.ndata > 0:
            if self._burn > 0:
                sample(self._model, self._prior, self._burn)
            self._samples = sample(self._model, self._prior, self._n, False)

        else:
            # FIXME: the likelihood won't play a role, so we can sample
            # directly from the prior. This of course requires the prior to
            # also be a well-defined distribution.
            pass

    def __iter__(self):
        return self._samples.__iter__()

    @property
    def ndata(self):
        return self._model.ndata

    @property
    def data(self):
        return self._model.data

    def add_data(self, X, y):
        # add the data
        nprev = self._model.ndata
        self._model.add_data(X, y)

        # if we've increased the amount of data by more than a factor two we'll
        # burn off some samples. Not sure if this is entirely necessary, but it
        # also accounts for burnin right after initial data is added.
        if self._model.ndata > 2*nprev and self._burn > 0:
            sample(self._model, self._prior, self._burn)

        # grab the samples.
        self._samples = sample(self._model, self._prior, self._n, False)

    def posterior(self, X, grad=False):
        parts = map(np.array,
                    zip(*[_.posterior(X, grad) for _ in self._samples]))

        mu_, s2_ = parts[:2]
        mu = np.mean(mu_, axis=0)
        s2 = np.mean(s2_ + (mu_ - mu)**2, axis=0)

        if not grad:
            return mu, s2

        dmu_, ds2_ = parts[2:]
        dmu = np.mean(dmu_, axis=0)
        Dmu = dmu_ - dmu
        ds2 = np.mean(ds2_
                      + 2 * mu_[:, :, None] * Dmu
                      - 2 * mu[None, :, None] * Dmu, axis=0)

        return mu, s2, dmu, ds2
"""
Meta models which take care of hyperparameter marginalization whenever data is
added.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
from scipy.misc import logsumexp

# local imports
from ..learning.sampling import sample
from ..utils.models import get_params

# exported symbols
__all__ = ['SMC']


def _sample_prior(model, priors, n):
    # unpack priors
    # TODO -- Bobak: This snippet is copied from learning/sampling.py
    # and should probably be put into a Prior base class.
    priors = dict(priors)
    active = np.ones(model.nhyper, dtype=bool)
    logged = np.ones(model.nhyper, dtype=bool)

    for (key, block, log) in get_params(model):
        inactive = (key in priors) and (priors[key] is None)
        logged[block] = log
        active[block] = not inactive
        if inactive:
            del priors[key]
        else:
            priors[key] = (block, log, priors[key])
    priors = priors.values()

    # sample hyperparameters from prior
    hypers = np.zeros((n, model.nhyper))
    for (block, log, prior) in priors:
        hypers[:, block] = prior.sample(n, log=log)

    return hypers


class SMC(object):
    def __init__(self, model, prior, n=100):
        self._prior = prior
        self._n = n

        # we won't add any data unless the model already has it.
        data = None

        if model.ndata > 0:
            data = model.data
            model = model.copy()
            model.reset()

        self._samples = [model.copy(h) for h in _sample_prior(model, prior, n)]
        self._logweights = np.zeros(n) - np.log(n)
        self._loglikes = np.zeros(n)

        if data is not None:
            self.add_data(data[0], data[1])

    def __iter__(self):
        return self._samples.__iter__()

    @property
    def ndata(self):
        return self._samples[-1].ndata

    @property
    def data(self):
        return self._samples[-1].data

    def add_data(self, X, y):
        X = self._samples[0]._kernel.transform(X)
        y = self._samples[0]._likelihood.transform(y)

        for (xi, yi) in zip(X, y):
            # resample if effective sample size is less than N/2
            if -logsumexp(2*self._logweights) < np.log(self._n/2):
                # FIXME: can use a better resampling strategy here. ie,
                # stratified, etc.
                p = np.exp(self._logweights)
                idx = np.random.choice(self._n, self._n, p=p)
                self._samples = [self._samples[i].copy() for i in idx]
                self._logweights = np.zeros(self._n) - np.log(self._n)
                self._loglikes = self._loglikes[idx]

            # add data
            for model in self._samples:
                model.add_data(xi, yi)

            # we will propose new hyperparameters using an MCMC kernel, which
            # corresponds to Eqs. 30--31 of (Del Moral et al, 2006). To compute
            # the incremental weights we just need the loglikelihoods before
            # and after adding the new data but before propagating the
            # particles.

            # self._loglikes already contains the loglikelihood before adding
            # the data, so what follows computes it after adding the data.
            loglikes = np.fromiter((model.loglikelihood()
                                    for model in self._samples), float)

            # update and normalize the weights.
            self._logweights += loglikes - self._loglikes
            self._logweights -= logsumexp(self._logweights)

            # propagate the particles.
            for model in self._samples:
                sample(model, self._prior, 1)

            # update the loglikelihoods given the new samples.
            self._loglikes = np.fromiter((model.loglikelihood()
                                          for model in self._samples), float)

    def posterior(self, X, grad=False):
        parts = [_.posterior(X, grad) for _ in self._samples]
        parts = [np.array(_) for _ in zip(*parts)]

        weights = np.exp(self._logweights)

        mu_, s2_ = parts[:2]
        mu = np.average(mu_, weights=weights, axis=0)
        s2 = np.average(s2_ + (mu_ - mu)**2, weights=weights, axis=0)

        if not grad:
            return mu, s2

        dmu_, ds2_ = parts[2:]
        dmu = np.average(dmu_, weights=weights, axis=0)

        Dmu = dmu_ - dmu
        ds2 = np.average(ds2_
                         + 2 * mu_[:, :, None] * Dmu
                         - 2 * mu[None, :, None] * Dmu,
                         weights=weights, axis=0)

        return mu, s2, dmu, ds2
"""
Plotting methods for GP objects.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import matplotlib.pyplot as pl

# local imports
from .utils.models import get_params

# exported symbols
__all__ = ['plot_posterior', 'plot_samples']


def plot_posterior(model,
                   xmin=None, xmax=None,
                   mean=True, data=True, error=True, pseudoinputs=True,
                   lw=2, ls='-', color=None, marker='o', marker2='x'):
    """
    Plot a one-dimensional posterior model.

    Parameters:
        xmin: minimum x value
        xmax: maximum x value
        mean: plot the mean
        data: plot the data
        error: plot the error bands
        pseudoinputs: plot pseudoinputs (if there are any)
    """

    # grab the data.
    X, y = model.data
    if X is None and (xmin is None or xmax is None):
        raise Exception('bounds must be given if no data is present')

    # get the input points.
    xmin = X[:, 0].min() if (xmin is None) else xmin
    xmax = X[:, 0].max() if (xmax is None) else xmax
    x = np.linspace(xmin, xmax, 500)

    # get the mean and confidence bands.
    mu, s2 = model.posterior(x[:, None])
    lo = mu - 2 * np.sqrt(s2)
    hi = mu + 2 * np.sqrt(s2)

    # get the axes.
    ax = pl.gca()

    if color is None:
        color = next(ax._get_lines.color_cycle)

    # default arguments for markers.
    margs = dict(color='k', zorder=3)
    margs = {
        'x': dict(marker='x', facecolors='none', s=30, lw=1, **margs),
        'o': dict(marker='o', facecolors='none', s=30, lw=1, **margs),
        '*': dict(marker='*', facecolors='none', s=30, lw=1, **margs),
        ',': dict(marker=',', facecolors='none', s=30, lw=1, **margs),
        '.': dict(marker='.', **margs)}

    if mean:
        # plot the mean
        ax.plot(x, mu, lw=lw, ls=ls, label='mean', color=color)

    if error:
        # plot the error bars and add an empty plot that will be used by the
        # legend if it's called for.
        alpha = 0.25
        ax.fill_between(x, lo, hi, color=color, alpha=alpha)
        ax.plot([], [], color=color, alpha=alpha, linewidth=10,
                label='uncertainty')

    if data and X is not None:
        # plot the data; use smaller markers if we have a lot of data.
        ax.scatter(X.ravel(), y, label='data', **margs[marker])

    if hasattr(model, 'pseudoinputs') and pseudoinputs:
        # plot any pseudo-inputs.
        ymin, ymax = ax.get_ylim()
        u = model.pseudoinputs.ravel()
        v = np.full_like(u, ymin + 0.1 * (ymax-ymin))
        ax.scatter(u, v, label='pseudo-inputs', **margs[marker2])

    pl.axis('tight')


def plot_samples(model):
    """
    Plot the posterior over hyperparameters for a sample-based meta model.
    """
    # get the figure and clear it.
    fg = pl.gcf()
    fg.clf()

    samples = np.array(list(m.get_hyper() for m in model))
    values = np.zeros((samples.shape[0], 0))
    labels = []

    for key, block, log in get_params(next(model.__iter__())):
        for i in range(block.start, block.stop):
            vals = samples[:, i]
            size = block.stop - block.start
            name = key + ('' if (size == 1) else '_%d' % (i - block.start))
            if not np.allclose(vals, vals[0]):
                values = np.c_[values, np.exp(vals) if log else vals]
                labels.append(name)

    naxes = values.shape[1]

    if naxes == 1:
        ax = fg.add_subplot(111)
        ax.hist(values[:, 0], bins=20)
        ax.set_xlabel(labels[0])
        ax.set_yticklabels([])

    else:
        for i, j in np.ndindex(naxes, naxes):
            if i >= j:
                continue
            ax = fg.add_subplot(naxes-1, naxes-1, (j-1)*(naxes-1)+i+1)
            ax.scatter(values[:, i], values[:, j], alpha=0.1)

            if i == 0:
                ax.set_ylabel(labels[j])
            else:
                ax.set_yticklabels([])

            if j == naxes-1:
                ax.set_xlabel(labels[i])
            else:
                ax.set_xticklabels([])
"""
Objects implementing priors (and sampling from them).
"""

# pylint: disable=wildcard-import
from .priors import *

# import the named modules themselves.
from . import priors

# export everything.
__all__ = []
__all__ += priors.__all__
"""
Implementations of various prior objects.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# exported symbols
__all__ = ['Uniform']


class Uniform(object):
    def __init__(self, a, b):
        self._a = np.array(a, copy=True, ndmin=1)
        self._b = np.array(b, copy=True, ndmin=1)
        self.ndim = len(self._a)

        if len(self._a) != len(self._b):
            raise RuntimeError("bound sizes don't match")

        if np.any(self._b < self._a):
            raise RuntimeError("malformed upper/lower bounds")

    def sample(self, size=1, log=True):
        sample = self._a + (self._b - self._a) * np.random.rand(size, self.ndim)
        return np.log(sample) if log else sample

    def logprior(self, theta):
        theta = np.array(theta, copy=False, ndmin=1)
        for a, b, t in zip(self._a, self._b, theta):
            if (t < a) or (t > b):
                return -np.inf
        return 0.0
"""
Modifications to ABC to allow for additional metaclass actions.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
from abc import ABCMeta as ABCMeta_
from abc import abstractmethod

# exported symbols
__all__ = ['ABCMeta', 'abstractmethod']


class ABCMeta(ABCMeta_):
    """
    Slight modification to ABCMeta that copies docstrings from an
    abstractmethod to its implementation if the implementation lacks a
    docstring.
    """
    def __new__(mcs, name, bases, attrs):
        abstracts = dict(
            (attr, getattr(base, attr))
            for base in bases
            for attr in getattr(base, '__abstractmethods__', set()))

        for attr, value in attrs.items():
            implements = (attr in abstracts and
                          not getattr(value, '__isabstractmethod__', False))
            if implements and not getattr(value, '__doc__', False):
                docstring = getattr(abstracts[attr], '__doc__', None)
                setattr(value, '__doc__', docstring)

        return super(ABCMeta, mcs).__new__(mcs, name, bases, attrs)
"""
Exception classes.
"""

# exported symbols
__all__ = ['ModelError']


class Error(Exception):
    pass


class ModelError(Error):
    pass
"""
Interfaces for parameterized objects.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import copy

# local imports
from .abc import ABCMeta, abstractmethod

# exported symbols
__all__ = ['Parameterized', 'printable', 'dot_params', 'get_params']


class Parameterized(object):
    """
    Interface for objects that are parameterized by some set of
    hyperparameters.
    """
    __metaclass__ = ABCMeta

    @abstractmethod
    def _params(self):
        """
        Define the set of parameters for the model. This should return a list
        of tuples of the form `(name, size, islog)`. If only a 2-tuple is given
        then islog will be assumed to be `True`.
        """
        pass

    @abstractmethod
    def get_hyper(self):
        """Return a vector of model hyperparameters."""
        pass

    @abstractmethod
    def set_hyper(self, hyper):
        """Set the model hyperparameters to the given vector."""
        pass

    def copy(self, hyper=None):
        """
        Copy the model. If `hyper` is given use this vector to immediately set
        the copied model's hyperparameters.
        """
        model = copy.deepcopy(self)
        if hyper is not None:
            model.set_hyper(hyper)
        return model


def printable(cls):
    """
    Decorator which marks classes as being able to be pretty-printed as a
    function of their hyperparameters. This decorator defines a __repr__ method
    for the given class which uses the class's `get_hyper` and `_params`
    methods to print it.
    """
    def _repr(obj):
        """Represent the object as a function of its hyperparameters."""
        hyper = obj.get_hyper()
        substrings = []
        for key, block, log in get_params(obj):
            val = hyper[block]
            val = val[0] if (len(val) == 1) else val
            val = np.exp(val) if log else val
            substrings += ['%s=%s' % (key, val)]
        return obj.__class__.__name__ + '(' + ', '.join(substrings) + ')'
    cls.__repr__ = _repr
    return cls


# FIXME: it's unclear how useful dot_params is. This might be replaced.

def dot_params(ns, params):
    """
    Extend a param tuple with a 'namespace'. IE prepend the key string with ns
    plus a dot.
    """
    return [("%s.%s" % (ns, p[0]),) + p[1:] for p in params]


# FIXME: the get_params function is kind of a hack in order to allow for
# simpler definitions of the _params() method. This should probably be
# replaced.

def get_params(obj):
    """
    Helper function which translates the values returned by _params() into
    something more meaningful.
    """
    offset = 0
    for param in obj._params():
        key = param[0]
        size = param[1]
        block = slice(offset, offset+size)
        log = (len(param) < 3) or param[2]
        offset += size
        yield key, block, log
"""
Simple utilities for random number generation.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# exported symbols
__all__ = ['rstate']


def rstate(rng=None):
    """
    Return a numpy RandomState object. If an integer value is given then a new
    RandomState will be returned with this seed. If None is given then the
    global numpy state will be returned. If an already instantiated state is
    given this will be passed back.
    """
    if rng is None:
        rng = np.random.mtrand._rand
    elif isinstance(rng, int):
        rng = np.random.RandomState(rng)
    elif not isinstance(rng, np.random.RandomState):
        raise ValueError('unknown seed given to rstate')
    return rng
"""
Basic demo showing how to instantiate a simple GP model, add data to it, and
optimize its hyperparameters.
"""

# global imports.
import os
import numpy as np
import matplotlib.pyplot as pl

# local imports
import pygp
import pygp.plotting as pp


if __name__ == '__main__':
    # load the data.
    cdir = os.path.abspath(os.path.dirname(__file__))
    data = np.load(os.path.join(cdir, 'xy.npz'))
    X = data['X']
    y = data['y']

    # create the model, add data, and optimize it.
    gp = pygp.BasicGP(sn=.1, sf=1, ell=.1, mu=0)
    gp.add_data(X, y)
    pygp.optimize(gp)

    # plot the posterior.
    pl.figure(1)
    pl.clf()
    pp.plot_posterior(gp)
    pl.legend(loc=2)
    pl.draw()
    pl.show()
"""
Simple demo showing how to use a more complicated kernel. This fits the data of
CO_2 levels at Mauna Loa; see chapter 5 of Rasmussen and Williams. Note that we
don't fit the hyperparameters as the values given below are reasonable and this
would just take some time.
"""

import os
import numpy as np
import matplotlib.pyplot as pl

import pygp
import pygp.plotting as pp
import pygp.kernels as pk


# load the file from the current directory and get rid of any censored data.
cdir = os.path.abspath(os.path.dirname(__file__))
data = np.loadtxt(os.path.join(cdir, 'maunaloa.txt')).flatten()
data = np.array([(x, y) for x, y in enumerate(data) if y > -99])

# minor manipulations of the data to make the ranges reasonable.
X = data[:, 0, None] / 12. + 1958
y = data[:, 1]

# these are near the values called for in Rasmussen and Williams, so they
# should give reasonable results and thus we'll skip the fit.
kernel = \
    pk.SE(67, 66) + \
    pk.SE(2.4, 90) * pk.Periodic(1, 1, 1) + \
    pk.RQ(1.2, .66, 0.78) + \
    pk.SE(0.15, 0.15)

# use a gaussian likeihood with this standard deviation.
likelihood = pygp.likelihoods.Gaussian(sigma=0.2)

# construct the model and add the data.
gp = pygp.inference.ExactGP(likelihood, kernel, y.mean())
gp.add_data(X, y)

# plot everything.
pl.figure(1)
pl.clf()
pp.plot_posterior(gp, mean=False, xmax=2020, marker='.')
pl.legend(loc='upper left')
pl.draw()
pl.show()
"""
Basic demo plotting the resulting hyperparameter samples by using MCMC.
"""

# global imports.
import os
import numpy as np
import matplotlib.pyplot as pl

# local imports
import pygp
import pygp.priors
import pygp.plotting as pp


if __name__ == '__main__':
    # load the data.
    cdir = os.path.abspath(os.path.dirname(__file__))
    data = np.load(os.path.join(cdir, 'xy.npz'))
    X = data['X']
    y = data['y']

    # create the model and add data to it.
    model = pygp.BasicGP(sn=.1, sf=1, ell=.1)
    model.add_data(X, y)

    # find the ML hyperparameters and plot the predictions.
    pygp.optimize(model)

    # create a prior structure.
    priors = {
        'sn': pygp.priors.Uniform(0.01, 1.0),
        'sf': pygp.priors.Uniform(0.01, 5.0),
        'ell': pygp.priors.Uniform(0.01, 1.0),
        'mu': pygp.priors.Uniform(-2, 2)}

    # create a sample-based model.
    mcmc = pygp.meta.MCMC(model, priors, n=5000)

    pl.figure(1)
    pp.plot_samples(mcmc)
    pl.draw()
    pl.show()
"""
Basic demo showing how to instantiate a simple GP model, add data to it, and
optimize its hyperparameters.
"""

# global imports.
import os
import numpy as np
import matplotlib.pyplot as pl

# local imports
import pygp
import pygp.priors
import pygp.plotting as pp


if __name__ == '__main__':
    # load the data.
    cdir = os.path.abspath(os.path.dirname(__file__))
    data = np.load(os.path.join(cdir, 'xy.npz'))
    X = data['X']
    y = data['y']

    # create the model and add data to it.
    model = pygp.BasicGP(sn=.1, sf=1, ell=.1)
    model.add_data(X, y)

    # find the ML hyperparameters and plot the predictions.
    pygp.optimize(model)

    # create a prior structure.
    priors = {
        'sn': pygp.priors.Uniform(0.01, 1.0),
        'sf': pygp.priors.Uniform(0.01, 5.0),
        'ell': pygp.priors.Uniform(0.01, 1.0),
        'mu': pygp.priors.Uniform(-2, 2)}

    # create sample-based models.
    mcmc = pygp.meta.MCMC(model, priors, n=200, burn=100)
    smc = pygp.meta.SMC(model, priors, n=200)

    pl.figure(1)
    pl.clf()

    pl.subplot(131)
    pp.plot_posterior(model)
    pl.title('Type-II ML')
    pl.legend(loc='best')

    axis = pl.axis()

    pl.subplot(132)
    pp.plot_posterior(mcmc)
    pl.axis(axis)
    pl.title('MCMC')

    pl.subplot(133)
    pp.plot_posterior(mcmc)
    pl.axis(axis)
    pl.title('SMC')
    pl.draw()
    pl.show()
"""
Basic demo showing how to instantiate a simple GP model, add data to it, and
optimize its hyperparameters.
"""

# global imports.
import os
import numpy as np
import matplotlib.pyplot as pl

# local imports
import pygp
import pygp.plotting as pp


if __name__ == '__main__':
    # load the data.
    cdir = os.path.abspath(os.path.dirname(__file__))
    data = np.load(os.path.join(cdir, 'xy.npz'))
    X = data['X']
    y = data['y']

    # create a basic GP.
    gp1 = pygp.BasicGP(sn=.1, sf=1, ell=.1)
    gp1.add_data(X, y)

    # create a sparse GPs.
    U = np.linspace(-1.3, 2, 10)[:, None]
    gp2 = pygp.inference.FITC.from_gp(gp1, U)
    gp3 = pygp.inference.DTC.from_gp(gp1, U)

    # find the ML parameters
    pygp.optimize(gp1)
    pygp.optimize(gp2)
    pygp.optimize(gp3)

    # plot the dense gp.
    pl.figure(1)
    pl.clf()
    pl.subplot(131)
    pp.plot_posterior(gp1)
    pl.title('Full GP')

    # grab the axis limits.
    axis = pl.axis()

    # plot the FITC sparse gp.
    pl.subplot(132)
    pp.plot_posterior(gp2, pseudoinputs=True)
    pl.title('Sparse GP (FITC)')
    pl.axis(axis)
    pl.draw()

    # plot the sparse gp.
    pl.subplot(133)
    pp.plot_posterior(gp3, pseudoinputs=True)
    pl.title('Sparse GP (DTC)')
    pl.axis(axis)
    pl.legend(loc='upper left')
    pl.draw()
    pl.show()
"""
Interface to GP inference.
"""

# import the basic things by default
from . import inference
from . import kernels
from . import learning
from . import likelihoods
from . import meta
from . import priors

# import the basic things by default
from .inference import BasicGP
from .learning import optimize

# and make them available.
__all__ = ['BasicGP', 'optimize']
"""
Objects which implement GP inference.
"""

# pylint: disable=wildcard-import
from .exact import *
from .fitc import *
from .basic import *
from .dtc import *

from . import exact
from . import fitc
from . import basic
from . import dtc

__all__ = []
__all__ += exact.__all__
__all__ += fitc.__all__
__all__ += basic.__all__
__all__ += dtc.__all__
"""
Interface for latent function inference in Gaussian process models. These
models will assume that the hyperparameters are fixed and any optimization
and/or sampling of these parameters will be left to a higher-level wrapper.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import scipy.linalg as sla

# local imports
from ..utils.abc import abstractmethod
from ..utils.models import Parameterized, dot_params
from ..utils.random import rstate
from ._fourier import FourierSample

# exported symbols
__all__ = ['GP']


class GP(Parameterized):
    """
    GP inference interface.

    This class defines the GP interface. Although it implements the
    Parameterized interface it defines additional abstract methods and is still
    abstract as a result.

    The methods that must be implemented are:

        `_update`: update internal statistics given all the data.
        `_posterior`: compute the full posterior for use with sampling.
        `posterior`: compute the marginal posterior and its gradient.
        `loglikelihood`: compute the loglikelihood of observed data.

    Additionally, the following method can be implemented for improved
    performance in some circumstances:

        `_updateinc`: incremental update given new data.
    """
    def __init__(self, likelihood, kernel, mean):
        self._likelihood = likelihood
        self._kernel = kernel
        self._mean = float(mean)
        self._X = None
        self._y = None

        # record the number of hyperparameters. the additional +1 is due to the
        # mean hyperparameter.
        self.nhyper = (self._likelihood.nhyper +
                       self._kernel.nhyper + 1)

    def reset(self):
        """Remove all data from the model."""
        self._X = None
        self._y = None

    def __repr__(self):
        def indent(pre, text):
            return pre + ('\n' + ' '*len(pre)).join(text.splitlines())

        return indent(
            self.__class__.__name__ + '(',
            ',\n'.join([
                indent('likelihood=', repr(self._likelihood)),
                indent('kernel=', repr(self._kernel)),
                indent('mean=', str(self._mean))]) + ')')

    def _params(self):
        params = dot_params('like', self._likelihood._params())
        params += dot_params('kern', self._kernel._params())
        params += [('mean', 1, False)]
        return params

    @classmethod
    def from_gp(cls, gp, *args, **kwargs):
        """
        Create a new GP object given another. This allows one to make a "copy"
        of a GP using the same likelihood, kernel, etc. and using the same
        data, but possibly a different inference method.
        """
        args = (gp._likelihood.copy(), gp._kernel.copy(), gp._mean) + args
        newgp = cls(*args, **kwargs)
        if gp.ndata > 0:
            X, y = gp.data
            newgp.add_data(X, y)
        return newgp

    def get_hyper(self):
        # NOTE: if subclasses define any "inference" hyperparameters they can
        # implement their own get/set methods and call super().
        return np.r_[self._likelihood.get_hyper(),
                     self._kernel.get_hyper(),
                     self._mean]

    def set_hyper(self, hyper):
        # FIXME: should set_hyper check the number of hyperparameters?
        a = self._likelihood.nhyper
        b = self._kernel.nhyper

        self._likelihood.set_hyper(hyper[:a])
        self._kernel.set_hyper(hyper[a:a+b])
        self._mean = hyper[-1]

        if self.ndata > 0:
            self._update()

    @property
    def ndata(self):
        """The number of current input/output data pairs."""
        return 0 if (self._X is None) else self._X.shape[0]

    @property
    def data(self):
        """The current input/output data."""
        return (self._X, self._y)

    def add_data(self, X, y):
        """
        Add new data to the GP model.
        """
        X = self._kernel.transform(X)
        y = self._likelihood.transform(y)

        if self._X is None:
            self._X = X.copy()
            self._y = y.copy()
            self._update()

        else:
            try:
                self._updateinc(X, y)
                self._X = np.r_[self._X, X]
                self._y = np.r_[self._y, y]

            except NotImplementedError:
                self._X = np.r_[self._X, X]
                self._y = np.r_[self._y, y]
                self._update()

    def sample(self, X, m=None, latent=True, rng=None):
        """
        Sample values from the posterior at points `X`. Given an `(n,d)`-array
        `X` this will return an `n`-vector corresponding to the resulting
        sample.

        If `m` is not `None` an `(m,n)`-array will be returned instead,
        corresponding to `m` such samples. If `latent` is `False` the sample
        will instead be returned corrupted by the observation noise. Finally
        `rng` can be used to seed the randomness.
        """
        X = self._kernel.transform(X)

        # this boolean indicates whether we'll flatten the sample to return a
        # vector, or if we'll return a set of samples as an array.
        flatten = (m is None)

        # get the relevant sizes.
        m = 1 if flatten else m
        n = len(X)

        # if a seed or instantiated RandomState is given use that, otherwise
        # use the global object.
        rng = rstate(rng)

        # add a tiny amount to the diagonal to make the cholesky of Sigma
        # stable and then add this correlated noise onto mu to get the sample.
        mu, Sigma = self._full_posterior(X)
        Sigma += 1e-10 * np.eye(n)
        f = mu[None] + np.dot(rng.normal(size=(m, n)), sla.cholesky(Sigma))

        if not latent:
            f = self._likelihood.sample(f.ravel(), rng).reshape(m, n)

        return f.ravel() if flatten else f

    def posterior(self, X, grad=False):
        """
        Return the marginal posterior. This should return the mean and variance
        of the given points, and if `grad == True` should return their
        derivatives with respect to the input location as well (i.e. a
        4-tuple).
        """
        return self._marg_posterior(self._kernel.transform(X), grad)

    def sample_fourier(self, N, rng=None):
        """
        Approximately sample a function from the GP using a fourier-basis
        expansion with N bases. See the documentation on `FourierSample` for
        details on the returned function object.
        """
        return FourierSample(N,
                             self._likelihood, self._kernel, self._mean,
                             self._X, self._y, rng)

    @abstractmethod
    def _update(self):
        """
        Update any internal parameters (ie sufficient statistics) given the
        entire set of current data.
        """

    # NOTE: the following method is not abstract since we don't require that it
    # is implemented. if it is not implemented the full _update is performed
    # when new data is added.

    def _updateinc(self, X, y):
        """
        Update any internal parameters given additional data in the form of
        input/output pairs `X` and `y`. This method is called before data is
        appended to the internal data-store and no subsequent call to `_update`
        is performed.
        """
        raise NotImplementedError

    @abstractmethod
    def _full_posterior(self, X):
        """
        Compute the full posterior at points `X`. Return the mean vector and
        full covariance matrix for the given inputs.
        """

    @abstractmethod
    def _marg_posterior(self, X, grad=False):
        """
        Compute the marginal posterior at points `X`. Return the mean and
        variance vectors for the given inputs. If `grad` is True return the
        gradients with respect to the inputs as well.
        """

    @abstractmethod
    def loglikelihood(self, grad=False):
        """
        Return the marginal loglikelihood of the data. If `grad == True` also
        return the gradient with respect to the hyperparameters.
        """
"""
Approximations to the GP using random Fourier features.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import scipy.linalg as sla

# local imports
from ..utils.random import rstate
from ..utils.exceptions import ModelError
from ..likelihoods import Gaussian

# exported symbols
__all__ = ['FourierSample']


class FourierSample(object):
    def __init__(self, N, likelihood, kernel, mean, X, y, rng=None):
        # if given a seed or an instantiated RandomState make sure that we use
        # it here, but also within the sample_spectrum code.
        rng = rstate(rng)

        if not isinstance(likelihood, Gaussian):
            raise ModelError('Fourier samples only defined for Gaussian'
                             'likelihoods')

        # this randomizes the feature.
        W, alpha = kernel.sample_spectrum(N, rng)

        self._W = W
        self._b = rng.rand(N) * 2 * np.pi
        self._a = np.sqrt(2 * alpha / N)
        self._mean = mean
        self._theta = None

        if X is not None:
            Phi = self.phi(X)
            A = np.dot(Phi.T, Phi) + likelihood.s2 * np.eye(Phi.shape[1])
            R = sla.cholesky(A)
            r = y - mean

            # FIXME: we can do a smarter update here when the number of points
            # is less than the number of features.

            rnd = np.sqrt(likelihood.s2) * rng.randn(N)

            self._theta = sla.cho_solve((R, False), np.dot(Phi.T, r))
            self._theta += sla.solve_triangular(R, rnd)

        else:
            self._theta = rng.randn(N)

    def phi(self, X):
        """
        Evaluate the random features.
        """
        # x is n-by-D,
        # W is N-by-D,
        # Phi, the return value, should be n-by-N.
        rnd = np.dot(X, self._W.T) + self._b
        Phi = np.cos(rnd) * self._a
        return Phi

    def get(self, X):
        """
        Evaluate the function at a collection of points.
        """
        Phi = self.phi(np.array(X, ndmin=2, copy=False))
        return self._mean + np.dot(Phi, self._theta)

    def __call__(self, x):
        return self.get(x)[0]
"""
Simple wrapper class for a Basic GP.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# local imports
from ..utils.models import printable
from ..likelihoods import Gaussian
from ..kernels import SE, Matern
from .exact import ExactGP

# exported symbols
__all__ = ['BasicGP']


@printable
class BasicGP(ExactGP):
    """
    Basic GP frontend which assumes an ARD kernel and a Gaussian likelihood
    (and hence performs exact inference).
    """
    def __init__(self, sn, sf, ell, mu=0, ndim=None, kernel='se'):
        likelihood = Gaussian(sn)
        kernel = (
            SE(sf, ell, ndim) if (kernel == 'se') else
            Matern(sf, ell, 1, ndim) if (kernel == 'matern1') else
            Matern(sf, ell, 3, ndim) if (kernel == 'matern3') else
            Matern(sf, ell, 5, ndim) if (kernel == 'matern5') else None)

        if kernel is None:
            raise RuntimeError('Unknown kernel type')

        super(BasicGP, self).__init__(likelihood, kernel, mu)

    def _params(self):
        # replace the parameters for the base GP model with a simplified
        # structure and rename the likelihood's sigma parameter to sn (ie its
        # the sigma corresponding to the noise).
        params = [('sn', 1)]
        params += self._kernel._params()
        params += [('mu', 1, False)]
        return params
"""
Nystrom approximate inference in a Gaussian process model
for regression.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import scipy.linalg as sla
import itertools as it

# local imports
from ..utils.exceptions import ModelError
from ..likelihoods import Gaussian
from ._base import GP

# exported symbols
__all__ = ['DTC']


class DTC(GP):
    """Deterministic training conditional approximation to GP inference."""

    def __init__(self, likelihood, kernel, mean, U):
        # NOTE: exact inference will only work with Gaussian likelihoods.
        if not isinstance(likelihood, Gaussian):
            raise ModelError('exact inference requires a Gaussian likelihood')

        super(DTC, self).__init__(likelihood, kernel, mean)
        # save the pseudo-input locations.
        self._U = np.array(U, ndmin=2, dtype=float, copy=True)

        self._Ruu = None
        self._Rux = None
        self._a = None

    @property
    def pseudoinputs(self):
        """The pseudo-input points."""
        return self._U

    def _update(self):
        p = self._U.shape[0]
        su2 = self._likelihood.s2 * 1e-6

        # choleskies of Kuu and (Kuu + Kfu * Kuf / sn2), respectively,
        # see Eq 20b of (Quinonero-Candela and Rasmussen, 2005)
        Kuu = self._kernel.get(self._U)
        self._Ruu = sla.cholesky(Kuu + su2 * np.eye(p))

        # formulate data-dependent problem
        Kux = self._kernel.get(self._U, self._X)
        S = Kuu + np.dot(Kux, Kux.T) / self._likelihood.s2

        # compute cholesky of data dependent problem
        r = self._y - self._mean
        self._Rux = sla.cholesky(S + su2 * np.eye(p))
        self._a = sla.solve_triangular(self._Rux,
                                       np.dot(Kux, r),
                                       trans=True)

    def _full_posterior(self, X):
        # grab the prior mean and covariance.
        mu = np.full(X.shape[0], self._mean)
        Sigma = self._kernel.get(X)

        if self._X is not None:
            K = self._kernel.get(self._U, X)
            b = sla.solve_triangular(self._Ruu, K, trans=True)
            c = sla.solve_triangular(self._Rux, K, trans=True)

            # add the contribution to the mean coming from the posterior and
            # subtract off the information gained in the posterior from the
            # prior variance.
            mu += np.dot(c.T, self._a) / self._likelihood.s2
            Sigma += -np.dot(b.T, b) + np.dot(c.T, c)

        return mu, Sigma

    def _marg_posterior(self, X, grad=False):
        # grab the prior mean and variance.
        mu = np.full(X.shape[0], self._mean)
        s2 = self._kernel.dget(X)

        if self._X is not None:
            K = self._kernel.get(self._U, X)
            b = sla.solve_triangular(self._Ruu, K, trans=True)
            c = sla.solve_triangular(self._Rux, K, trans=True)

            # add the contribution to the mean coming from the posterior and
            # subtract off the information gained in the posterior from the
            # prior variance.
            mu += np.dot(c.T, self._a) / self._likelihood.s2
            s2 += -np.sum(b * b, axis=0) + np.sum(c * c, axis=0)

        if not grad:
            return (mu, s2)

        # Get the prior gradients. Note that this assumes a constant mean and
        # stationary kernel.
        dmu = np.zeros_like(X)
        ds2 = np.zeros_like(X)

        if self._X is not None:
            dK = self._kernel.grady(self._U, X)
            dK = dK.reshape(self._U.shape[0], -1)

            db = sla.solve_triangular(self._Ruu, dK, trans=True)
            db = np.rollaxis(np.reshape(db, (-1,) + X.shape), 2)

            dc = sla.solve_triangular(self._Rux, dK, trans=True)
            dmu += np.dot(dc.T, self._a).reshape(X.shape)

            dc = np.rollaxis(np.reshape(dc, (-1,) + X.shape), 2)
            ds2 += -2 * np.sum(db * b, axis=1).T + \
                    2 * np.sum(dc * c, axis=1).T

        return (mu, s2, dmu, ds2)

    def loglikelihood(self, grad=False):
        # noise hyperparameters
        sn2 = self._likelihood.s2
        su2 = sn2 * 1e-6
        ell = np.sqrt(sn2)

        # get the rest of the kernels and the residual.
        Kux = self._kernel.get(self._U, self._X)
        r = self._y.copy() - self._mean
        r /= ell

        # the cholesky of Q.
        V = sla.solve_triangular(self._Ruu, Kux, trans=True)
        V /= ell

        p = self._U.shape[0]
        A = sla.cholesky(np.eye(p) + np.dot(V, V.T))
        beta = sla.solve_triangular(A, V.dot(r), trans=True)

        lZ = -np.sum(np.log(np.diag(A))) - self.ndata * np.log(ell)
        lZ -= 0.5 * (np.inner(r, r) - np.inner(beta, beta))
        lZ -= 0.5 * self.ndata * np.log(2*np.pi)

        if not grad:
            return lZ

        alpha = (r - V.T.dot(sla.solve_triangular(A, beta)))
        B = sla.solve_triangular(self._Ruu, V)
        W = sla.solve_triangular(A, V, trans=True)
        VW = np.dot(V, W.T)
        BW = np.dot(B, W.T)
        w = B.dot(alpha)
        v = V.dot(alpha)

        # allocate space for the gradients.
        dlZ = np.zeros(self.nhyper)

        # gradient wrt the noise parameter.
        dlZ[0] = -(
            # gradient of the mahalanobis term
            - np.inner(r, r)
            + np.inner(beta, beta)
            + np.inner(v, v)
            + su2 * np.inner(w, w)
            # gradient of the log determinant term
            + self.ndata
            - np.sum(V**2)
            + np.sum(VW**2)
            - su2 * (np.sum(B**2) - np.sum(BW**2)))

        # iterator over gradients of the kernels
        dK = it.izip(
            self._kernel.grad(self._U),
            self._kernel.grad(self._U, self._X))

        # gradient wrt the kernel hyperparameters.
        i = 1
        for i, (dKuu, dKux) in enumerate(dK, i):
            M = 2 * dKux / ell - dKuu.dot(B)
            dlZ[i] = -0.5 * (
                - np.inner(w, np.dot(M, alpha))
                + np.sum(M*B)
                - np.sum(M.dot(W.T) * B.dot(W.T)))

        # gradient wrt the constant mean.
        dlZ[-1] = np.sum(alpha) / ell

        return lZ, dlZ
"""
Implementation of exact latent-function inference in a Gaussian process model
for regression.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import scipy.linalg as sla

# local imports
from ..utils.exceptions import ModelError
from ..likelihoods import Gaussian
from ._base import GP

# exported symbols
__all__ = ['ExactGP']


class ExactGP(GP):
    """
    Exact GP inference.

    This class implements exact inference for GPs. Note that exact inference
    only works with regression so an exception will be thrown if the given
    likelihood is not Gaussian.
    """
    def __init__(self, likelihood, kernel, mean):
        # NOTE: exact inference will only work with Gaussian likelihoods.
        if not isinstance(likelihood, Gaussian):
            raise ModelError('exact inference requires a Gaussian likelihood')

        super(ExactGP, self).__init__(likelihood, kernel, mean)
        self._R = None
        self._a = None

    def reset(self):
        for attr in 'Ra':
            setattr(self, '_' + attr, None)
        super(ExactGP, self).reset()

    def _update(self):
        sn2 = self._likelihood.s2
        K = self._kernel.get(self._X) + sn2 * np.eye(len(self._X))
        r = self._y - self._mean
        self._R = sla.cholesky(K)
        self._a = sla.solve_triangular(self._R, r, trans=True)

    def _updateinc(self, X, y):
        sn2 = self._likelihood.s2
        Kss = self._kernel.get(X) + sn2 * np.eye(len(X))
        Kxs = self._kernel.get(self._X, X)
        r = y - self._mean
        self._R, self._a = chol_update(self._R, Kxs, Kss, self._a, r)

    def _full_posterior(self, X):
        # grab the prior mean and covariance.
        mu = np.full(X.shape[0], self._mean)
        Sigma = self._kernel.get(X)

        if self._X is not None:
            K = self._kernel.get(self._X, X)
            V = sla.solve_triangular(self._R, K, trans=True)

            # add the contribution to the mean coming from the posterior and
            # subtract off the information gained in the posterior from the
            # prior variance.
            mu += np.dot(V.T, self._a)
            Sigma -= np.dot(V.T, V)

        return mu, Sigma

    def _marg_posterior(self, X, grad=False):
        # grab the prior mean and variance.
        mu = np.full(X.shape[0], self._mean)
        s2 = self._kernel.dget(X)

        if self._X is not None:
            K = self._kernel.get(self._X, X)
            RK = sla.solve_triangular(self._R, K, trans=True)

            # add the contribution to the mean coming from the posterior and
            # subtract off the information gained in the posterior from the
            # prior variance.
            mu += np.dot(RK.T, self._a)
            s2 -= np.sum(RK**2, axis=0)

        if not grad:
            return (mu, s2)

        # Get the prior gradients.
        dmu = np.zeros_like(X)
        ds2 = np.zeros_like(X)

        # NOTE: the above assumes a constant mean and stationary kernel (which
        # we satisfy, but should we change either assumption...).

        if self._X is not None:
            dK = self._kernel.grady(self._X, X)
            dK = dK.reshape(self.ndata, -1)

            RdK = sla.solve_triangular(self._R, dK, trans=True)
            dmu += np.dot(RdK.T, self._a).reshape(X.shape)

            RdK = np.rollaxis(np.reshape(RdK, (-1,) + X.shape), 2)
            ds2 -= 2 * np.sum(RdK * RK, axis=1).T

        return (mu, s2, dmu, ds2)

    def loglikelihood(self, grad=False):
        lZ = -0.5 * np.inner(self._a, self._a)
        lZ -= 0.5 * np.log(2 * np.pi) * self.ndata
        lZ -= np.sum(np.log(self._R.diagonal()))

        # bail early if we don't need the gradient.
        if not grad:
            return lZ

        # intermediate terms.
        alpha = sla.solve_triangular(self._R, self._a, trans=False)
        Q = sla.cho_solve((self._R, False), np.eye(self.ndata))
        Q -= np.outer(alpha, alpha)

        dlZ = np.r_[
            # derivative wrt the likelihood's noise term.
            -self._likelihood.s2 * np.trace(Q),

            # derivative wrt each kernel hyperparameter.
            [-0.5*np.sum(Q*dK)
             for dK in self._kernel.grad(self._X)],

            # derivative wrt the mean.
            np.sum(alpha)]

        return lZ, dlZ


def chol_update(A, B, C, a, b):
    """
    Update the cholesky decomposition of a growing matrix.

    Let `A` denote a cholesky decomposition of some matrix and `a` the inverse
    of `A` applied to some vector `y`. This computes the cholesky to a new
    matrix which has additional elements `B` and the non-diagonal and `C` on
    the diagonal block. It also computes the solution to the application of the
    inverse where the vector has additional elements `b`.
    """
    n = A.shape[0]
    m = C.shape[0]

    B = sla.solve_triangular(A, B, trans=True)
    C = sla.cholesky(C - np.dot(B.T, B))
    c = np.dot(B.T, a)

    # grow the new cholesky and use then use this to grow the vector a.
    A = np.r_[np.c_[A, B], np.c_[np.zeros((m, n)), C]]
    a = np.r_[a, sla.solve_triangular(C, b-c, trans=True)]

    return A, a
"""
FITC approximation for sparse pseudo-input GPs.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import scipy.linalg as sla
import itertools as it

# local imports
from ..utils.exceptions import ModelError
from ..likelihoods import Gaussian
from ._base import GP

# exported symbols
__all__ = ['FITC']


class FITC(GP):
    """
    GP inference using sparse pseudo-inputs.
    """
    def __init__(self, likelihood, kernel, mean, U):
        # NOTE: exact FITC inference will only work with Gaussian likelihoods.
        if not isinstance(likelihood, Gaussian):
            raise ModelError('exact inference requires a Gaussian likelihood')

        super(FITC, self).__init__(likelihood, kernel, mean)

        # save the pseudo-input locations.
        self._U = np.array(U, ndmin=2, dtype=float, copy=True)

        # sufficient statistics that we'll need.
        self._L = None
        self._R = None
        self._b = None

        # these are useful in computing the loglikelihood and updating the
        # sufficient statistics.
        self._A = None
        self._a = None

    def reset(self):
        for attr in 'LRbAa':
            setattr(self, '_' + attr, None)
        super(FITC, self).reset()

    @property
    def pseudoinputs(self):
        """The pseudo-input points."""
        return self._U

    def _update(self):
        sn2 = self._likelihood.s2
        su2 = sn2 / 1e6

        # kernel wrt the inducing points.
        Kuu = self._kernel.get(self._U)
        p = self._U.shape[0]

        # cholesky for the information gain. note that we only need to compute
        # this once as it is independent from the data.
        self._L = sla.cholesky(Kuu + su2*np.eye(p))

        # evaluate the kernel and residuals at the new points
        Kux = self._kernel.get(self._U, self._X)
        kxx = self._kernel.dget(self._X)
        r = self._y - self._mean

        # the cholesky of Q.
        V = sla.solve_triangular(self._L, Kux, trans=True)

        # rescale everything by the diagonal matrix ell.
        ell = np.sqrt(kxx + sn2 - np.sum(V**2, axis=0))
        Kux /= ell
        V /= ell
        r /= ell

        # NOTE: to update things incrementally all we need to do is store these
        # components. A just needs to be initialized at the identity and then
        # we just accumulate here.
        self._A = np.eye(p) + np.dot(V, V.T)
        self._a = np.dot(Kux, r)

        # update the posterior.
        self._R = np.dot(sla.cholesky(self._A), self._L)
        self._b = sla.solve_triangular(self._R, self._a, trans=True)

    def _full_posterior(self, X):
        mu = np.full(X.shape[0], self._mean)
        Sigma = self._kernel.get(X)

        if self._X is not None:
            # get the kernel and do two backsolves by the lower-dimensional
            # choleskys that we've stored.
            K = self._kernel.get(self._U, X)
            LK = sla.solve_triangular(self._L, K, trans=True)
            RK = sla.solve_triangular(self._R, K, trans=True)

            # add on the posterior mean contribution and reduce the variance
            # based on the information that we gain from the posterior but add
            # additional uncertainty the further away we are from the inducing
            # points.
            mu += np.dot(RK.T, self._b)
            Sigma += np.dot(RK.T, RK) - np.dot(LK.T, LK)

        return mu, Sigma

    def _marg_posterior(self, X, grad=False):
        # grab the prior mean and variance.
        mu = np.full(X.shape[0], self._mean)
        s2 = self._kernel.dget(X)

        if self._X is not None:
            # get the kernel and do two backsolves by the lower-dimensional
            # choleskys that we've stored.
            K = self._kernel.get(self._U, X)
            LK = sla.solve_triangular(self._L, K, trans=True)
            RK = sla.solve_triangular(self._R, K, trans=True)

            # add on the posterior mean contribution and reduce the variance
            # based on the information that we gain from the posterior but add
            # additional uncertainty the further away we are from the inducing
            # points.
            mu += np.dot(RK.T, self._b)
            s2 += np.sum(RK**2, axis=0) - np.sum(LK**2, axis=0)

        if not grad:
            return (mu, s2)

        # Get the prior gradients. Note that this assumes a constant mean and
        # stationary kernel.
        dmu = np.zeros_like(X)
        ds2 = np.zeros_like(X)

        if self._X is not None:
            p = self._U.shape[0]
            dK = self._kernel.grady(self._U, X)
            dK = dK.reshape(p, -1)

            LdK = sla.solve_triangular(self._L, dK, trans=True)
            RdK = sla.solve_triangular(self._R, dK, trans=True)

            dmu += np.dot(RdK.T, self._b).reshape(X.shape)

            LdK = np.rollaxis(np.reshape(LdK, (p,) + X.shape), 2)
            RdK = np.rollaxis(np.reshape(RdK, (p,) + X.shape), 2)

            ds2 += 2 * np.sum(RdK * RK, axis=1).T
            ds2 -= 2 * np.sum(LdK * LK, axis=1).T

        return (mu, s2, dmu, ds2)

    def loglikelihood(self, grad=False):
        # noise hyperparameters
        sn2 = self._likelihood.s2
        su2 = sn2 / 1e6

        # get the rest of the kernels and the residual.
        Kux = self._kernel.get(self._U, self._X)
        kxx = self._kernel.dget(self._X)
        r = self._y - self._mean

        # the cholesky of Q.
        V = sla.solve_triangular(self._L, Kux, trans=True)

        # rescale everything by the diagonal matrix ell.
        ell = np.sqrt(kxx + sn2 - np.sum(V**2, axis=0))
        V /= ell
        r /= ell

        # Note this A corresponds to chol(A) from _update.
        A = sla.cholesky(self._A)
        beta = sla.solve_triangular(A, V.dot(r), trans=True)
        alpha = (r - V.T.dot(sla.solve_triangular(A, beta))) / ell

        lZ = -np.sum(np.log(np.diag(A))) - np.sum(np.log(ell))
        lZ -= 0.5 * (np.inner(r, r) - np.inner(beta, beta))
        lZ -= 0.5 * ell.shape[0] * np.log(2*np.pi)

        if not grad:
            return lZ

        B = sla.solve_triangular(self._L, V*ell)
        W = sla.solve_triangular(A, V/ell, trans=True)
        w = B.dot(alpha)
        v = 2*su2*np.sum(B**2, axis=0)

        # allocate space for the gradients.
        dlZ = np.zeros(self.nhyper)

        # gradient wrt the noise parameter.
        dlZ[0] = (
            - sn2 * (np.sum(1/ell**2) - np.sum(W**2) - np.inner(alpha, alpha))
            - su2 * (np.sum(w**2) + np.sum(B.dot(W.T)**2))
            + 0.5 * (
                np.inner(alpha, v*alpha) + np.inner(np.sum(W**2, axis=0), v)))

        # iterator over gradients of the kernels
        dK = it.izip(
            self._kernel.grad(self._U),
            self._kernel.grad(self._U, self._X),
            self._kernel.dgrad(self._X))

        # gradient wrt the kernel hyperparameters.
        i = 1
        for i, (dKuu, dKux, dkxx) in enumerate(dK, i):
            M = 2*dKux - dKuu.dot(B)
            v = dkxx - np.sum(M*B, axis=0)
            dlZ[i] = (
                - np.sum(dkxx/ell**2)
                - np.inner(w, dKuu.dot(w) - 2*dKux.dot(alpha))
                + np.inner(alpha, v*alpha) + np.inner(np.sum(W**2, axis=0), v)
                + np.sum(M.dot(W.T) * B.dot(W.T))) / 2.0

        # gradient wrt the constant mean.
        dlZ[-1] = np.sum(alpha)

        return lZ, dlZ
"""
Objects which implement the kernel interface.
"""

# pylint: disable=wildcard-import
from .se import *
from .periodic import *
from .rq import *
from .matern import *

from . import se
from . import periodic
from . import rq
from . import matern

__all__ = []
__all__ += se.__all__
__all__ += periodic.__all__
__all__ += rq.__all__
__all__ += matern.__all__
"""
Definition of the kernel interface.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# local imports
from ..utils.abc import abstractmethod
from ..utils.models import Parameterized

# exported symbols
__all__ = ['Kernel']


### BASE KERNEL INTERFACE #####################################################

class Kernel(Parameterized):
    """
    The base Kernel interface.
    """
    def __call__(self, x1, x2):
        return self.get(x1[None], x2[None])[0]

    @abstractmethod
    def get(self, X1, X2=None):
        """
        Evaluate the kernel.

        Returns the matrix of covariances between points in `X1` and `X2`. If
        `X2` is not given this will return the pairwise covariances between
        points in `X1`.
        """

    @abstractmethod
    def dget(self, X):
        """Evaluate the self covariances."""

    @abstractmethod
    def grad(self, X1, X2=None):
        """
        Evaluate the gradient of the kernel.

        Returns an iterator over the gradients of the covariances between
        points in `X1` and `X2`. If `X2` is not given this will iterate over
        the the gradients of the pairwise covariances.
        """

    @abstractmethod
    def dgrad(self, X):
        """Evaluate the gradients of the self covariances."""

    @abstractmethod
    def transform(self, X):
        """Format the inputs X as arrays."""
"""
Combination classes.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import itertools as it
import functools as ft
import operator as op

# local imports
from ._base import Kernel
from ..utils.models import dot_params

# exported symbols
__all__ = ['ComboKernel', 'SumKernel', 'ProductKernel', 'combine']


### HELPER METHODS ############################################################

def product(fiterable):
    """
    The equivalent object to sum but for products.
    """
    return ft.reduce(op.mul, fiterable, 1)


def product_but(fiterable):
    """
    Given an iterator over function evaluations return an array such that
    `M[i]` is the product of every evaluation except for the ith one.
    """
    A = list(fiterable)

    # allocate memory for M and fill everything but the last element with
    # the product of A[i+1:]. Note that we're using the cumprod in place.
    M = np.empty_like(A)
    np.cumprod(A[:0:-1], axis=0, out=M[:-1][::-1])

    # use an explicit loop to iteratively set M[-1] equal to the product of
    # A[:-1]. While doing this we can multiply M[i] by A[:i].
    M[-1] = A[0]
    for i in xrange(1, len(A)-1):
        M[i] *= M[-1]
        M[-1] *= A[i]

    return M


### GENERAL COMBINATION KERNEL ################################################

class ComboKernel(Kernel):
    """
    Implementation of mixin methods for kernels that are themselves
    combinations of other kernels.
    """
    def __init__(self, *parts):
        self._parts = [part.copy() for part in parts]
        self.nhyper = sum(p.nhyper for p in self._parts)

    def __repr__(self):
        string = self.__class__.__name__ + '('
        indent = len(string) * ' '
        substrings = [repr(p) for p in self._parts]
        string += (',\n').join(substrings) + ')'
        string = ('\n'+indent).join(string.splitlines())
        return string

    def _params(self):
        # this is complicated somewhat because I want to return a flat list of
        # parts. so I avoid calling _params() recursively since we could also
        # contain combo objects.
        params = []
        nparts = 0
        parts = list(reversed(self._parts))
        while len(parts) > 0:
            part = parts.pop()
            if isinstance(part, ComboKernel):
                parts.extend(reversed(part._parts))
            else:
                params.extend(dot_params('part%d' % nparts, part._params()))
                nparts += 1
        return params

    def get_hyper(self):
        return np.hstack(p.get_hyper() for p in self._parts)

    def set_hyper(self, hyper):
        a = 0
        for p in self._parts:
            b = a + p.nhyper
            p.set_hyper(hyper[a:b])
            a = b


### SUM AND PRODUCT KERNELS ###################################################

class SumKernel(ComboKernel):
    """Kernel representing a sum of other kernels."""

    def get(self, X1, X2=None):
        fiterable = (p.get(X1, X2) for p in self._parts)
        return sum(fiterable)

    def dget(self, X):
        fiterable = (p.dget(X) for p in self._parts)
        return sum(fiterable)

    def grad(self, X1, X2=None):
        giterable = (p.grad(X1, X2) for p in self._parts)
        return it.chain.from_iterable(giterable)

    def dgrad(self, X):
        giterable = (p.dgrad(X) for p in self._parts)
        return it.chain.from_iterable(giterable)


class ProductKernel(ComboKernel):
    """Kernel representing a product of other kernels."""

    def get(self, X1, X2=None):
        fiterable = (p.get(X1, X2) for p in self._parts)
        return product(fiterable)

    def dget(self, X):
        fiterable = (p.dget(X) for p in self._parts)
        return product(fiterable)

    def grad(self, X1, X2=None):
        fiterable = (p.get(X1, X2) for p in self._parts)
        giterable = (p.grad(X1, X2) for p in self._parts)
        for Mi, grads in zip(product_but(fiterable), giterable):
            for dM in grads:
                yield Mi*dM

    def dgrad(self, X):
        fiterable = (p.dget(X) for p in self._parts)
        giterable = (p.dgrad(X) for p in self._parts)
        for Mi, grads in zip(product_but(fiterable), giterable):
            for dM in grads:
                yield Mi*dM


### HELPER FOR ASSOCIATIVE OPERATIONS #########################################

def combine(cls, *parts):
    """
    Given a list of kernels return another list of kernels where objects of
    type cls have been "combined". This applies to ComboKernel objects which
    represent associative operations.
    """
    combined = []
    for part in parts:
        combined += part._parts if isinstance(part, cls) else [part]
    return combined
"""
Implementation of distance computations.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import scipy.spatial.distance as ssd

# exported symbols
__all__ = ['rescale', 'sqdist', 'sqdist_foreach']


def rescale(ell, X1, X2):
    """
    Rescale the two sets of vectors by `ell`.
    """
    X1 = X1 / ell
    X2 = X2 / ell if (X2 is not None) else None
    return X1, X2


def diff(X1, X2=None):
    """
    Return the differences between vectors in `X1` and `X2`. If `X2` is not
    given this will return the pairwise differences in `X1`.
    """
    X2 = X1 if (X2 is None) else X2
    return X1[:, None, :] - X2[None, :, :]


def sqdist(X1, X2=None):
    """
    Return the squared-distance between two sets of vector. If `X2` is not
    given this will return the pairwise squared-distances in `X1`.
    """
    X2 = X1 if (X2 is None) else X2
    return ssd.cdist(X1, X2, 'sqeuclidean')


def sqdist_foreach(X1, X2=None):
    """
    Return an iterator over each dimension returning the squared-distance
    between two sets of vector. If `X2` is not given this will iterate over the
    pairwise squared-distances in `X1` in each dimension.
    """
    X2 = X1 if (X2 is None) else X2
    for i in xrange(X1.shape[1]):
        yield ssd.cdist(X1[:, i, None], X2[:, i, None], 'sqeuclidean')
"""
Base class for real-valued kernels.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ._base import Kernel
from ..utils.abc import abstractmethod

# import the generic sum/product kernels and change their names. We'll call the
# real-valued versions SumKernel and ProductKernel as well since they really
# shouldn't be used outside of this module anyway.
from ._combo import SumKernel as SumKernel_
from ._combo import ProductKernel as ProductKernel_
from ._combo import combine
from ._combo import product_but

# exported symbols
__all__ = ['RealKernel']


class RealKernel(Kernel):
    """Kernel whose inputs are real-valued vectors."""

    def __add__(self, other):
        return SumKernel(*combine(SumKernel, self, other))

    def __mul__(self, other):
        return ProductKernel(*combine(ProductKernel, self, other))

    def transform(self, X):
        return np.array(X, ndmin=2, dtype=float, copy=False)

    @abstractmethod
    def gradx(self, X1, X2=None):
        """
        Derivatives of the kernel with respect to its first argument. Returns
        an (m,n,d)-array.
        """

    @abstractmethod
    def grady(self, X1, X2=None):
        """
        Derivatives of the kernel with respect to its second argument. Returns
        an (m,n,d)-array.
        """

    @abstractmethod
    def gradxy(self, X1, X2=None):
        """
        Derivatives of the kernel with respect to both its first and second
        arguments. Returns an (m,n,d,d)-array. The (a,b,i,j)th element
        corresponds to the derivative with respect to `X1[a,i]` and `X2[b,j]`.
        """

    @abstractmethod
    def sample_spectrum(self, N, rng=None):
        """
        Sample N values from the spectral density of the kernel, returning a
        set of weights W of size (n,d) and a scalar value representing the
        normalizing constant.
        """


def _can_combine(*parts):
    """
    Return whether a set of real-valued kernels can be combined. Here this
    requires them to all be RealKernel objects and have the same number of
    input dimensions.
    """
    return (all(isinstance(_, RealKernel) for _ in parts) and
            all(_.ndim == parts[0].ndim for _ in parts))


class SumKernel(RealKernel, SumKernel_):
    """A sum of real-valued kernels."""

    def __init__(self, *parts):
        if not _can_combine(*parts):
            raise ValueError('cannot add mismatched kernels')

        super(SumKernel, self).__init__(*parts)
        self.ndim = self._parts[0].ndim

    def gradx(self, X1, X2=None):
        return sum(p.gradx(X1, X2) for p in self._parts)

    def grady(self, X1, X2=None):
        return sum(p.grady(X1, X2) for p in self._parts)

    def gradxy(self, X1, X2=None):
        return sum(p.gradxy(X1, X2) for p in self._parts)

    def sample_spectrum(self, N, rng=None):
        raise NotImplementedError


class ProductKernel(RealKernel, ProductKernel_):
    """A product of real-valued kernels."""

    def __init__(self, *parts):
        if not _can_combine(*parts):
            raise ValueError('cannot multiply mismatched kernels')

        super(ProductKernel, self).__init__(*parts)
        self.ndim = self._parts[0].ndim

    def gradx(self, X1, X2=None):
        fiterable = (p.get(X1, X2)[:, :, None] for p in self._parts)
        giterable = (p.gradx(X1, X2) for p in self._parts)
        return sum(f*g for f, g in zip(product_but(fiterable), giterable))

    def grady(self, X1, X2=None):
        fiterable = (p.get(X1, X2)[:, :, None] for p in self._parts)
        giterable = (p.grady(X1, X2) for p in self._parts)
        return sum(f*g for f, g in zip(product_but(fiterable), giterable))

    def gradxy(self, X1, X2=None):
        # the kernel evaluations.
        K = [p.get(X1, X2) for p in self._parts]
        Kn = product_but(K)

        # the gradients we need.
        Gx = [p.gradx(X1, X2) for p in self._parts]
        Gy = [p.grady(X1, X2) for p in self._parts]
        Gxy = [p.gradxy(X1, X2) for p in self._parts]

        # the part of the gradient corresponding to the two partial derivatives
        # with respect to xy.
        grad = sum(Kni[:, :, None, None] * dKi for Kni, dKi in zip(Kn, Gxy))

        # this is the combination of partials for different kernels.
        # multiplying in this way lets us avoid an explicit double-loop, but we
        # overcount.
        xpart = sum(dKi * Ki[:, :, None] for dKi, Ki in zip(Gx, Kn))
        ypart = sum(dKi / Ki[:, :, None] for dKi, Ki in zip(Gy, K))
        grad += xpart[:, :, :, None] * ypart[:, :, None, :]

        # get rid of the overcount.
        grad -= sum((Kni / Ki)[:, :, None, None]
                    * dKx[:, :, :, None]
                    * dKy[:, :, None, :]
                    for Kni, Ki, dKx, dKy in zip(Kn, K, Gx, Gy))

        return grad

    def sample_spectrum(self, N, rng=None):
        raise NotImplementedError
"""
Implementation of the squared-exponential kernels.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ._real import RealKernel
from ._distances import rescale, diff, sqdist, sqdist_foreach

from ..utils.random import rstate
from ..utils.models import printable

# exported symbols
__all__ = ['Matern']


@printable
class Matern(RealKernel):
    def __init__(self, sf, ell, d=3, ndim=None):
        self._logsf = np.log(float(sf))
        self._logell = np.log(ell)
        self._d = d
        self._iso = False
        self.ndim = np.size(self._logell)
        self.nhyper = 1 + np.size(self._logell)

        if ndim is not None:
            if np.size(self._logell) == 1:
                self._logell = float(self._logell)
                self._iso = True
                self.ndim = ndim
            else:
                raise ValueError('ndim only usable with scalar lengthscales')

        if self._d not in {1, 3, 5}:
            raise ValueError('d must be one of 1, 3, or 5')

    def _f(self, r):
        return (
            1 if (self._d == 1) else
            1+r if (self._d == 3) else
            1+r*(1+r/3.))

    def _df(self, r):
        return (
            1 if (self._d == 1) else
            r if (self._d == 3) else
            r*(1+r)/3.)

    def _params(self):
        return [
            ('sf', 1),
            ('ell', self.nhyper-1),
        ]

    def get_hyper(self):
        return np.r_[self._logsf, self._logell]

    def set_hyper(self, hyper):
        self._logsf = hyper[0]
        self._logell = hyper[1] if self._iso else hyper[1:]

    def get(self, X1, X2=None):
        X1, X2 = rescale(np.exp(self._logell)/np.sqrt(self._d), X1, X2)
        D = np.sqrt(sqdist(X1, X2))
        S = np.exp(self._logsf*2 - D)
        K = S * self._f(D)
        return K

    def grad(self, X1, X2=None):
        X1, X2 = rescale(np.exp(self._logell)/np.sqrt(self._d), X1, X2)
        D = np.sqrt(sqdist(X1, X2))
        S = np.exp(self._logsf*2 - D)
        K = S * self._f(D)
        M = S * self._df(D)

        yield 2*K           # derivative wrt logsf
        if self._iso:
            yield M*D       # derivative wrt logell (iso)
        else:
            for D_ in sqdist_foreach(X1, X2):
                            # derivative(s) wrt logell (ard)
                with np.errstate(invalid='ignore'):
                    yield np.where(D < 1e-12, 0, M*D_/D)

    def dget(self, X1):
        return np.exp(self._logsf*2) * np.ones(len(X1))

    def dgrad(self, X1):
        yield 2 * self.dget(X1)
        for _ in xrange(self.nhyper-1):
            yield np.zeros(len(X1))

    def gradx(self, X1, X2=None):
        ell = np.exp(self._logell) / np.sqrt(self._d)
        X1, X2 = rescale(ell, X1, X2)
        D1 = diff(X1, X2)

        D = np.sqrt(np.sum(D1**2, axis=-1))
        S = np.exp(self._logsf*2 - D)
        with np.errstate(invalid='ignore'):
            M = np.where(D < 1e-12, 0, S * self._df(D) / D)
        G = -M[:, :, None] * D1 / ell

        return G

    def grady(self, X1, X2=None):
        return -self.gradx(X1, X2)

    def gradxy(self, X1, X2=None):
        raise NotImplementedError

    def sample_spectrum(self, N, rng=None):
        rng = rstate(rng)
        sf2 = np.exp(self._logsf*2)
        ell = np.exp(self._logell)
        a = self._d / 2.
        g = np.tile(rng.gamma(a, 1/a, N), (self.ndim, 1)).T
        W = (rng.randn(N, self.ndim) / ell) / np.sqrt(g)
        return W, sf2
"""
Kernel which places a prior over periodic functions.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ._real import RealKernel
from ._distances import diff, sqdist
from ..utils.models import printable

# exported symbols
__all__ = ['Periodic']


@printable
class Periodic(RealKernel):
    """
    Covariance function for a 1-dimensional smooth periodic function with
    period p, lenthscale ell, and signal variance sf. The kernel function is
    given by::

        k(x, y) = sf^2 exp(-2 sin^2( ||x-y|| pi / p ) / ell^2)
    """
    def __init__(self, sf, ell, p):
        self._logsf = np.log(float(sf))
        self._logell = np.log(float(ell))
        self._logp = np.log(float(p))
        self.ndim = 1
        self.nhyper = 3

    def _params(self):
        return [
            ('sf', 1),
            ('ell', 1),
            ('p', 1),
        ]

    def get_hyper(self):
        return np.r_[self._logsf, self._logell, self._logp]

    def set_hyper(self, hyper):
        self._logsf = hyper[0]
        self._logell = hyper[1]
        self._logp = hyper[2]

    def get(self, X1, X2=None):
        sf2 = np.exp(self._logsf*2)
        ell = np.exp(self._logell)
        p = np.exp(self._logp)
        D = np.sqrt(sqdist(X1, X2)) * np.pi / p
        K = sf2 * np.exp(-2*(np.sin(D) / ell)**2)
        return K

    def grad(self, X1, X2=None):
        sf2 = np.exp(self._logsf*2)
        ell = np.exp(self._logell)
        p = np.exp(self._logp)

        # get the distance and a few transformations
        D = np.sqrt(sqdist(X1, X2)) * np.pi / p
        R = np.sin(D) / ell
        S = R**2
        E = 2 * sf2 * np.exp(-2*S)

        yield E
        yield 2*E*S
        yield 2*E*R*D * np.cos(D) / ell

    def dget(self, X1):
        return np.exp(self._logsf*2) * np.ones(len(X1))

    def dgrad(self, X):
        yield 2 * self.dget(X)
        yield np.zeros(len(X))
        yield np.zeros(len(X))

    def gradx(self, X1, X2=None):
        sf2 = np.exp(self._logsf*2)
        ell = np.exp(self._logell)
        p = np.exp(self._logp)

        # get the distance and a few transformations
        D = diff(X1, X2) * np.pi / p
        K = sf2 * np.exp(-2*(np.sin(D) / ell)**2)
        G = -2 * np.pi / ell**2 / p * K * np.sin(2*D)

        return G

    def grady(self, X1, X2=None):
        return -self.gradx(X1, X2)

    def gradxy(self, X1, X2=None):
        raise NotImplementedError

    def sample_spectrum(self, N, rng=None):
        raise NotImplementedError
"""
Kernel which places a prior over periodic functions.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ._real import RealKernel
from ..utils.models import printable
from ._distances import rescale, diff, sqdist, sqdist_foreach

# exported symbols
__all__ = ['RQ']


@printable
class RQ(RealKernel):
    def __init__(self, sf, ell, alpha, ndim=None):
        self._logsf = np.log(float(sf))
        self._logell = np.log(ell)
        self._logalpha = np.log(float(alpha))

        self._iso = False
        self.ndim = np.size(self._logell)
        self.nhyper = 2 + np.size(self._logell)

        if ndim is not None:
            if np.size(self._logell) == 1:
                self._logell = float(self._logell)
                self._iso = True
                self.ndim = ndim
            else:
                raise ValueError('ndim only usable with scalar lengthscales')

    def _params(self):
        return [
            ('sf', 1),
            ('ell', self.nhyper-2),
            ('alpha', 1),
        ]

    def get_hyper(self):
        return np.r_[self._logsf, self._logell, self._logalpha]

    def set_hyper(self, hyper):
        self._logsf = hyper[0]
        self._logell = hyper[1] if self._iso else hyper[1:-1]
        self._logalpha = hyper[-1]

    def get(self, X1, X2=None):
        sf2 = np.exp(self._logsf*2)
        ell = np.exp(self._logell)
        alpha = np.exp(self._logalpha)

        X1, X2 = rescale(ell, X1, X2)
        K = sf2 * (1 + 0.5*sqdist(X1, X2)/alpha) ** (-alpha)
        return K

    def grad(self, X1, X2=None):
        # hypers
        sf2 = np.exp(self._logsf*2)
        ell = np.exp(self._logell)
        alpha = np.exp(self._logalpha)

        # precomputations
        X1, X2 = rescale(ell, X1, X2)
        D = sqdist(X1, X2)
        E = 1 + 0.5*D/alpha
        K = sf2 * E**(-alpha)
        M = K*D/E

        yield 2*K                               # derivative wrt logsf
        if self._iso:
            yield M                             # derivative wrt logell (iso)
        else:
            for D in sqdist_foreach(X1, X2):
                yield K*D/E                     # derivative wrt logell (ard)
        yield 0.5*M - alpha*K*np.log(E)         # derivative wrt alpha

    def dget(self, X1):
        return np.exp(self._logsf*2) * np.ones(len(X1))

    def dgrad(self, X):
        yield 2 * self.dget(X)
        for _ in xrange(self.nhyper-2):
            yield np.zeros(len(X))
        yield np.zeros(len(X))

    def gradx(self, X1, X2=None):
        # hypers
        sf2 = np.exp(self._logsf*2)
        ell = np.exp(self._logell)
        alpha = np.exp(self._logalpha)

        # precomputations
        X1, X2 = rescale(ell, X1, X2)
        D = diff(X1, X2)
        E = 1 + np.sum(D**2, axis=-1) / 2 / alpha
        K = sf2 * E**(-alpha)
        G = -(K/E)[:, :, None] * D / ell

        return G

    def grady(self, X1, X2=None):
        return -self.gradx(X1, X2)

    def gradxy(self, X1, X2=None):
        raise NotImplementedError

    def sample_spectrum(self, N, rng=None):
        raise NotImplementedError
"""
Implementation of the squared-exponential kernels.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ._real import RealKernel
from ._distances import rescale, diff, sqdist, sqdist_foreach

from ..utils.random import rstate
from ..utils.models import printable

# exported symbols
__all__ = ['SE']


@printable
class SE(RealKernel):
    def __init__(self, sf, ell, ndim=None):
        self._logsf = np.log(float(sf))
        self._logell = np.log(ell)
        self._iso = False
        self.ndim = np.size(self._logell)
        self.nhyper = 1 + np.size(self._logell)

        if ndim is not None:
            if np.size(self._logell) == 1:
                self._logell = float(self._logell)
                self._iso = True
                self.ndim = ndim
            else:
                raise ValueError('ndim only usable with scalar lengthscales')

    def _params(self):
        return [
            ('sf', 1),
            ('ell', self.nhyper-1),
        ]

    def get_hyper(self):
        return np.r_[self._logsf, self._logell]

    def set_hyper(self, hyper):
        self._logsf = hyper[0]
        self._logell = hyper[1] if self._iso else hyper[1:]

    def get(self, X1, X2=None):
        X1, X2 = rescale(np.exp(self._logell), X1, X2)
        return np.exp(self._logsf*2 - sqdist(X1, X2)/2)

    def grad(self, X1, X2=None):
        X1, X2 = rescale(np.exp(self._logell), X1, X2)
        D = sqdist(X1, X2)
        K = np.exp(self._logsf*2 - D/2)
        yield 2*K                               # derivative wrt logsf
        if self._iso:
            yield K*D                           # derivative wrt logell (iso)
        else:
            for D in sqdist_foreach(X1, X2):
                yield K*D                       # derivatives wrt logell (ard)

    def dget(self, X1):
        return np.exp(self._logsf*2) * np.ones(len(X1))

    def dgrad(self, X):
        yield 2 * self.dget(X)
        for _ in xrange(self.nhyper-1):
            yield np.zeros(len(X))

    def gradx(self, X1, X2=None):
        ell = np.exp(self._logell)
        X1, X2 = rescale(ell, X1, X2)

        D = diff(X1, X2)
        K = np.exp(self._logsf*2 - np.sum(D**2, axis=-1)/2)
        G = -K[:, :, None] * D / ell
        return G

    def grady(self, X1, X2=None):
        return -self.gradx(X1, X2)

    def gradxy(self, X1, X2=None):
        ell = np.exp(self._logell)
        X1, X2 = rescale(ell, X1, X2)
        D = diff(X1, X2)
        _, _, d = D.shape

        K = np.exp(self._logsf*2 - np.sum(D**2, axis=-1)/2)
        D /= ell
        M = np.eye(d)/ell**2 - D[:, :, None] * D[:, :, :, None]
        G = M * K[:, :, None, None]

        return G

    def sample_spectrum(self, N, rng=None):
        rng = rstate(rng)
        sf2 = np.exp(self._logsf*2)
        ell = np.exp(self._logell)
        W = rng.randn(N, self.ndim) / ell
        return W, sf2
"""
Methods for learning the hyperparameters.
"""

# pylint: disable=wildcard-import
from .optimization import *
from .sampling import *

from . import optimization
from . import sampling

__all__ = []
__all__ += optimization.__all__
__all__ += sampling.__all__
"""
Perform type-II maximum likelihood to fit the hyperparameters of a GP model.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import scipy.optimize as so

# local imports
from ..utils.models import get_params

# exported symbols
__all__ = ['optimize']


def optimize(gp, priors=None):
    """
    Perform type-II maximum likelihood to fit GP hyperparameters.

    If given the priors object should be a dictionary mapping named parameters
    to an object which implements `prior.loglikelihood(hyper, grad)`. If a
    parameter is mapped to the `None` value then this will be assumed fixed.

    Note: nothing is returned by this function. Instead it will modify the
    hyperparameters of the given GP object in place.
    """
    hyper0 = gp.get_hyper()
    active = np.ones(gp.nhyper, dtype=bool)

    # this just manipulates a few lists so that we transform priors into a list
    # of tuples of the form (block, log, prior) for each named prior.
    params = dict((key, (block, log)) for (key, block, log) in get_params(gp))
    priors = dict() if (priors is None) else priors
    priors = [params[key] + (prior,) for (key, prior) in priors.items()]
    del params

    # remove from the active any block where the prior is None.
    for block, _, prior in priors:
        if prior is None:
            active[block] = False

    # get rid of these simple constraint priors.
    priors = [(b, l, p) for (b, l, p) in priors if p is not None]

    # FIXME: right now priors won't work because I am not dealing with the any
    # of the log transformed components.
    assert len(priors) == 0

    def objective(x):
        hyper = hyper0.copy()
        hyper[active] = x
        gp.set_hyper(hyper)
        lZ, dlZ = gp.loglikelihood(True)
        return -lZ, -dlZ[active]

    # optimize the model
    x, _, info = so.fmin_l_bfgs_b(objective, hyper0[active])

    # make sure that the gp is using the correct hypers
    hyper = hyper0.copy()
    hyper[active] = x
    gp.set_hyper(hyper)
"""
Perform hyperparameter sampling.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ..utils.models import get_params

# exported symbols
__all__ = ['sample']


#==============================================================================
# basic sampler(s) that don't know anything about GP objects.

def _slice_sample(logprob, x0, sigma=1.0, step_out=True, max_steps_out=1000):
    """
    Implementation of slice sampling taken almost directly from Snoek's
    spearmint package (with a few minor modifications).
    """
    def direction_slice(direction, x0):
        def dir_logprob(z):
            return logprob(direction*z + x0)

        upper = sigma*np.random.rand()
        lower = upper - sigma
        llh_s = np.log(np.random.rand()) + dir_logprob(0.0)

        l_steps_out = 0
        u_steps_out = 0
        if step_out:
            while dir_logprob(lower) > llh_s and l_steps_out < max_steps_out:
                l_steps_out += 1
                lower -= sigma
            while dir_logprob(upper) > llh_s and u_steps_out < max_steps_out:
                u_steps_out += 1
                upper += sigma

        while True:
            new_z = (upper - lower)*np.random.rand() + lower
            new_llh = dir_logprob(new_z)
            if np.isnan(new_llh):
                raise Exception("Slice sampler got a NaN")
            if new_llh > llh_s:
                break
            elif new_z < 0:
                lower = new_z
            elif new_z > 0:
                upper = new_z
            else:
                raise Exception("Slice sampler shrank to zero!")

        return new_z*direction + x0

    # FIXME: I've removed how blocks work because I want to rewrite that bit.
    # so right now this samples everything as one big block.
    direction = np.random.randn(x0.shape[0])
    direction = direction / np.sqrt(np.sum(direction**2))
    return direction_slice(direction, x0)


#==============================================================================
# interface for sampling hyperparameters from a GP.

def sample(gp, priors, n, raw=True):
    priors = dict(priors)
    active = np.ones(gp.nhyper, dtype=bool)
    logged = np.ones(gp.nhyper, dtype=bool)

    for (key, block, log) in get_params(gp):
        inactive = (key in priors) and (priors[key] is None)
        logged[block] = log
        active[block] = not inactive
        if inactive:
            del priors[key]
        else:
            priors[key] = (block, log, priors[key])

    # priors is now just a list of the form (block, log, prior).
    priors = priors.values()

    # get the initial hyperparameters and transform into the non-log space.
    hyper0 = gp.get_hyper()
    hyper0[logged] = np.exp(hyper0[logged])

    def logprob(x):
        # copy the initial hyperparameters and then assign the "active"
        # parameters that come from x.
        hyper = hyper0.copy()
        hyper[active] = x
        logprob = 0

        # compute the prior probabilities. we do this first so that if there
        # are any infs they'll be caught in the least expensive computations
        # first.
        for block, log, prior in priors:
            logprob += prior.logprior(hyper[block])
            if np.isinf(logprob):
                break

        # now compute the likelihood term. note that we'll have to take the log
        # of any logspace parameters before calling set_hyper.
        if not np.isinf(logprob):
            hyper[logged] = np.log(hyper[logged])
            gp.set_hyper(hyper)
            logprob += gp.loglikelihood()

        return logprob

    # create a big list of the hyperparameters so that we can just assign to
    # the components that are active. also get an initial sample x
    # corresponding only to the active parts of hyper0.
    hypers = np.tile(hyper0, (n, 1))
    x = hyper0.copy()[active]

    # do the sampling.
    for i in xrange(n):
        x = _slice_sample(logprob, x)
        hypers[i][active] = x

    # change the logspace components back into logspace.
    hypers[:, logged] = np.log(hypers[:, logged])

    # make sure the gp gets updated to the last sampled hyperparameter.
    gp.set_hyper(hypers[-1])

    if raw:
        return hypers
    else:
        return [gp.copy(h) for h in hypers]
"""
Objects implementing likelihoods.
"""

# pylint: disable=wildcard-import
from .gaussian import *

from . import gaussian

__all__ = []
__all__ += gaussian.__all__
"""
Implementation of the squared-exponential kernels.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ..utils.abc import abstractmethod
from ..utils.models import Parameterized

# exported symbols
__all__ = ['Likelihood', 'RealLikelihood']


class Likelihood(Parameterized):
    """
    Likelihood interface.
    """
    @abstractmethod
    def transform(self, y):
        pass

    @abstractmethod
    def sample(self, f, rng=None):
        pass


class RealLikelihood(Likelihood):
    def transform(self, y):
        return np.array(y, ndmin=1, dtype=float, copy=False)
"""
Implementation of the Gaussian likelihood model.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ._base import RealLikelihood
from ..utils.models import printable
from ..utils.random import rstate

# exported symbols
__all__ = ['Gaussian']


@printable
class Gaussian(RealLikelihood):
    """
    Likelihood model for standard Gaussian distributed errors.
    """
    def __init__(self, sigma):
        self._logsigma = np.log(float(sigma))
        self.nhyper = 1

    def _params(self):
        return [
            ('sigma', 1),
        ]

    @property
    def s2(self):
        """Simple access to the noise variance."""
        return np.exp(self._logsigma*2)

    def get_hyper(self):
        return np.r_[self._logsigma]

    def set_hyper(self, hyper):
        self._logsigma = hyper[0]

    def sample(self, f, rng=None):
        rng = rstate(rng)
        return f + rng.normal(size=len(f), scale=np.exp(self._logsigma))
"""
Meta-models which act like a GP object but also marginalize over the
hyperparameters.
"""

# pylint: disable=wildcard-import
from .mcmc import *
from .smc import *

from . import mcmc
from . import smc

__all__ = []
__all__ += mcmc.__all__
__all__ += smc.__all__
"""
Meta models which take care of hyperparameter marginalization whenever data is
added.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# local imports
from ..learning.sampling import sample

# exported symbols
__all__ = ['MCMC']


class MCMC(object):
    def __init__(self, model, prior, n=100, burn=100):
        self._model = model.copy()
        self._prior = prior
        self._samples = []
        self._n = n
        self._burn = burn

        if self._model.ndata > 0:
            if self._burn > 0:
                sample(self._model, self._prior, self._burn)
            self._samples = sample(self._model, self._prior, self._n, False)

        else:
            # FIXME: the likelihood won't play a role, so we can sample
            # directly from the prior. This of course requires the prior to
            # also be a well-defined distribution.
            pass

    def __iter__(self):
        return self._samples.__iter__()

    @property
    def ndata(self):
        return self._model.ndata

    @property
    def data(self):
        return self._model.data

    def add_data(self, X, y):
        # add the data
        nprev = self._model.ndata
        self._model.add_data(X, y)

        # if we've increased the amount of data by more than a factor two we'll
        # burn off some samples. Not sure if this is entirely necessary, but it
        # also accounts for burnin right after initial data is added.
        if self._model.ndata > 2*nprev and self._burn > 0:
            sample(self._model, self._prior, self._burn)

        # grab the samples.
        self._samples = sample(self._model, self._prior, self._n, False)

    def posterior(self, X, grad=False):
        parts = map(np.array,
                    zip(*[_.posterior(X, grad) for _ in self._samples]))

        mu_, s2_ = parts[:2]
        mu = np.mean(mu_, axis=0)
        s2 = np.mean(s2_ + (mu_ - mu)**2, axis=0)

        if not grad:
            return mu, s2

        dmu_, ds2_ = parts[2:]
        dmu = np.mean(dmu_, axis=0)
        Dmu = dmu_ - dmu
        ds2 = np.mean(ds2_
                      + 2 * mu_[:, :, None] * Dmu
                      - 2 * mu[None, :, None] * Dmu, axis=0)

        return mu, s2, dmu, ds2
"""
Meta models which take care of hyperparameter marginalization whenever data is
added.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
from scipy.misc import logsumexp

# local imports
from ..learning.sampling import sample
from ..utils.models import get_params

# exported symbols
__all__ = ['SMC']


def _sample_prior(model, priors, n):
    # unpack priors
    # TODO -- Bobak: This snippet is copied from learning/sampling.py
    # and should probably be put into a Prior base class.
    priors = dict(priors)
    active = np.ones(model.nhyper, dtype=bool)
    logged = np.ones(model.nhyper, dtype=bool)

    for (key, block, log) in get_params(model):
        inactive = (key in priors) and (priors[key] is None)
        logged[block] = log
        active[block] = not inactive
        if inactive:
            del priors[key]
        else:
            priors[key] = (block, log, priors[key])
    priors = priors.values()

    # sample hyperparameters from prior
    hypers = np.zeros((n, model.nhyper))
    for (block, log, prior) in priors:
        hypers[:, block] = prior.sample(n, log=log)

    return hypers


class SMC(object):
    def __init__(self, model, prior, n=100):
        self._prior = prior
        self._n = n

        # we won't add any data unless the model already has it.
        data = None

        if model.ndata > 0:
            data = model.data
            model = model.copy()
            model.reset()

        self._samples = [model.copy(h) for h in _sample_prior(model, prior, n)]
        self._logweights = np.zeros(n) - np.log(n)
        self._loglikes = np.zeros(n)

        if data is not None:
            self.add_data(data[0], data[1])

    def __iter__(self):
        return self._samples.__iter__()

    @property
    def ndata(self):
        return self._samples[-1].ndata

    @property
    def data(self):
        return self._samples[-1].data

    def add_data(self, X, y):
        X = self._samples[0]._kernel.transform(X)
        y = self._samples[0]._likelihood.transform(y)

        for (xi, yi) in zip(X, y):
            # resample if effective sample size is less than N/2
            if -logsumexp(2*self._logweights) < np.log(self._n/2):
                # FIXME: can use a better resampling strategy here. ie,
                # stratified, etc.
                p = np.exp(self._logweights)
                idx = np.random.choice(self._n, self._n, p=p)
                self._samples = [self._samples[i].copy() for i in idx]
                self._logweights = np.zeros(self._n) - np.log(self._n)
                self._loglikes = self._loglikes[idx]

            # add data
            for model in self._samples:
                model.add_data(xi, yi)

            # we will propose new hyperparameters using an MCMC kernel, which
            # corresponds to Eqs. 30--31 of (Del Moral et al, 2006). To compute
            # the incremental weights we just need the loglikelihoods before
            # and after adding the new data but before propagating the
            # particles.

            # self._loglikes already contains the loglikelihood before adding
            # the data, so what follows computes it after adding the data.
            loglikes = np.fromiter((model.loglikelihood()
                                    for model in self._samples), float)

            # update and normalize the weights.
            self._logweights += loglikes - self._loglikes
            self._logweights -= logsumexp(self._logweights)

            # propagate the particles.
            for model in self._samples:
                sample(model, self._prior, 1)

            # update the loglikelihoods given the new samples.
            self._loglikes = np.fromiter((model.loglikelihood()
                                          for model in self._samples), float)

    def posterior(self, X, grad=False):
        parts = [_.posterior(X, grad) for _ in self._samples]
        parts = [np.array(_) for _ in zip(*parts)]

        weights = np.exp(self._logweights)

        mu_, s2_ = parts[:2]
        mu = np.average(mu_, weights=weights, axis=0)
        s2 = np.average(s2_ + (mu_ - mu)**2, weights=weights, axis=0)

        if not grad:
            return mu, s2

        dmu_, ds2_ = parts[2:]
        dmu = np.average(dmu_, weights=weights, axis=0)

        Dmu = dmu_ - dmu
        ds2 = np.average(ds2_
                         + 2 * mu_[:, :, None] * Dmu
                         - 2 * mu[None, :, None] * Dmu,
                         weights=weights, axis=0)

        return mu, s2, dmu, ds2
"""
Plotting methods for GP objects.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import matplotlib.pyplot as pl

# local imports
from .utils.models import get_params

# exported symbols
__all__ = ['plot_posterior', 'plot_samples']


def plot_posterior(model,
                   xmin=None, xmax=None,
                   mean=True, data=True, error=True, pseudoinputs=True,
                   lw=2, ls='-', color=None, marker='o', marker2='x'):
    """
    Plot a one-dimensional posterior model.

    Parameters:
        xmin: minimum x value
        xmax: maximum x value
        mean: plot the mean
        data: plot the data
        error: plot the error bands
        pseudoinputs: plot pseudoinputs (if there are any)
    """

    # grab the data.
    X, y = model.data
    if X is None and (xmin is None or xmax is None):
        raise Exception('bounds must be given if no data is present')

    # get the input points.
    xmin = X[:, 0].min() if (xmin is None) else xmin
    xmax = X[:, 0].max() if (xmax is None) else xmax
    x = np.linspace(xmin, xmax, 500)

    # get the mean and confidence bands.
    mu, s2 = model.posterior(x[:, None])
    lo = mu - 2 * np.sqrt(s2)
    hi = mu + 2 * np.sqrt(s2)

    # get the axes.
    ax = pl.gca()

    if color is None:
        color = next(ax._get_lines.color_cycle)

    # default arguments for markers.
    margs = dict(color='k', zorder=3)
    margs = {
        'x': dict(marker='x', facecolors='none', s=30, lw=1, **margs),
        'o': dict(marker='o', facecolors='none', s=30, lw=1, **margs),
        '*': dict(marker='*', facecolors='none', s=30, lw=1, **margs),
        ',': dict(marker=',', facecolors='none', s=30, lw=1, **margs),
        '.': dict(marker='.', **margs)}

    if mean:
        # plot the mean
        ax.plot(x, mu, lw=lw, ls=ls, label='mean', color=color)

    if error:
        # plot the error bars and add an empty plot that will be used by the
        # legend if it's called for.
        alpha = 0.25
        ax.fill_between(x, lo, hi, color=color, alpha=alpha)
        ax.plot([], [], color=color, alpha=alpha, linewidth=10,
                label='uncertainty')

    if data and X is not None:
        # plot the data; use smaller markers if we have a lot of data.
        ax.scatter(X.ravel(), y, label='data', **margs[marker])

    if hasattr(model, 'pseudoinputs') and pseudoinputs:
        # plot any pseudo-inputs.
        ymin, ymax = ax.get_ylim()
        u = model.pseudoinputs.ravel()
        v = np.full_like(u, ymin + 0.1 * (ymax-ymin))
        ax.scatter(u, v, label='pseudo-inputs', **margs[marker2])

    pl.axis('tight')


def plot_samples(model):
    """
    Plot the posterior over hyperparameters for a sample-based meta model.
    """
    # get the figure and clear it.
    fg = pl.gcf()
    fg.clf()

    samples = np.array(list(m.get_hyper() for m in model))
    values = np.zeros((samples.shape[0], 0))
    labels = []

    for key, block, log in get_params(next(model.__iter__())):
        for i in range(block.start, block.stop):
            vals = samples[:, i]
            size = block.stop - block.start
            name = key + ('' if (size == 1) else '_%d' % (i - block.start))
            if not np.allclose(vals, vals[0]):
                values = np.c_[values, np.exp(vals) if log else vals]
                labels.append(name)

    naxes = values.shape[1]

    if naxes == 1:
        ax = fg.add_subplot(111)
        ax.hist(values[:, 0], bins=20)
        ax.set_xlabel(labels[0])
        ax.set_yticklabels([])

    else:
        for i, j in np.ndindex(naxes, naxes):
            if i >= j:
                continue
            ax = fg.add_subplot(naxes-1, naxes-1, (j-1)*(naxes-1)+i+1)
            ax.scatter(values[:, i], values[:, j], alpha=0.1)

            if i == 0:
                ax.set_ylabel(labels[j])
            else:
                ax.set_yticklabels([])

            if j == naxes-1:
                ax.set_xlabel(labels[i])
            else:
                ax.set_xticklabels([])
"""
Objects implementing priors (and sampling from them).
"""

# pylint: disable=wildcard-import
from .priors import *

# import the named modules themselves.
from . import priors

# export everything.
__all__ = []
__all__ += priors.__all__
"""
Implementations of various prior objects.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# exported symbols
__all__ = ['Uniform']


class Uniform(object):
    def __init__(self, a, b):
        self._a = np.array(a, copy=True, ndmin=1)
        self._b = np.array(b, copy=True, ndmin=1)
        self.ndim = len(self._a)

        if len(self._a) != len(self._b):
            raise RuntimeError("bound sizes don't match")

        if np.any(self._b < self._a):
            raise RuntimeError("malformed upper/lower bounds")

    def sample(self, size=1, log=True):
        sample = self._a + (self._b - self._a) * np.random.rand(size, self.ndim)
        return np.log(sample) if log else sample

    def logprior(self, theta):
        theta = np.array(theta, copy=False, ndmin=1)
        for a, b, t in zip(self._a, self._b, theta):
            if (t < a) or (t > b):
                return -np.inf
        return 0.0
"""
Modifications to ABC to allow for additional metaclass actions.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
from abc import ABCMeta as ABCMeta_
from abc import abstractmethod

# exported symbols
__all__ = ['ABCMeta', 'abstractmethod']


class ABCMeta(ABCMeta_):
    """
    Slight modification to ABCMeta that copies docstrings from an
    abstractmethod to its implementation if the implementation lacks a
    docstring.
    """
    def __new__(mcs, name, bases, attrs):
        abstracts = dict(
            (attr, getattr(base, attr))
            for base in bases
            for attr in getattr(base, '__abstractmethods__', set()))

        for attr, value in attrs.items():
            implements = (attr in abstracts and
                          not getattr(value, '__isabstractmethod__', False))
            if implements and not getattr(value, '__doc__', False):
                docstring = getattr(abstracts[attr], '__doc__', None)
                setattr(value, '__doc__', docstring)

        return super(ABCMeta, mcs).__new__(mcs, name, bases, attrs)
"""
Exception classes.
"""

# exported symbols
__all__ = ['ModelError']


class Error(Exception):
    pass


class ModelError(Error):
    pass
"""
Interfaces for parameterized objects.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import copy

# local imports
from .abc import ABCMeta, abstractmethod

# exported symbols
__all__ = ['Parameterized', 'printable', 'dot_params', 'get_params']


class Parameterized(object):
    """
    Interface for objects that are parameterized by some set of
    hyperparameters.
    """
    __metaclass__ = ABCMeta

    @abstractmethod
    def _params(self):
        """
        Define the set of parameters for the model. This should return a list
        of tuples of the form `(name, size, islog)`. If only a 2-tuple is given
        then islog will be assumed to be `True`.
        """
        pass

    @abstractmethod
    def get_hyper(self):
        """Return a vector of model hyperparameters."""
        pass

    @abstractmethod
    def set_hyper(self, hyper):
        """Set the model hyperparameters to the given vector."""
        pass

    def copy(self, hyper=None):
        """
        Copy the model. If `hyper` is given use this vector to immediately set
        the copied model's hyperparameters.
        """
        model = copy.deepcopy(self)
        if hyper is not None:
            model.set_hyper(hyper)
        return model


def printable(cls):
    """
    Decorator which marks classes as being able to be pretty-printed as a
    function of their hyperparameters. This decorator defines a __repr__ method
    for the given class which uses the class's `get_hyper` and `_params`
    methods to print it.
    """
    def _repr(obj):
        """Represent the object as a function of its hyperparameters."""
        hyper = obj.get_hyper()
        substrings = []
        for key, block, log in get_params(obj):
            val = hyper[block]
            val = val[0] if (len(val) == 1) else val
            val = np.exp(val) if log else val
            substrings += ['%s=%s' % (key, val)]
        return obj.__class__.__name__ + '(' + ', '.join(substrings) + ')'
    cls.__repr__ = _repr
    return cls


# FIXME: it's unclear how useful dot_params is. This might be replaced.

def dot_params(ns, params):
    """
    Extend a param tuple with a 'namespace'. IE prepend the key string with ns
    plus a dot.
    """
    return [("%s.%s" % (ns, p[0]),) + p[1:] for p in params]


# FIXME: the get_params function is kind of a hack in order to allow for
# simpler definitions of the _params() method. This should probably be
# replaced.

def get_params(obj):
    """
    Helper function which translates the values returned by _params() into
    something more meaningful.
    """
    offset = 0
    for param in obj._params():
        key = param[0]
        size = param[1]
        block = slice(offset, offset+size)
        log = (len(param) < 3) or param[2]
        offset += size
        yield key, block, log
"""
Simple utilities for random number generation.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np

# exported symbols
__all__ = ['rstate']


def rstate(rng=None):
    """
    Return a numpy RandomState object. If an integer value is given then a new
    RandomState will be returned with this seed. If None is given then the
    global numpy state will be returned. If an already instantiated state is
    given this will be passed back.
    """
    if rng is None:
        rng = np.random.mtrand._rand
    elif isinstance(rng, int):
        rng = np.random.RandomState(rng)
    elif not isinstance(rng, np.random.RandomState):
        raise ValueError('unknown seed given to rstate')
    return rng
NAME = 'pygp'
VERSION = '0.0.1'
AUTHOR = 'Matthew W. Hoffman'
AUTHOR_EMAIL = 'mwh30@cam.ac.uk'
URL = 'http://github.com/mwhoffman/pygp2'
DESCRIPTION = 'A python library for inference with Gaussian processes'


from setuptools import setup, find_packages


if __name__ == '__main__':
    setup(
        name=NAME,
        version=VERSION,
        author=AUTHOR,
        author_email=AUTHOR_EMAIL,
        description=DESCRIPTION,
        url=URL,
        packages=find_packages())
"""
Tests of inference methods.
"""

# pylint: disable=no-member
# pylint: disable=missing-docstring

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import numpy.testing as nt
import scipy.optimize as spop
import nose

# local imports
import pygp


### BASE TEST CLASS ###########################################################

class InferenceTest(object):
    def test_repr(self):
        _ = repr(self.gp)

    def test_params(self):
        _ = self.gp._params()

    def test_data(self):
        _ = self.gp.data

    def test_copy(self):
        _ = self.gp.copy()

    def test_hyper(self):
        hyper1 = self.gp.get_hyper()
        self.gp.set_hyper(self.gp.get_hyper())
        hyper2 = self.gp.get_hyper()
        nt.assert_allclose(hyper1, hyper2)

    def test_add_data(self):
        # add additional data.
        gp1 = self.gp.copy()
        gp1.add_data(self.X, self.y)

        # add additional data but make sure we don't do so incrementally.
        updateinc = pygp.inference._base.GP._updateinc
        gp2 = self.gp.copy()
        gp2._updateinc = lambda X, y: updateinc(gp2, X, y)
        gp2.add_data(self.X, self.y)

        # make sure the posteriors match.
        p1 = gp1.posterior(self.X)
        p2 = gp2.posterior(self.X)
        nt.assert_allclose(p1, p2)

    def test_sample(self):
        _ = self.gp.sample(self.X, m=2, latent=False)

    def test_sample_fourier(self):
        _ = self.gp.sample_fourier(10)

    def test_loglikelihood(self):
        x = self.gp.get_hyper()
        f = lambda x: self.gp.copy(x).loglikelihood()
        _, g1 = self.gp.loglikelihood(grad=True)
        g2 = spop.approx_fprime(x, f, 1e-8)

        # slightly lesser gradient tolerance. mostly due to FITC.
        nt.assert_allclose(g1, g2, rtol=1e-5, atol=1e-5)


### TEST CLASS FOR REAL-VALUED INPUTS #########################################

class RealTest(InferenceTest):
    def __init__(self, gp):
        # create some data.
        rng = np.random.RandomState(1)
        X = rng.rand(10, gp._kernel.ndim)
        y = gp._likelihood.sample(rng.rand(10), rng)

        # create a gp.
        self.gp = gp
        self.gp.add_data(X, y)

        # new set of points to predict at.
        self.X = rng.rand(10, gp._kernel.ndim)
        self.y = gp._likelihood.sample(rng.rand(10), rng)

    def test_posterior_mu(self):
        f = lambda x: self.gp.posterior(x[None])[0]
        G1 = self.gp.posterior(self.X, grad=True)[2]
        G2 = np.array([spop.approx_fprime(x, f, 1e-8) for x in self.X])
        nt.assert_allclose(G1, G2, rtol=1e-6, atol=1e-6)

    def test_posterior_s2(self):
        f = lambda x: self.gp.posterior(x[None])[1]
        G1 = self.gp.posterior(self.X, grad=True)[3]
        G2 = np.array([spop.approx_fprime(x, f, 1e-8) for x in self.X])
        nt.assert_allclose(G1, G2, rtol=1e-5, atol=1e-5)


### PER INFERENCE METHOD TESTS ################################################

class TestExact(RealTest):
    def __init__(self):
        likelihood = pygp.likelihoods.Gaussian(1)
        kernel = pygp.kernels.SE(1, 1, ndim=2)
        gp = pygp.inference.ExactGP(likelihood, kernel, 0.0)
        RealTest.__init__(self, gp)


class TestFITC(RealTest):
    def __init__(self):
        rng = np.random.RandomState(1)
        likelihood = pygp.likelihoods.Gaussian(1)
        kernel = pygp.kernels.SE(1, 1, ndim=2)
        mean = 0.0
        U = rng.rand(10, kernel.ndim)
        gp = pygp.inference.FITC(likelihood, kernel, mean, U)
        RealTest.__init__(self, gp)


class TestDTC(RealTest):
    def __init__(self):
        rng = np.random.RandomState(1)
        likelihood = pygp.likelihoods.Gaussian(1)
        kernel = pygp.kernels.SE(1, 1, ndim=2)
        mean = 0.0
        U = rng.rand(10, kernel.ndim)
        gp = pygp.inference.DTC(likelihood, kernel, mean, U)
        RealTest.__init__(self, gp)
"""
Kernel tests.
"""

# pylint: disable=no-member
# pylint: disable=missing-docstring

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import numpy.testing as nt
import scipy.optimize as spop
import nose
import operator as op

# pygp imports
import pygp.kernels as pk


### BASE TEST CLASS ###########################################################

# children tests should initialize a kernel and two sets of points x1 and x2 in
# their __init__ method.

class KernelTest(object):
    def test_repr(self):
        _ = repr(self.kernel)

    def test_params(self):
        params = self.kernel._params()
        assert all(2 <= len(p) <= 3 for p in params)
        assert sum(p[1] for p in params) == self.kernel.nhyper

    def test_copy(self):
        _ = self.kernel.copy()

    def test_hyper(self):
        hyper1 = self.kernel.get_hyper()
        self.kernel.set_hyper(self.kernel.get_hyper())
        hyper2 = self.kernel.get_hyper()
        nt.assert_allclose(hyper1, hyper2)

    def test_get(self):
        _ = self.kernel.get(self.x1, self.x2)

    def test_dget(self):
        _ = self.kernel.dget(self.x1)

    def test_transpose(self):
        K1 = self.kernel.get(self.x1, self.x2)
        K2 = self.kernel.get(self.x2, self.x1).T
        G1 = np.array(list(self.kernel.grad(self.x1, self.x2)))
        G2 = np.array(list(self.kernel.grad(self.x2, self.x1))).swapaxes(1, 2)
        nt.assert_allclose(K1, K2)
        nt.assert_allclose(G1, G2)

    def test_self(self):
        K1 = self.kernel.get(self.x1)
        K2 = self.kernel.get(self.x1, self.x1)
        G1 = np.array(list(self.kernel.grad(self.x1)))
        G2 = np.array(list(self.kernel.grad(self.x1, self.x1)))
        nt.assert_allclose(K1, K2)
        nt.assert_allclose(G1, G2)

    def test_grad(self):
        x = self.kernel.get_hyper()
        k = lambda x, x1, x2: self.kernel.copy(x)(x1, x2)

        G1 = np.array(list(self.kernel.grad(self.x1, self.x2)))
        G2 = np.array([spop.approx_fprime(x, k, 1e-8, x1, x2)
                       for x1 in self.x1
                       for x2 in self.x2])\
            .swapaxes(0, 1)\
            .reshape(-1, self.x1.shape[0], self.x2.shape[0])

        nt.assert_allclose(G1, G2, rtol=1e-6, atol=1e-6)

    def test_dgrad(self):
        g1 = list(self.kernel.dgrad(self.x1))
        g2 = [np.diag(_) for _ in self.kernel.grad(self.x1)]
        nt.assert_allclose(g1, g2)


### REAL KERNEL TEST CLASS ####################################################

class RealKernelTest(KernelTest):
    def __init__(self, kernel):
        self.kernel = kernel
        rng = np.random.RandomState(0)
        self.x1 = rng.rand(5, self.kernel.ndim)
        self.x2 = rng.rand(3, self.kernel.ndim)

    def test_gradx(self):
        try:
            G1 = self.kernel.gradx(self.x1, self.x2)
        except NotImplementedError:
            raise nose.SkipTest()

        m = self.x1.shape[0]
        n = self.x2.shape[0]
        d = self.x1.shape[1]
        k = self.kernel

        G2 = np.array([spop.approx_fprime(x1, k, 1e-8, x2)
                       for x1 in self.x1
                       for x2 in self.x2]).reshape(m, n, d)

        nt.assert_allclose(G1, G2, rtol=1e-6, atol=1e-6)

    def test_grady(self):
        try:
            G1 = self.kernel.grady(self.x1, self.x2)
        except NotImplementedError:
            raise nose.SkipTest()

        m = self.x1.shape[0]
        n = self.x2.shape[0]
        d = self.x1.shape[1]
        k = lambda x2, x1: self.kernel(x1, x2)

        G2 = np.array([spop.approx_fprime(x2, k, 1e-8, x1)
                       for x1 in self.x1
                       for x2 in self.x2]).reshape(m, n, d)

        nt.assert_allclose(G1, G2, rtol=1e-6, atol=1e-6)

    def test_gradxy(self):
        try:
            G1 = self.kernel.gradxy(self.x1, self.x2)
        except NotImplementedError:
            raise nose.SkipTest()

        m = self.x1.shape[0]
        n = self.x2.shape[0]
        d = self.x1.shape[1]
        g = lambda x2, x1, i: self.kernel.gradx(x1[None], x2[None])[0, 0, i]

        G2 = np.array([spop.approx_fprime(x2, g, 1e-8, x1, i)
                       for x1 in self.x1
                       for x2 in self.x2
                       for i in xrange(d)]).reshape(m, n, d, d)

        nt.assert_allclose(G1, G2, rtol=1e-6, atol=1e-6)

    def test_spectrum(self):
        try:
            W, alpha = self.kernel.sample_spectrum(100)
        except NotImplementedError:
            raise nose.SkipTest()

        assert np.isscalar(alpha)
        assert W.shape[0] == 100


### PER KERNEL TESTS ##########################################################

class TestSEARD(RealKernelTest):
    def __init__(self):
        RealKernelTest.__init__(self, pk.SE(0.8, [0.3, 0.4]))


class TestSEIso(RealKernelTest):
    def __init__(self):
        RealKernelTest.__init__(self, pk.SE(0.8, 0.3, ndim=2))


class TestPeriodic(RealKernelTest):
    def __init__(self):
        RealKernelTest.__init__(self, pk.Periodic(0.5, 0.4, 0.3))


class TestRQARD(RealKernelTest):
    def __init__(self):
        RealKernelTest.__init__(self, pk.RQ(0.5, [0.4, 0.5], 0.3))


class TestRQIso(RealKernelTest):
    def __init__(self):
        RealKernelTest.__init__(self, pk.RQ(0.5, 0.4, 0.3, ndim=2))


class TestMaternARD1(RealKernelTest):
    def __init__(self):
        RealKernelTest.__init__(self, pk.Matern(0.5, [0.4, 0.3], d=1))


class TestMaternARD3(RealKernelTest):
    def __init__(self):
        RealKernelTest.__init__(self, pk.Matern(0.5, [0.4, 0.3], d=3))


class TestMaternARD5(RealKernelTest):
    def __init__(self):
        RealKernelTest.__init__(self, pk.Matern(0.5, [0.4, 0.3], d=5))


class TestMaternIso1(RealKernelTest):
    def __init__(self):
        RealKernelTest.__init__(self, pk.Matern(0.5, 0.4, d=1, ndim=2))


class TestMaternIso3(RealKernelTest):
    def __init__(self):
        RealKernelTest.__init__(self, pk.Matern(0.5, 0.4, d=3, ndim=2))


class TestMaternIso5(RealKernelTest):
    def __init__(self):
        RealKernelTest.__init__(self, pk.Matern(0.5, 0.4, d=5, ndim=2))


class TestRealSum(RealKernelTest):
    def __init__(self):
        RealKernelTest.__init__(self,
                                pk.SE(0.8, 0.3, ndim=2) +
                                pk.SE(0.1, 0.2, ndim=2) +
                                pk.SE(0.1, 0.2, ndim=2))


class TestRealProduct(RealKernelTest):
    def __init__(self):
        RealKernelTest.__init__(self,
                                pk.SE(0.8, 0.3, ndim=2) *
                                pk.SE(0.1, 0.2, ndim=2) *
                                pk.SE(0.1, 0.2, ndim=2))


class TestRealSumProduct(RealKernelTest):
    def __init__(self):
        RealKernelTest.__init__(self,
                                pk.SE(0.8, 0.3, ndim=2) *
                                pk.SE(0.1, 0.2, ndim=2) +
                                pk.SE(0.8, 0.3, ndim=2) *
                                pk.SE(0.1, 0.2, ndim=2))


### INITIALIZATION TESTS ######################################################

# the following tests attempt to initialize a few kernels with invalid
# parameters, each of which should raise an exception.

def test_init_sum():
    k1 = pk.SE(1, 1, ndim=1)
    k2 = pk.SE(1, 1, ndim=2)
    nt.assert_raises(ValueError, op.add, k1, k2)


def test_init_product():
    k1 = pk.SE(1, 1, ndim=1)
    k2 = pk.SE(1, 1, ndim=2)
    nt.assert_raises(ValueError, op.mul, k1, k2)


def test_init_ard():
    def check_ard(Kernel, args):
        nt.assert_raises(ValueError, Kernel, *args, ndim=1)

    kernel_args = [
        (pk.SE, (1, [1, 1])),
        (pk.Matern, (1, [1, 1])),
        (pk.RQ, (1, [1, 1], 1))
    ]

    for Kernel, args in kernel_args:
        yield check_ard, Kernel, args


def test_init_matern():
    nt.assert_raises(ValueError, pk.Matern, 1, 1, d=12)
"""
Unit tests for different acquisition functions. This mainly tests that the
gradients of each acquisition function are computed correctly.
"""

# future imports
from __future__ import division
from __future__ import absolute_import
from __future__ import print_function

# global imports
import numpy as np
import numpy.testing as nt
import scipy.optimize as spop

# local imports
import pygp
import pygp.meta as meta
import pygp.priors as priors


class BaseMetaTest(object):
    def __init__(self):
        ndim = 2
        prior = {
            'sn':  priors.Uniform(0.01, 1.0),
            'sf':  priors.Uniform(0.01, 5.0),
            'ell': priors.Uniform([0.01]*ndim, [1.0]*ndim),
            'mu':  priors.Uniform([-2.0]*ndim, [2.0]*ndim)}

        # create the model.
        model = pygp.BasicGP(0.5, 1, [1]*ndim)
        model = self.MetaModel(model, prior, n=10, burn=0)

        # randomly generate some data and add it.
        rng = np.random.RandomState(0)
        X = rng.rand(10, ndim)
        y = rng.rand(10)
        model.add_data(X, y)

        self.model = model
        self.X = rng.rand(10, ndim)

    def test_grad_mu(self):
        _, _, dmu, _ = self.model.posterior(self.X, grad=True)
        fmu = lambda x: self.model.posterior(x[None], grad=True)[0][0]
        dmu_ = np.array([spop.approx_fprime(x, fmu, 1e-8) for x in self.X])
        nt.assert_allclose(dmu, dmu_, rtol=1e-6, atol=1e-6)

    def test_grad_s2(self):
        _, _, _, ds2 = self.model.posterior(self.X, grad=True)
        fs2 = lambda x: self.model.posterior(x[None], grad=True)[1][0]
        ds2_ = np.array([spop.approx_fprime(x, fs2, 1e-8) for x in self.X])
        nt.assert_allclose(ds2, ds2_, rtol=1e-6, atol=1e-6)


class TestMCMC(BaseMetaTest):
    MetaModel = meta.MCMC


def fib(x):
    if x == 0 or x == 1:
        return 1
    else:
        return fib(x-1) + fib(x-2)

#Memoized Fibonacci
def fastFib(x, memo = {}):
    if x == 0 or x == 1:
        return 1
    try:
        return memo[x]
    except KeyError:
        result = fastFib(x-1, memo) + fastFib(x-2, memo)
        memo[x] = result
        return result

def testFastFib(n):
    for i in range(n):
        print ('fib of', i, '=', fastFib(i))


def coins(row, memo = {}):
    """Takes in a list of coins and finds selection that maximizes sum.
        Selection obeys the constraint that adjacent coins are not selected.
        Coins are row[0], row[1], ... row[len(row)-1].
        Solution for a row is in memo[len(row)]."""

    if (len(row) == 0):
        return 0
    elif (len(row) == 1):
        return row[0]
    try:
        return memo[len(row)]
    except KeyError:
        result = max(coins(row[:-2], memo) + row[-1], coins(row[:-1], memo))
        memo[len(row)] = result
        return result
    

def iterative_coins(row):
    """Iterative version of row_coins, tracing coins selected.
        Coins are row[0], row[1], ... row[len(row)-1].
        Solution for a row is in memo[len(row)]"""

    table = [None] * (len(row) + 1)
    table[0] = 0
    table[1] = row[0]
    for i in range(2, len(row)+1):
        table[i] = max(table[i-2] + row[i-1], table[i-1])

    #Tracing back the coin selection
    select = []
    i = len(row)
    while i >= 1:
        if table[i] > table[i-1]:
            select.append(row[i-1])
            i -= 2
        else:
            i -= 1
            
    print 'Input row = ', row
    print 'Table = ', table
    print 'Selected coins are', select[::-1], 'and sum up to', table[len(row)]





import networkx as nx
import csv

com_file = open('communities.txt', 'r')

for line in com_file:
	(a, b) = line.split(',')
	if int(a) == 13:
		c = [int(x) for x in b.split(" ")]
		break

print "Started reading"
stanf = nx.read_edgelist('web-Stanford.txt', create_using = nx.DiGraph(), nodetype = int)
print "Finished reading"

subs = stanf.subgraph(c)
print 'Finding betweenness'
bet_cen = nx.betweenness_centrality(subs)
w = csv.writer(open("betweens_13.csv", "w"))
for key, val in bet_cen.items():
    w.writerow([key, val])

print 'Finding closeness'
clo_cen = nx.closeness_centrality(subs)
w = csv.writer(open("close_13.csv", "w"))
for key, val in clo_cen.items():
    w.writerow([key, val])

print 'Finding eigenvector'
eig_cen = nx.eigenvector_centrality(subs)
w = csv.writer(open("eig_13.csv", "w"))
for key, val in eig_cen.items():
    w.writerow([key, val])import community
import networkx as nx
import matplotlib.pyplot as plt

#better with karate_graph() as defined in networkx example.
#erdos renyi don't have true community structure
G = nx.erdos_renyi_graph(30, 0.05)
plt.figure()
nx.draw(G)
#first compute the best partition
partition = community.best_partition(G)
print partition
#drawing
print len(partition)
size = float(len(set(partition.values())))
print size
pos = nx.spring_layout(G)
count = 0.
plt.figure()
for com in set(partition.values()) :
    count = count + 1.
    list_nodes = [nodes for nodes in partition.keys()
                                if partition[nodes] == com]
    nx.draw_networkx_nodes(G, pos, list_nodes, node_size = 20,
                                node_color = str(count / size))


nx.draw_networkx_edges(G,pos, alpha=0.5)
plt.show()import csv
import numpy as np

def get_top_keys(dictionary, top):
	print 'Getting top', top
	items = dictionary.items()
	items.sort(reverse=True, key=lambda x: x[1])
	return map(lambda x: x[0], items[:top])

bets = {}
b = []
for key, val in csv.reader(open("betweens_13.csv")):
	b.append(val)
	bets[int(key)] = float(val)

top_bets = get_top_keys(bets, 20)
print 'Bet'
for k in top_bets:
	print k

clos = {}
c = []
for key, val in csv.reader(open("close_13.csv")):
	c.append(val)
	clos[int(key)] = float(val)

top_clos = get_top_keys(clos, 20)
print 'Clo'
for k in top_clos:
	print k

eigs = {}
e = []
for key, val in csv.reader(open("eig_13.csv")):
	e.append(val)
	eigs[int(key)] = float(val)

top_eigs = get_top_keys(eigs, 20)
print 'Eig'
for k in top_eigs:
	print k

print len(set(top_bets).intersection(top_clos)), 'Between and Close'
print len(set(top_eigs).intersection(top_clos)), 'Eig and Close'
print len(set(top_eigs).intersection(top_bets)), 'Eig and Bet'import numpy as np
import csv
import matplotlib.pyplot as plt

def get_top_keys(dictionary, top):
	print 'Getting top', top
	items = dictionary.items()
	items.sort(reverse=True, key=lambda x: x[1])
	return map(lambda x: x[0], items[:top])

in_degrees = {}
ins = []
for key, val in csv.reader(open("in-degrees.csv")):
	ins.append(val)
	in_degrees[int(key)] = int(val)

top_ins = get_top_keys(in_degrees, 20)
#for k in top_ins:
#	print k

page_ranks = {}
pgs = []
for key, val in csv.reader(open("page-ranks.csv")):
	pgs.append(val)
	page_ranks[int(key)] = float(val)
'''
print 'new'
for k in get_top_keys(page_ranks, 20):
	print k

print 'new'
'''

top_prs = get_top_keys(page_ranks, 20)

print len(set(top_ins).intersection(top_prs))


hubs = {}
hbs = []
for key, val in csv.reader(open("hubs.csv")):
	hbs.append(val)
	hubs[int(key)] = float(val)

top_hubs = get_top_keys(hubs, 20)
for k in top_hubs:
	print k
'''
print 'Histogram comp'
hist, bin_edges = np.histogram(hubs.values(), bins= 250000)

fig = plt.figure(1)
ax = plt.gca()
ax.grid(True)
ax.plot(bin_edges[:-1], hist, 'o')
ax.set_yscale('log')
ax.set_xscale('log')
ax.set_xlim([0, 1])
ax.set_xlabel('Hub score')
ax.set_ylabel('Number of Nodes')
ax.set_title('Hub score distribution on  a log-log scale')
'''
auths = {}
aths = []
for key, val in csv.reader(open("authorities.csv")):
	aths.append(val)
	auths[int(key)] = float(val)

print 'new'
top_aths = get_top_keys(auths, 20)
#for k in top_aths:
#	print k

print np.corrcoef(aths, ins)
print 'PR and ath', len(set(top_prs).intersection(top_aths))
print 'hub and ath', len(set(top_hubs).intersection(top_aths))
print 'in and ath', len(set(top_ins).intersection(top_aths))
print 'in and hub', len(set(top_ins).intersection(top_hubs))
print 'PR and hub', len(set(top_prs).intersection(top_hubs))

'''
print 'Histogram comp'
hist, bin_edges = np.histogram(auths.values(), bins= 250000)

fig = plt.figure(2)
ax = plt.gca()
ax.grid(True)
ax.plot(bin_edges[:-1], hist, 'o')
ax.set_yscale('log')
ax.set_xscale('log')
ax.set_xlim([0, 1])
ax.set_xlabel('Authority score')
ax.set_ylabel('Number of Nodes')
ax.set_title('Authority score distribution on  a log-log scale')

plt.show()
'''import networkx as nx
import matplotlib.pyplot as plt

node_data = {}
for line in open('cambridge_net_titles.txt'):
    splits = line.split(';')
    node_id = int(splits[0])
    place_title = splits[1]
    lat = float(splits[2])
    lon = float(splits[3])
    node_data[node_id] = (place_title, lat, lon)



cam_net = nx.read_edgelist('cambridge_net.txt',create_using=nx.DiGraph(), nodetype=int)
N, K = cam_net.order(), cam_net.size()
avg_deg = float(K) / N
print "Nodes: ", N
print "Edges: ", K
print "Average degree: ", avg_deg
print "SCC: ", nx.number_strongly_connected_components(cam_net)
print "WCC: ", nx.number_weakly_connected_components(cam_net)

in_degrees = cam_net.in_degree()
in_values = sorted(set(in_degrees.values()))
in_hist = [in_degrees.values().count(x) for x in in_values]

out_degrees = cam_net.out_degree()
out_values = sorted(set(out_degrees.values()))
out_hist = [out_degrees.values().count(x) for x in out_values]

degrees = cam_net.degree()


plt.figure() # you need to first do 'import pylab as plt' plt.grid(True)
plt.grid(True)
plt.loglog(in_values, in_hist, 'ro-') # in-degree
plt.loglog(out_values, out_hist, 'bv-')
plt.legend(['In-degree', 'Out-degree'])
plt.xlabel('Degree')
plt.ylabel('Number of Nodes')
plt.title('network of places in Cambridge')
plt.xlim([0, 2*10**2])
#plt.show()

cam_net_ud = cam_net.to_undirected()
# Clustering coefficient of node 0
print "Clustering coeficient of node 0:", nx.clustering(cam_net_ud, 0)
# Clustering coefficient of all nodes (in a dictionary)
clust_coefficients = nx.clustering(cam_net_ud)
# Average clustering coefficient
avg_clust = sum(clust_coefficients.values()) / len(clust_coefficients)
print "Average clustering coefficient", avg_clust
# Or use directly the built-in method
print "Average clustering coeffeicient again with built in method", nx.average_clustering(cam_net_ud)

def get_top_keys(dictionary, top):
    items = dictionary.items()
    items.sort(reverse=True, key=lambda x: x[1])
    return map(lambda x: x[0], items[:top])

print [node_data[x][0] + ":" + str(degrees[x]) for x in get_top_keys(degrees, 10)]

# Connected components are sorted in descending order of their size
cam_net_components = nx.connected_component_subgraphs(cam_net_ud)
cam_net_mc = cam_net_components.next()
print cam_net_mc
# Betweenness centrality
bet_cen = nx.betweenness_centrality(cam_net_mc)
print len(bet_cen.values())
print "Betweenness"
print [node_data[x][0] + ":" + str(bet_cen[x]) for x in get_top_keys(bet_cen, 10)]

# Closeness centrality
clo_cen = nx.closeness_centrality(cam_net_mc)
print "Closeness"
print [node_data[x][0] + ":" + str(clo_cen[x]) for x in get_top_keys(clo_cen, 10)]
#print "Closeness", clo_cen
# Eigenvector centrality
eig_cen = nx.eigenvector_centrality(cam_net_mc)
print "Eigenvector"
print [node_data[x][0] + ":" + str(eig_cen[x]) for x in get_top_keys(eig_cen, 10)]
#print "Eigenvector", eig_cen



# draw the graph using information about the nodes geographic position
pos_dict = {}
for node_id, node_info in node_data.items():
    pos_dict[node_id] = (node_info[2], node_info[1])
plt.figure()
nx.draw(cam_net, pos=pos_dict, with_labels=False, node_size=25)
plt.show()
#plt.savefig('cam_net_graph.pdf')
#plt.close()import networkx as nx
import powerlaw
import matplotlib.pyplot as plt
import csv
import numpy as np
import community

print "Started reading"
stanf = nx.read_edgelist('web-Stanford.txt', create_using = nx.DiGraph(), nodetype = int)
print "Finished reading"

def save_graph(graph,file_name):
    #initialze Figure
    plt.figure(num=None, figsize=(20, 20), dpi=80)
    plt.axis('off')
    fig = plt.figure(1)
    pos = nx.spring_layout(graph)
    nx.draw_networkx_nodes(graph,pos)
    nx.draw_networkx_edges(graph,pos)
    nx.draw_networkx_labels(graph,pos)

    cut = 1.00
    xmax = cut * max(xx for xx, yy in pos.values())
    ymax = cut * max(yy for xx, yy in pos.values())
    plt.xlim(0, xmax)
    plt.ylim(0, ymax)

    plt.savefig(file_name,bbox_inches="tight")

    del fig

def get_top_keys(dictionary, top):
	print 'Getting top 10'
	items = dictionary.items()
	items.sort(reverse=True, key=lambda x: x[1])
	return map(lambda x: x[0], items[:top])

'''
print "Hubsies"
h, a = nx.hits(stanf)

print "Storing hubsies"
w = csv.writer(open("hubs.csv", "w"))
for key, val in h.items():
    w.writerow([key, val])

w = csv.writer(open("authorities.csv", "w"))
for key, val in a.items():
    w.writerow([key, val])
'''

print 'Making undirected'
stanf_ud = stanf.to_undirected()
print 'Partitioning'
#print 'Average clustering coefficient ', nx.average_clustering(stanf_ud)
partition = community.best_partition(stanf_ud)
print 'Number of nodes', len(partition)
comms = set(partition.values())
size = len(comms)
print 'Number of comms', size

fw = open('communities.txt', 'w')
for com in comms:
	list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]
	print 'Number of nodes in community', com, ':', len(list_nodes)
	fw.write(str(com) + ',')
	fw.write(" ".join([str(x) for x in list_nodes]) + '\n')

fw.close()


'''
print 'Making comps'
components = nx.connected_component_subgraphs(stanf_ud)
largest_component = components.next()
print 'Finding subgraph'
sub_largest_component = largest_component.subgraph(np.random.permutation(largest_component.nodes())[:50000])
print 'Finding betweenness'
bet_cen = nx.betweenness_centrality(sub_largest_component)
print 'Saving'
w = csv.writer(open("betweens.csv", "w"))
for key, val in bet_cen.items():
    w.writerow([key, val])
print len(bet_cen)
print get_top_keys(bet_cen, 10)

print 'Finding components'
strong_components = nx.connected_components(stanf_ud)
b = [len(x) for x in strong_components]
print len(b)
b_values = sorted(set(b))
b_hist = [b.count(x) for x in b_values]

fig = plt.figure(1)
ax = plt.gca()
ax.grid(True)
ax.plot(b_values, b_hist, 'o')
ax.set_yscale('log')
ax.set_xscale('log')
ax.set_xlim([0, 2*10**6])
ax.set_xlabel('Size of weakly connected component')
ax.set_ylabel('Number of components')
ax.set_title('Distribution of size of WCCs on  a log-log scale')


plt.figure(3)
print "Fitting"
fitted_pl_in = powerlaw.Fit(b)
print fitted_pl_in.alpha
print fitted_pl_in.distribution_compare('power_law', 'lognormal')
fig4 = fitted_pl_in.plot_ccdf(linewidth = 3)
fitted_pl_in.power_law.plot_ccdf(ax=fig4, color = 'r', linestyle = '-')
plt.xlabel('Size of WCC')
plt.ylabel('p(Number of WCCs > S)')
plt.title('CCDF of empirical size of WCCs and best fit power law')
plt.show()
#print largest_strong.size()
'''

'''
print 'Clustering undirected'
clusts_dir = nx.clustering(stanf_ud)
print 'Saving undirected clusters'
w = csv.writer(open("clusters_undir.csv", "w"))
for key, val in clusts_dir.items():
    w.writerow([key, val])

#save_graph(nx.watts_strogatz_graph(100, 8, 0.1),"my_graph.pdf")

print "Basic stats started"
N, K = stanf.order(), stanf.size()
avg_deg = float(K) / N
print "Nodes: ", N
print "Edges: ", K
print "Average degree: ", avg_deg
print "SCC: ", nx.number_strongly_connected_components(stanf)
print "WCC: ", nx.number_weakly_connected_components(stanf)
print "Basic stats ended"

#print "computing degrees"

in_degrees = {}
ins = []
for key, val in csv.reader(open("in-degrees.csv")):
	ins.append(val)
	in_degrees[int(key)] = int(val)
#in_degrees = stanf.in_degree()
in_values = sorted(set(in_degrees.values()))
in_hist = [in_degrees.values().count(x) for x in in_values]

print 'writing degrees'
w = csv.writer(open("in-degrees.csv", "w"))
for key, val in in_degrees.items():
    w.writerow([key, val])

out_degrees = {}
for key, val in csv.reader(open("out-degrees.csv")):
    out_degrees[int(key)] = int(val)

#out_degrees = stanf.out_degree()
out_values = sorted(set(out_degrees.values()))
out_hist = [out_degrees.values().count(x) for x in out_values]
w = csv.writer(open("out-degrees.csv", "w"))
for key, val in out_degrees.items():
    w.writerow([key, val])

'''
#degrees = stanf.degree()

'''
print "plotting"
plt.figure() # you need to first do 'import pylab as plt' plt.grid(True)
plt.grid(True)
plt.loglog(in_values, in_hist, 'ro-') # in-degree
plt.loglog(out_values, out_hist, 'bv-')
plt.legend(['In-degree', 'Out-degree'])
plt.xlabel('Degree')
plt.ylabel('Number of Nodes')
plt.title('Web hyperlinking in the Stanford Web Dataset')
plt.xlim([0, 2*10**5])
'''

'''
fig = plt.figure(1)
ax = plt.gca()
ax.grid(True)
ax.plot(in_values, in_hist, 'o')
ax.set_yscale('log')
ax.set_xscale('log')
ax.set_xlim([0, 2*10**5])
ax.set_xlabel('Degree')
ax.set_ylabel('Number of Nodes')
ax.set_title('In-degree distribution on  a log-log scale')

fig = plt.figure(2)
ax = plt.gca()
ax.grid(True)
ax.plot(out_values, out_hist, 'o')
ax.set_yscale('log')
ax.set_xscale('log')
ax.set_xlim([0, 2*10**4])
ax.set_xlabel('Degree')
ax.set_ylabel('Number of Nodes')
ax.set_title('Out-degree distribution on  a log-log scale')
#plt.show()
'''

'''
plt.figure(3)
print "Fitting"
fitted_pl_in = powerlaw.Fit(in_degrees.values())
print "Number of in " + str(len(in_degrees.values()))
print fitted_pl_in.alpha
print fitted_pl_in.distribution_compare('power_law', 'lognormal')
fig4 = fitted_pl_in.plot_ccdf(linewidth = 3)
fitted_pl_in.power_law.plot_ccdf(ax=fig4, color = 'r', linestyle = '-')
plt.xlabel('In-degree')
plt.ylabel('p(Number of nodes > N)')
plt.title('Complementary cumulative distribution functions of empirical in-degrees and best fit power law')
plt.legend()

fitted_pl_out = powerlaw.Fit(out_degrees.values())
print "Number of outs " + str(len(out_degrees.values()))
print fitted_pl_out.alpha
'''

'''
print 'Computing pagerank'
#page_ranks = nx.pagerank(stanf, alpha=0.9)
page_ranks = {}
pgs = []
for key, val in csv.reader(open("page-ranks.csv")):
	pgs.append(val)
	page_ranks[int(key)] = float(val)
#print page_ranks.values()[45]
#print "Number of prs " + str(len(page_ranks.values()))
print 'Done pagerank'
print np.corrcoef(ins, pgs)
#print np.corrcoef(page_ranks.values())
#pr_values = sorted(set(page_ranks.values()))
#pr_hist = [page_ranks.values().count(x) for x in pr_values]
'''


'''
w = csv.writer(open("page-ranks.csv", "w"))
for key, val in page_ranks.items():
    w.writerow([key, val])

print 'Histogram comp'
hist, bin_edges = np.histogram(page_ranks.values(), bins= 10000)

fig = plt.figure(3)
ax = plt.gca()
ax.grid(True)
ax.plot(bin_edges[:-1], hist, 'o')
ax.set_yscale('log')
ax.set_xscale('log')
ax.set_xlim([0, 1])
ax.set_xlabel('Pagerank Value')
ax.set_ylabel('Number of Nodes')
ax.set_title('Pagerank value distribution on  a log-log scale')

plt.figure(3)
print "Fitting"
print type(page_ranks[46])
fitted_pl_pr = powerlaw.Fit(page_ranks.values(), fit_method='KS')
print "Number of in " + str(len(page_ranks.values()))
print fitted_pl_pr.alpha
print fitted_pl_pr.distribution_compare('power_law', 'lognormal')
fig4 = fitted_pl_pr.plot_ccdf(linewidth = 3)
fitted_pl_pr.power_law.plot_ccdf(ax=fig4, color = 'r', linestyle = '-')
plt.xlabel('Page-rank values')
plt.ylabel('p(Number of nodes > N)')
plt.title('CCDF of empirical pagerank values and best fit power law')

plt.show()
'''#!/usr/bin/python
# -*- coding: utf-8 -*-
"""
This module implements community detection.
"""
from __future__ import print_function
__all__ = ["partition_at_level", "modularity", "best_partition", "generate_dendrogram", "generate_dendogram", "induced_graph"]
__author__ = """Thomas Aynaud (thomas.aynaud@lip6.fr)"""
#    Copyright (C) 2009 by
#    Thomas Aynaud <thomas.aynaud@lip6.fr>
#    All rights reserved.
#    BSD license.

__PASS_MAX = -1
__MIN = 0.0000001

import networkx as nx
import sys
import types
import array


def partition_at_level(dendrogram, level) :
    """Return the partition of the nodes at the given level

    A dendrogram is a tree and each level is a partition of the graph nodes.
    Level 0 is the first partition, which contains the smallest communities, and the best is len(dendrogram) - 1.
    The higher the level is, the bigger are the communities

    Parameters
    ----------
    dendrogram : list of dict
       a list of partitions, ie dictionnaries where keys of the i+1 are the values of the i.
    level : int
       the level which belongs to [0..len(dendrogram)-1]

    Returns
    -------
    partition : dictionnary
       A dictionary where keys are the nodes and the values are the set it belongs to

    Raises
    ------
    KeyError
       If the dendrogram is not well formed or the level is too high

    See Also
    --------
    best_partition which directly combines partition_at_level and generate_dendrogram to obtain the partition of highest modularity

    Examples
    --------
    >>> G=nx.erdos_renyi_graph(100, 0.01)
    >>> dendo = generate_dendrogram(G)
    >>> for level in range(len(dendo) - 1) :
    >>>     print "partition at level", level, "is", partition_at_level(dendo, level)
    """
    partition = dendrogram[0].copy()
    for index in range(1, level + 1) :
        for node, community in partition.items() :
            partition[node] = dendrogram[index][community]
    return partition


def modularity(partition, graph) :
    """Compute the modularity of a partition of a graph

    Parameters
    ----------
    partition : dict
       the partition of the nodes, i.e a dictionary where keys are their nodes and values the communities
    graph : networkx.Graph
       the networkx graph which is decomposed

    Returns
    -------
    modularity : float
       The modularity

    Raises
    ------
    KeyError
       If the partition is not a partition of all graph nodes
    ValueError
        If the graph has no link
    TypeError
        If graph is not a networkx.Graph

    References
    ----------
    .. 1. Newman, M.E.J. & Girvan, M. Finding and evaluating community structure in networks. Physical Review E 69, 26113(2004).

    Examples
    --------
    >>> G=nx.erdos_renyi_graph(100, 0.01)
    >>> part = best_partition(G)
    >>> modularity(part, G)
    """
    if type(graph) != nx.Graph :
        raise TypeError("Bad graph type, use only non directed graph")

    inc = dict([])
    deg = dict([])
    links = graph.size(weight='weight')
    if links == 0 :
        raise ValueError("A graph without link has an undefined modularity")

    for node in graph :
        com = partition[node]
        deg[com] = deg.get(com, 0.) + graph.degree(node, weight = 'weight')
        for neighbor, datas in graph[node].items() :
            weight = datas.get("weight", 1)
            if partition[neighbor] == com :
                if neighbor == node :
                    inc[com] = inc.get(com, 0.) + float(weight)
                else :
                    inc[com] = inc.get(com, 0.) + float(weight) / 2.

    res = 0.
    for com in set(partition.values()) :
        res += (inc.get(com, 0.) / links) - (deg.get(com, 0.) / (2.*links))**2
    return res


def best_partition(graph, partition = None) :
    """Compute the partition of the graph nodes which maximises the modularity
    (or try..) using the Louvain heuristices

    This is the partition of highest modularity, i.e. the highest partition of the dendrogram
    generated by the Louvain algorithm.

    Parameters
    ----------
    graph : networkx.Graph
       the networkx graph which is decomposed
    partition : dict, optionnal
       the algorithm will start using this partition of the nodes. It's a dictionary where keys are their nodes and values the communities

    Returns
    -------
    partition : dictionnary
       The partition, with communities numbered from 0 to number of communities

    Raises
    ------
    NetworkXError
       If the graph is not Eulerian.

    See Also
    --------
    generate_dendrogram to obtain all the decompositions levels

    Notes
    -----
    Uses Louvain algorithm

    References
    ----------
    .. 1. Blondel, V.D. et al. Fast unfolding of communities in large networks. J. Stat. Mech 10008, 1-12(2008).

    Examples
    --------
    >>>  #Basic usage
    >>> G=nx.erdos_renyi_graph(100, 0.01)
    >>> part = best_partition(G)

    >>> #other example to display a graph with its community :
    >>> #better with karate_graph() as defined in networkx examples
    >>> #erdos renyi don't have true community structure
    >>> G = nx.erdos_renyi_graph(30, 0.05)
    >>> #first compute the best partition
    >>> partition = community.best_partition(G)
    >>>  #drawing
    >>> size = float(len(set(partition.values())))
    >>> pos = nx.spring_layout(G)
    >>> count = 0.
    >>> for com in set(partition.values()) :
    >>>     count = count + 1.
    >>>     list_nodes = [nodes for nodes in partition.keys()
    >>>                                 if partition[nodes] == com]
    >>>     nx.draw_networkx_nodes(G, pos, list_nodes, node_size = 20,
                                    node_color = str(count / size))
    >>> nx.draw_networkx_edges(G,pos, alpha=0.5)
    >>> plt.show()
    """
    dendo = generate_dendrogram(graph, partition)
    return partition_at_level(dendo, len(dendo) - 1 )


def generate_dendogram(graph, part_init = None) :
    """Deprecated, use generate_dendrogram"""
    return generate_dendrogram(graph, part_init)
    
    
def generate_dendrogram(graph, part_init = None) :
    """Find communities in the graph and return the associated dendrogram

    A dendrogram is a tree and each level is a partition of the graph nodes.  Level 0 is the first partition, which contains the smallest communities, and the best is len(dendrogram) - 1. The higher the level is, the bigger are the communities


    Parameters
    ----------
    graph : networkx.Graph
        the networkx graph which will be decomposed
    part_init : dict, optionnal
        the algorithm will start using this partition of the nodes. It's a dictionary where keys are their nodes and values the communities

    Returns
    -------
    dendrogram : list of dictionaries
        a list of partitions, ie dictionnaries where keys of the i+1 are the values of the i. and where keys of the first are the nodes of graph

    Raises
    ------
    TypeError
        If the graph is not a networkx.Graph

    See Also
    --------
    best_partition

    Notes
    -----
    Uses Louvain algorithm

    References
    ----------
    .. 1. Blondel, V.D. et al. Fast unfolding of communities in large networks. J. Stat. Mech 10008, 1-12(2008).

    Examples
    --------
    >>> G=nx.erdos_renyi_graph(100, 0.01)
    >>> dendo = generate_dendrogram(G)
    >>> for level in range(len(dendo) - 1) :
    >>>     print "partition at level", level, "is", partition_at_level(dendo, level)
    """
    if type(graph) != nx.Graph :
        raise TypeError("Bad graph type, use only non directed graph")

    #special case, when there is no link
    #the best partition is everyone in its community
    if graph.number_of_edges() == 0 :
        part = dict([])
        for node in graph.nodes() :
            part[node] = node
        return part

    current_graph = graph.copy()
    status = Status()
    status.init(current_graph, part_init)
    mod = __modularity(status)
    status_list = list()
    __one_level(current_graph, status)
    new_mod = __modularity(status)
    partition = __renumber(status.node2com)
    status_list.append(partition)
    mod = new_mod
    current_graph = induced_graph(partition, current_graph)
    status.init(current_graph)

    while True :
        __one_level(current_graph, status)
        new_mod = __modularity(status)
        if new_mod - mod < __MIN :
            break
        partition = __renumber(status.node2com)
        status_list.append(partition)
        mod = new_mod
        current_graph = induced_graph(partition, current_graph)
        status.init(current_graph)
    return status_list[:]


def induced_graph(partition, graph) :
    """Produce the graph where nodes are the communities

    there is a link of weight w between communities if the sum of the weights of the links between their elements is w

    Parameters
    ----------
    partition : dict
       a dictionary where keys are graph nodes and  values the part the node belongs to
    graph : networkx.Graph
        the initial graph

    Returns
    -------
    g : networkx.Graph
       a networkx graph where nodes are the parts

    Examples
    --------
    >>> n = 5
    >>> g = nx.complete_graph(2*n)
    >>> part = dict([])
    >>> for node in g.nodes() :
    >>>     part[node] = node % 2
    >>> ind = induced_graph(part, g)
    >>> goal = nx.Graph()
    >>> goal.add_weighted_edges_from([(0,1,n*n),(0,0,n*(n-1)/2), (1, 1, n*(n-1)/2)])
    >>> nx.is_isomorphic(int, goal)
    True
    """
    ret = nx.Graph()
    ret.add_nodes_from(partition.values())

    for node1, node2, datas in graph.edges_iter(data = True) :
        weight = datas.get("weight", 1)
        com1 = partition[node1]
        com2 = partition[node2]
        w_prec = ret.get_edge_data(com1, com2, {"weight":0}).get("weight", 1)
        ret.add_edge(com1, com2, weight = w_prec + weight)

    return ret


def __renumber(dictionary) :
    """Renumber the values of the dictionary from 0 to n
    """
    count = 0
    ret = dictionary.copy()
    new_values = dict([])

    for key in dictionary.keys() :
        value = dictionary[key]
        new_value = new_values.get(value, -1)
        if new_value == -1 :
            new_values[value] = count
            new_value = count
            count = count + 1
        ret[key] = new_value

    return ret


def __load_binary(data) :
    """Load binary graph as used by the cpp implementation of this algorithm
    """
    data = open(data, "rb")

    reader = array.array("I")
    reader.fromfile(data, 1)
    num_nodes = reader.pop()
    reader = array.array("I")
    reader.fromfile(data, num_nodes)
    cum_deg = reader.tolist()
    num_links = reader.pop()
    reader = array.array("I")
    reader.fromfile(data, num_links)
    links = reader.tolist()
    graph = nx.Graph()
    graph.add_nodes_from(range(num_nodes))
    prec_deg = 0

    for index in range(num_nodes) :
        last_deg = cum_deg[index]
        neighbors = links[prec_deg:last_deg]
        graph.add_edges_from([(index, int(neigh)) for neigh in neighbors])
        prec_deg = last_deg

    return graph


def __one_level(graph, status) :
    """Compute one level of communities
    """
    modif = True
    nb_pass_done = 0
    cur_mod = __modularity(status)
    new_mod = cur_mod

    while modif  and nb_pass_done != __PASS_MAX :
        cur_mod = new_mod
        modif = False
        nb_pass_done += 1

        for node in graph.nodes() :
            com_node = status.node2com[node]
            degc_totw = status.gdegrees.get(node, 0.) / (status.total_weight*2.)
            neigh_communities = __neighcom(node, graph, status)
            __remove(node, com_node,
                    neigh_communities.get(com_node, 0.), status)
            best_com = com_node
            best_increase = 0
            for com, dnc in neigh_communities.items() :
                incr =  dnc  - status.degrees.get(com, 0.) * degc_totw
                if incr > best_increase :
                    best_increase = incr
                    best_com = com
            __insert(node, best_com,
                    neigh_communities.get(best_com, 0.), status)
            if best_com != com_node :
                modif = True
        new_mod = __modularity(status)
        if new_mod - cur_mod < __MIN :
            break


class Status :
    """
    To handle several data in one struct.

    Could be replaced by named tuple, but don't want to depend on python 2.6
    """
    node2com = {}
    total_weight = 0
    internals = {}
    degrees = {}
    gdegrees = {}

    def __init__(self) :
        self.node2com = dict([])
        self.total_weight = 0
        self.degrees = dict([])
        self.gdegrees = dict([])
        self.internals = dict([])
        self.loops = dict([])

    def __str__(self) :
        return ("node2com : " + str(self.node2com) + " degrees : "
            + str(self.degrees) + " internals : " + str(self.internals)
            + " total_weight : " + str(self.total_weight))

    def copy(self) :
        """Perform a deep copy of status"""
        new_status = Status()
        new_status.node2com = self.node2com.copy()
        new_status.internals = self.internals.copy()
        new_status.degrees = self.degrees.copy()
        new_status.gdegrees = self.gdegrees.copy()
        new_status.total_weight = self.total_weight

    def init(self, graph, part = None) :
        """Initialize the status of a graph with every node in one community"""
        count = 0
        self.node2com = dict([])
        self.total_weight = 0
        self.degrees = dict([])
        self.gdegrees = dict([])
        self.internals = dict([])
        self.total_weight = graph.size(weight = 'weight')
        if part == None :
            for node in graph.nodes() :
                self.node2com[node] = count
                deg = float(graph.degree(node, weight = 'weight'))
                if deg < 0 :
                    raise ValueError("Bad graph type, use positive weights")
                self.degrees[count] = deg
                self.gdegrees[node] = deg
                self.loops[node] = float(graph.get_edge_data(node, node,
                                                 {"weight":0}).get("weight", 1))
                self.internals[count] = self.loops[node]
                count = count + 1
        else :
            for node in graph.nodes() :
                com = part[node]
                self.node2com[node] = com
                deg = float(graph.degree(node, weight = 'weight'))
                self.degrees[com] = self.degrees.get(com, 0) + deg
                self.gdegrees[node] = deg
                inc = 0.
                for neighbor, datas in graph[node].items() :
                    weight = datas.get("weight", 1)
                    if weight <= 0 :
                        raise ValueError("Bad graph type, use positive weights")
                    if part[neighbor] == com :
                        if neighbor == node :
                            inc += float(weight)
                        else :
                            inc += float(weight) / 2.
                self.internals[com] = self.internals.get(com, 0) + inc



def __neighcom(node, graph, status) :
    """
    Compute the communities in the neighborood of node in the graph given
    with the decomposition node2com
    """
    weights = {}
    for neighbor, datas in graph[node].items() :
        if neighbor != node :
            weight = datas.get("weight", 1)
            neighborcom = status.node2com[neighbor]
            weights[neighborcom] = weights.get(neighborcom, 0) + weight

    return weights


def __remove(node, com, weight, status) :
    """ Remove node from community com and modify status"""
    status.degrees[com] = ( status.degrees.get(com, 0.)
                                    - status.gdegrees.get(node, 0.) )
    status.internals[com] = float( status.internals.get(com, 0.) -
                weight - status.loops.get(node, 0.) )
    status.node2com[node] = -1


def __insert(node, com, weight, status) :
    """ Insert node into community and modify status"""
    status.node2com[node] = com
    status.degrees[com] = ( status.degrees.get(com, 0.) +
                                status.gdegrees.get(node, 0.) )
    status.internals[com] = float( status.internals.get(com, 0.) +
                        weight + status.loops.get(node, 0.) )


def __modularity(status) :
    """
    Compute the modularity of the partition of the graph faslty using status precomputed
    """
    links = float(status.total_weight)
    result = 0.
    for community in set(status.node2com.values()) :
        in_degree = status.internals.get(community, 0.)
        degree = status.degrees.get(community, 0.)
        if links > 0 :
            result = result + in_degree / links - ((degree / (2.*links))**2)
    return result


def main() :
    """Main function to mimic C++ version behavior"""
    try :
        filename = sys.argv[1]
        graphfile = __load_binary(filename)
        partition = best_partition(graphfile)
        print(str(modularity(partition, graphfile)), file=sys.stderr)
        for elem, part in partition.items() :
            print(str(elem) + " " + str(part))
    except (IndexError, IOError):
        print("Usage : ./community filename")
        print("find the communities in graph filename and display the dendrogram")
        print("Parameters:")
        print("filename is a binary file as generated by the ")
        print("convert utility distributed with the C implementation")
#!/usr/bin/python
# -*- coding: utf-8 -*-
"""
This module implements community detection.
"""
from __future__ import print_function
__all__ = ["partition_at_level", "modularity", "best_partition", "generate_dendrogram", "generate_dendogram", "induced_graph"]
__author__ = """Thomas Aynaud (thomas.aynaud@lip6.fr)"""
#    Copyright (C) 2009 by
#    Thomas Aynaud <thomas.aynaud@lip6.fr>
#    All rights reserved.
#    BSD license.

__PASS_MAX = -1
__MIN = 0.0000001

import networkx as nx
import sys
import types
import array


def partition_at_level(dendrogram, level) :
    """Return the partition of the nodes at the given level

    A dendrogram is a tree and each level is a partition of the graph nodes.
    Level 0 is the first partition, which contains the smallest communities, and the best is len(dendrogram) - 1.
    The higher the level is, the bigger are the communities

    Parameters
    ----------
    dendrogram : list of dict
       a list of partitions, ie dictionnaries where keys of the i+1 are the values of the i.
    level : int
       the level which belongs to [0..len(dendrogram)-1]

    Returns
    -------
    partition : dictionnary
       A dictionary where keys are the nodes and the values are the set it belongs to

    Raises
    ------
    KeyError
       If the dendrogram is not well formed or the level is too high

    See Also
    --------
    best_partition which directly combines partition_at_level and generate_dendrogram to obtain the partition of highest modularity

    Examples
    --------
    >>> G=nx.erdos_renyi_graph(100, 0.01)
    >>> dendo = generate_dendrogram(G)
    >>> for level in range(len(dendo) - 1) :
    >>>     print "partition at level", level, "is", partition_at_level(dendo, level)
    """
    partition = dendrogram[0].copy()
    for index in range(1, level + 1) :
        for node, community in partition.items() :
            partition[node] = dendrogram[index][community]
    return partition


def modularity(partition, graph) :
    """Compute the modularity of a partition of a graph

    Parameters
    ----------
    partition : dict
       the partition of the nodes, i.e a dictionary where keys are their nodes and values the communities
    graph : networkx.Graph
       the networkx graph which is decomposed

    Returns
    -------
    modularity : float
       The modularity

    Raises
    ------
    KeyError
       If the partition is not a partition of all graph nodes
    ValueError
        If the graph has no link
    TypeError
        If graph is not a networkx.Graph

    References
    ----------
    .. 1. Newman, M.E.J. & Girvan, M. Finding and evaluating community structure in networks. Physical Review E 69, 26113(2004).

    Examples
    --------
    >>> G=nx.erdos_renyi_graph(100, 0.01)
    >>> part = best_partition(G)
    >>> modularity(part, G)
    """
    if type(graph) != nx.Graph :
        raise TypeError("Bad graph type, use only non directed graph")

    inc = dict([])
    deg = dict([])
    links = graph.size(weight='weight')
    if links == 0 :
        raise ValueError("A graph without link has an undefined modularity")

    for node in graph :
        com = partition[node]
        deg[com] = deg.get(com, 0.) + graph.degree(node, weight = 'weight')
        for neighbor, datas in graph[node].items() :
            weight = datas.get("weight", 1)
            if partition[neighbor] == com :
                if neighbor == node :
                    inc[com] = inc.get(com, 0.) + float(weight)
                else :
                    inc[com] = inc.get(com, 0.) + float(weight) / 2.

    res = 0.
    for com in set(partition.values()) :
        res += (inc.get(com, 0.) / links) - (deg.get(com, 0.) / (2.*links))**2
    return res


def best_partition(graph, partition = None) :
    """Compute the partition of the graph nodes which maximises the modularity
    (or try..) using the Louvain heuristices

    This is the partition of highest modularity, i.e. the highest partition of the dendrogram
    generated by the Louvain algorithm.

    Parameters
    ----------
    graph : networkx.Graph
       the networkx graph which is decomposed
    partition : dict, optionnal
       the algorithm will start using this partition of the nodes. It's a dictionary where keys are their nodes and values the communities

    Returns
    -------
    partition : dictionnary
       The partition, with communities numbered from 0 to number of communities

    Raises
    ------
    NetworkXError
       If the graph is not Eulerian.

    See Also
    --------
    generate_dendrogram to obtain all the decompositions levels

    Notes
    -----
    Uses Louvain algorithm

    References
    ----------
    .. 1. Blondel, V.D. et al. Fast unfolding of communities in large networks. J. Stat. Mech 10008, 1-12(2008).

    Examples
    --------
    >>>  #Basic usage
    >>> G=nx.erdos_renyi_graph(100, 0.01)
    >>> part = best_partition(G)

    >>> #other example to display a graph with its community :
    >>> #better with karate_graph() as defined in networkx examples
    >>> #erdos renyi don't have true community structure
    >>> G = nx.erdos_renyi_graph(30, 0.05)
    >>> #first compute the best partition
    >>> partition = community.best_partition(G)
    >>>  #drawing
    >>> size = float(len(set(partition.values())))
    >>> pos = nx.spring_layout(G)
    >>> count = 0.
    >>> for com in set(partition.values()) :
    >>>     count = count + 1.
    >>>     list_nodes = [nodes for nodes in partition.keys()
    >>>                                 if partition[nodes] == com]
    >>>     nx.draw_networkx_nodes(G, pos, list_nodes, node_size = 20,
                                    node_color = str(count / size))
    >>> nx.draw_networkx_edges(G,pos, alpha=0.5)
    >>> plt.show()
    """
    dendo = generate_dendrogram(graph, partition)
    return partition_at_level(dendo, len(dendo) - 1 )


def generate_dendogram(graph, part_init = None) :
    """Deprecated, use generate_dendrogram"""
    return generate_dendrogram(graph, part_init)
    
    
def generate_dendrogram(graph, part_init = None) :
    """Find communities in the graph and return the associated dendrogram

    A dendrogram is a tree and each level is a partition of the graph nodes.  Level 0 is the first partition, which contains the smallest communities, and the best is len(dendrogram) - 1. The higher the level is, the bigger are the communities


    Parameters
    ----------
    graph : networkx.Graph
        the networkx graph which will be decomposed
    part_init : dict, optionnal
        the algorithm will start using this partition of the nodes. It's a dictionary where keys are their nodes and values the communities

    Returns
    -------
    dendrogram : list of dictionaries
        a list of partitions, ie dictionnaries where keys of the i+1 are the values of the i. and where keys of the first are the nodes of graph

    Raises
    ------
    TypeError
        If the graph is not a networkx.Graph

    See Also
    --------
    best_partition

    Notes
    -----
    Uses Louvain algorithm

    References
    ----------
    .. 1. Blondel, V.D. et al. Fast unfolding of communities in large networks. J. Stat. Mech 10008, 1-12(2008).

    Examples
    --------
    >>> G=nx.erdos_renyi_graph(100, 0.01)
    >>> dendo = generate_dendrogram(G)
    >>> for level in range(len(dendo) - 1) :
    >>>     print "partition at level", level, "is", partition_at_level(dendo, level)
    """
    if type(graph) != nx.Graph :
        raise TypeError("Bad graph type, use only non directed graph")

    #special case, when there is no link
    #the best partition is everyone in its community
    if graph.number_of_edges() == 0 :
        part = dict([])
        for node in graph.nodes() :
            part[node] = node
        return part

    current_graph = graph.copy()
    status = Status()
    status.init(current_graph, part_init)
    mod = __modularity(status)
    status_list = list()
    __one_level(current_graph, status)
    new_mod = __modularity(status)
    partition = __renumber(status.node2com)
    status_list.append(partition)
    mod = new_mod
    current_graph = induced_graph(partition, current_graph)
    status.init(current_graph)

    while True :
        __one_level(current_graph, status)
        new_mod = __modularity(status)
        if new_mod - mod < __MIN :
            break
        partition = __renumber(status.node2com)
        status_list.append(partition)
        mod = new_mod
        current_graph = induced_graph(partition, current_graph)
        status.init(current_graph)
    return status_list[:]


def induced_graph(partition, graph) :
    """Produce the graph where nodes are the communities

    there is a link of weight w between communities if the sum of the weights of the links between their elements is w

    Parameters
    ----------
    partition : dict
       a dictionary where keys are graph nodes and  values the part the node belongs to
    graph : networkx.Graph
        the initial graph

    Returns
    -------
    g : networkx.Graph
       a networkx graph where nodes are the parts

    Examples
    --------
    >>> n = 5
    >>> g = nx.complete_graph(2*n)
    >>> part = dict([])
    >>> for node in g.nodes() :
    >>>     part[node] = node % 2
    >>> ind = induced_graph(part, g)
    >>> goal = nx.Graph()
    >>> goal.add_weighted_edges_from([(0,1,n*n),(0,0,n*(n-1)/2), (1, 1, n*(n-1)/2)])
    >>> nx.is_isomorphic(int, goal)
    True
    """
    ret = nx.Graph()
    ret.add_nodes_from(partition.values())

    for node1, node2, datas in graph.edges_iter(data = True) :
        weight = datas.get("weight", 1)
        com1 = partition[node1]
        com2 = partition[node2]
        w_prec = ret.get_edge_data(com1, com2, {"weight":0}).get("weight", 1)
        ret.add_edge(com1, com2, weight = w_prec + weight)

    return ret


def __renumber(dictionary) :
    """Renumber the values of the dictionary from 0 to n
    """
    count = 0
    ret = dictionary.copy()
    new_values = dict([])

    for key in dictionary.keys() :
        value = dictionary[key]
        new_value = new_values.get(value, -1)
        if new_value == -1 :
            new_values[value] = count
            new_value = count
            count = count + 1
        ret[key] = new_value

    return ret


def __load_binary(data) :
    """Load binary graph as used by the cpp implementation of this algorithm
    """
    data = open(data, "rb")

    reader = array.array("I")
    reader.fromfile(data, 1)
    num_nodes = reader.pop()
    reader = array.array("I")
    reader.fromfile(data, num_nodes)
    cum_deg = reader.tolist()
    num_links = reader.pop()
    reader = array.array("I")
    reader.fromfile(data, num_links)
    links = reader.tolist()
    graph = nx.Graph()
    graph.add_nodes_from(range(num_nodes))
    prec_deg = 0

    for index in range(num_nodes) :
        last_deg = cum_deg[index]
        neighbors = links[prec_deg:last_deg]
        graph.add_edges_from([(index, int(neigh)) for neigh in neighbors])
        prec_deg = last_deg

    return graph


def __one_level(graph, status) :
    """Compute one level of communities
    """
    modif = True
    nb_pass_done = 0
    cur_mod = __modularity(status)
    new_mod = cur_mod

    while modif  and nb_pass_done != __PASS_MAX :
        cur_mod = new_mod
        modif = False
        nb_pass_done += 1

        for node in graph.nodes() :
            com_node = status.node2com[node]
            degc_totw = status.gdegrees.get(node, 0.) / (status.total_weight*2.)
            neigh_communities = __neighcom(node, graph, status)
            __remove(node, com_node,
                    neigh_communities.get(com_node, 0.), status)
            best_com = com_node
            best_increase = 0
            for com, dnc in neigh_communities.items() :
                incr =  dnc  - status.degrees.get(com, 0.) * degc_totw
                if incr > best_increase :
                    best_increase = incr
                    best_com = com
            __insert(node, best_com,
                    neigh_communities.get(best_com, 0.), status)
            if best_com != com_node :
                modif = True
        new_mod = __modularity(status)
        if new_mod - cur_mod < __MIN :
            break


class Status :
    """
    To handle several data in one struct.

    Could be replaced by named tuple, but don't want to depend on python 2.6
    """
    node2com = {}
    total_weight = 0
    internals = {}
    degrees = {}
    gdegrees = {}

    def __init__(self) :
        self.node2com = dict([])
        self.total_weight = 0
        self.degrees = dict([])
        self.gdegrees = dict([])
        self.internals = dict([])
        self.loops = dict([])

    def __str__(self) :
        return ("node2com : " + str(self.node2com) + " degrees : "
            + str(self.degrees) + " internals : " + str(self.internals)
            + " total_weight : " + str(self.total_weight))

    def copy(self) :
        """Perform a deep copy of status"""
        new_status = Status()
        new_status.node2com = self.node2com.copy()
        new_status.internals = self.internals.copy()
        new_status.degrees = self.degrees.copy()
        new_status.gdegrees = self.gdegrees.copy()
        new_status.total_weight = self.total_weight

    def init(self, graph, part = None) :
        """Initialize the status of a graph with every node in one community"""
        count = 0
        self.node2com = dict([])
        self.total_weight = 0
        self.degrees = dict([])
        self.gdegrees = dict([])
        self.internals = dict([])
        self.total_weight = graph.size(weight = 'weight')
        if part == None :
            for node in graph.nodes() :
                self.node2com[node] = count
                deg = float(graph.degree(node, weight = 'weight'))
                if deg < 0 :
                    raise ValueError("Bad graph type, use positive weights")
                self.degrees[count] = deg
                self.gdegrees[node] = deg
                self.loops[node] = float(graph.get_edge_data(node, node,
                                                 {"weight":0}).get("weight", 1))
                self.internals[count] = self.loops[node]
                count = count + 1
        else :
            for node in graph.nodes() :
                com = part[node]
                self.node2com[node] = com
                deg = float(graph.degree(node, weight = 'weight'))
                self.degrees[com] = self.degrees.get(com, 0) + deg
                self.gdegrees[node] = deg
                inc = 0.
                for neighbor, datas in graph[node].items() :
                    weight = datas.get("weight", 1)
                    if weight <= 0 :
                        raise ValueError("Bad graph type, use positive weights")
                    if part[neighbor] == com :
                        if neighbor == node :
                            inc += float(weight)
                        else :
                            inc += float(weight) / 2.
                self.internals[com] = self.internals.get(com, 0) + inc



def __neighcom(node, graph, status) :
    """
    Compute the communities in the neighborood of node in the graph given
    with the decomposition node2com
    """
    weights = {}
    for neighbor, datas in graph[node].items() :
        if neighbor != node :
            weight = datas.get("weight", 1)
            neighborcom = status.node2com[neighbor]
            weights[neighborcom] = weights.get(neighborcom, 0) + weight

    return weights


def __remove(node, com, weight, status) :
    """ Remove node from community com and modify status"""
    status.degrees[com] = ( status.degrees.get(com, 0.)
                                    - status.gdegrees.get(node, 0.) )
    status.internals[com] = float( status.internals.get(com, 0.) -
                weight - status.loops.get(node, 0.) )
    status.node2com[node] = -1


def __insert(node, com, weight, status) :
    """ Insert node into community and modify status"""
    status.node2com[node] = com
    status.degrees[com] = ( status.degrees.get(com, 0.) +
                                status.gdegrees.get(node, 0.) )
    status.internals[com] = float( status.internals.get(com, 0.) +
                        weight + status.loops.get(node, 0.) )


def __modularity(status) :
    """
    Compute the modularity of the partition of the graph faslty using status precomputed
    """
    links = float(status.total_weight)
    result = 0.
    for community in set(status.node2com.values()) :
        in_degree = status.internals.get(community, 0.)
        degree = status.degrees.get(community, 0.)
        if links > 0 :
            result = result + in_degree / links - ((degree / (2.*links))**2)
    return result


def main() :
    """Main function to mimic C++ version behavior"""
    try :
        filename = sys.argv[1]
        graphfile = __load_binary(filename)
        partition = best_partition(graphfile)
        print(str(modularity(partition, graphfile)), file=sys.stderr)
        for elem, part in partition.items() :
            print(str(elem) + " " + str(part))
    except (IndexError, IOError):
        print("Usage : ./community filename")
        print("find the communities in graph filename and display the dendrogram")
        print("Parameters:")
        print("filename is a binary file as generated by the ")
        print("convert utility distributed with the C implementation")
# -*- coding: utf-8 -*-
#
# Community detection for NetworkX documentation build configuration file, created by
# sphinx-quickstart on Fri Jan 15 11:13:59 2010.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

import community
# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.append(os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.doctest', 'sphinx.ext.coverage', 'sphinx.ext.autosummary', 'numpydoc']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']


autodoc_member_order = "groupwise"
autosummary_generate = ["api"]


# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Community detection for NetworkX'
copyright = u'2010, Thomas Aynaud'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '1'
# The full version, including alpha/beta/rc tags.
release = '2'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directory, that shouldn't be searched
# for source files.
exclude_trees = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  Major themes that come with
# Sphinx are currently 'default' and 'sphinxdoc'.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'CommunitydetectionforNetworkXdoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'CommunitydetectionforNetworkX.tex', u'Community detection for NetworkX Documentation',
   u'Thomas Aynaud', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True
from setuptools import setup

setup(
    name="python-louvain",
    version="0.2",
    author="Thomas Aynaud",
    author_email="thomas.aynaud@lip6.fr",
    description="Louvain algorithm for community detection",
    license="BSD",
    url="http://perso.crans.org/aynaud/communities/",
    classifiers=[
        "Programming Language :: Python",
        "Programming Language :: Python :: 2.7",
        "License :: OSI Approved :: BSD License",
        "Development Status :: 4 - Beta",
    ],

    packages=['community'],
    install_requires=[
        "networkx",
    ],

    entry_points={
        'console_scripts': [
            'community = community:main',
        ]
    }
)
import unittest
import networkx as nx
import community as co
import random

def girvan_graphs(zout) :
    """
    Create a graph of 128 vertices, 4 communities, like in
    Community Structure in  social and biological networks.
    Girvan newman, 2002. PNAS June, vol 99 n 12

    community is node modulo 4
    """

    pout = float(zout)/96.
    pin = (16.-pout*96.)/31.
    graph = nx.Graph()
    graph.add_nodes_from(range(128))
    for x in graph.nodes() :
        for y in graph.nodes() :
            if x < y :
                val = random.random()
                if x % 4 == y % 4 :
                    #nodes belong to the same community
                    if val < pin :
                        graph.add_edge(x, y)

                else :
                    if val < pout :
                        graph.add_edge(x, y)
    return graph

class ModularityTest(unittest.TestCase):

    numtest = 10

    def test_allin_is_zero(self):
        """it test that everyone in one community has a modularity of 0"""
        for i in range(self.numtest) :
            g = nx.erdos_renyi_graph(50, 0.1)
            part = dict([])
            for node in g :
                part[node] = 0
            self.assertEqual(co.modularity(part, g), 0)

    def test_range(self) :
        """test that modularity is always between -1 and 1"""
        for i in range(self.numtest) :
            g = nx.erdos_renyi_graph(50, 0.1)
            part = dict([])
            for node in g :
                part[node] = random.randint(0, self.numtest/10)
            mod = co.modularity(part, g)
            self.assertGreaterEqual(mod, -1)
            self.assertLessEqual(mod, 1)

    def test_bad_graph_input(self) :
        """modularity is only defined with undirected graph"""
        g = nx.erdos_renyi_graph(50, 0.1, directed=True)
        part = dict([])
        for node in g :
            part[node] = 0
        self.assertRaises(TypeError, co.modularity, part, g)

    def test_empty_graph_input(self) :
        """modularity of a graph without links is undefined"""
        g = nx.Graph()
        g.add_nodes_from(range(10))
        part = dict([])
        for node in g :
            part[node] = 0
        self.assertRaises(ValueError, co.modularity, part, g)

    def test_bad_partition_input(self) :
        """modularity is undefined when some nodes are not in a community"""
        g = nx.erdos_renyi_graph(50, 0.1)
        part = dict([])
        for count, node in enumerate(g) :
            part[node] = 0
            if count == 40 :
                break
        self.assertRaises(KeyError, co.modularity, part, g)

    #These are known values taken from the paper
    #1. Bartheemy, M. & Fortunato, S. Resolution limit in community detection. Proceedings of the National Academy of Sciences of the United States of America 104, 36-41(2007).
    def test_disjoint_clique(self) :
        """"
        A group of num_clique of size size_clique disjoint, should maximize the modularity
        and have a modularity of 1 - 1/ num_clique
        """
        for num_test in range(self.numtest) :
            size_clique = random.randint(5, 20)
            num_clique = random.randint(5, 20)
            g = nx.Graph()
            for i in range(num_clique) :
                clique_i = nx.complete_graph(size_clique)
                g = nx.union(g, clique_i, rename=("",str(i)+"_"))
            part = dict([])
            for node in g :
                part[node] = node.split("_")[0].strip()
            mod = co.modularity(part, g)
            self.assertAlmostEqual(mod, 1. - 1./float(num_clique),  msg = "Num clique: " + str(num_clique) + " size_clique: " + str(size_clique))

    def test_ring_clique(self) :
        """"
        then, a group of num_clique of size size_clique connected with only two links to other in a ring
        have a modularity of 1 - 1/ num_clique - num_clique / num_links
        """
        for num_test in range(self.numtest) :
            size_clique = random.randint(5, 20)
            num_clique = random.randint(5, 20)
            g = nx.Graph()
            for i in range(num_clique) :
                clique_i = nx.complete_graph(size_clique)
                g = nx.union(g, clique_i, rename=("",str(i)+"_"))
                if i > 0 :
                    g.add_edge(str(i)+"_0", str(i-1)+"_1")
            g.add_edge("0_0", str(num_clique-1)+"_1")
            part = dict([])
            for node in g :
                part[node] = node.split("_")[0].strip()
            mod = co.modularity(part, g)
            self.assertAlmostEqual(mod, 1. - 1./float(num_clique) - float(num_clique) / float(g.number_of_edges()), msg = "Num clique: " + str(num_clique) + " size_clique: " + str(size_clique) )



class BestPartitionTest(unittest.TestCase):
    numtest = 10

    def test_bad_graph_input(self) :
        """best_partition is only defined with undirected graph"""
        g = nx.erdos_renyi_graph(50, 0.1, directed=True)
        self.assertRaises(TypeError, co.best_partition,  g)

    def test_girvan(self) :
        """
        Test that community found are good using Girvan & Newman benchmark
        """
        g = girvan_graphs(4)#use small zout, with high zout results may change
        part = co.best_partition(g)
        for node, com in part.items() :
            self.assertEqual(com, part[node%4])

    def test_ring(self) :
        """
        Test that community found are good using a ring of cliques
        """
        for num_test in range(self.numtest) :
            size_clique = random.randint(5, 20)
            num_clique = random.randint(5, 20)
            g = nx.Graph()
            for i in range(num_clique) :
                clique_i = nx.complete_graph(size_clique)
                g = nx.union(g, clique_i, rename=("",str(i)+"_"))
                if i > 0 :
                    g.add_edge(str(i)+"_0", str(i-1)+"_1")
            g.add_edge("0_0", str(num_clique-1)+"_1")
            part = co.best_partition(g)

            for clique in range(num_clique) :
                p = part[str(clique) + "_0"]
                for node in range(size_clique) :
                    self.assertEqual(p, part[str(clique) + "_" + str(node)])

    def test_allnodes(self) :
        """
        Test that all nodes are in a community
        """
        g = nx.erdos_renyi_graph(50, 0.1)
        part = co.best_partition(g)
        for node in g.nodes() :
            self.assert_(node in part)




class InducedGraphTest(unittest.TestCase):

    def test_nodes(self) :
        """
        Test that result nodes are the communities
        """
        g = nx.erdos_renyi_graph(50, 0.1)
        part = dict([])
        for node in g.nodes() :
            part[node] = node % 5
        self.assertSetEqual(set(part.values()), set(co.induced_graph(part, g).nodes()))

    def test_weight(self) :
        """
        Test that total edge weight does not change
        """
        g = nx.erdos_renyi_graph(50, 0.1)
        part = dict([])
        for node in g.nodes() :
            part[node] = node % 5
        self.assertEqual(g.size(weight = 'weight'), co.induced_graph(part, g).size(weight = 'weight'))

    def test_uniq(self) :
        """
        Test that the induced graph is the same when all nodes are alone
        """
        g = nx.erdos_renyi_graph(50, 0.1)
        part = dict([])
        for node in g.nodes() :
            part[node] = node
        ind = co.induced_graph(part, g)
        self.assert_(nx.is_isomorphic(g, ind))

    def test_clique(self):
        """
        Test that a complet graph of size 2*n has the right behavior when split in two
        """
        n = 5
        g = nx.complete_graph(2*n)
        part = dict([])
        for node in g.nodes() :
            part[node] = node % 2
        ind = co.induced_graph(part, g)
        goal = nx.Graph()
        goal.add_weighted_edges_from([(0,1,n*n),(0,0,n*(n-1)/2), (1, 1, n*(n-1)/2)])
        self.assert_(nx.is_isomorphic(ind, goal))


class PartitionAtLevelTest(unittest.TestCase):
    pass

class GenerateDendrogramTest(unittest.TestCase):
    def test_bad_graph_input(self) :
        """generate_dendrogram is only defined with undirected graph"""
        g = nx.erdos_renyi_graph(50, 0.1, directed=True)
        self.assertRaises(TypeError, co.best_partition,  g)

    def test_modularity_increase(self):
        """
        Generate a dendrogram and test that modularity is always increasing
        """
        g = nx.erdos_renyi_graph(1000, 0.01)
        dendo = co.generate_dendrogram(g)
        mod_prec = -1.
        mods = [co.modularity(co.partition_at_level(dendo, level), g) for level in range(len(dendo)) ]
        self.assertListEqual(mods, sorted(mods))

    def test_nodes_stay_together(self):
        """
        Test that two nodes in the same community at one level stay in the same at higher level
        """
        g = nx.erdos_renyi_graph(500, 0.01)
        dendo = co.generate_dendrogram(g)
        parts = dict([])
        for l in range(len(dendo)) :
            parts[l] = co.partition_at_level(dendo, l)
        for l in range(len(dendo)-1) :
            p1 = parts[l]
            p2 = parts[l+1]
            coms = set(p1.values())
            for com in coms :
                comhigher = [ p2[node] for node, comnode in p1.items() if comnode == com]
                self.assertEqual(len(set(comhigher)), 1)


if __name__ == '__main__':
    unittest.main()
import networkx as nx
import matplotlib.pyplot as plt

g = nx.Graph()

h = nx.path_graph(5)
g.add_nodes_from(h)
g.add_edge(1,3)
g.add_edges_from(h.edges())
#print g.nodes()

g.add_node(5, time= '0:00')
g.add_edge(2,5,weight=0.9)

#print nx.shortest_path(g, 0,1)
#plt.figure()
#nx.draw(g)

#g.remove_node(1)
#plt.figure()
#nx.draw(g)
"""
print g.neighbors(1)
print g.degree(1)
print g.edges()
print g.node[5]
print g.node[5]['time']
print g[2][5]

for n1, n2, attr in g.edges(data=True): # unpacking
	print n1, n2, attr

for node in g.nodes(): # or node in g.nodes_iter():
	print node, g.degree(node)
"""
dg = nx.DiGraph()
dg.add_weighted_edges_from([(1, 4, 0.5), (3, 1, 0.75)])

print dg.nodes()
print dg.degree(1, weight='weight')
print dg.degree(1)

barbell = nx.barbell_graph(10, 10)
K_5 = nx.complete_graph(5)

plt.figure()
#nx.draw(K_5)
nx.draw(barbell)
plt.show()